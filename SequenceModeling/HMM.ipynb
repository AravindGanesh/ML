{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-MnB988uaxjl"
   },
   "source": [
    "# Hidden Markov Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrId2WLWQVqj"
   },
   "source": [
    "## Part 2 : Implementing HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BurfAHxZZ5A2"
   },
   "source": [
    "\n",
    "For the set of training examples and test samples of isolated words (two word classes, 0\n",
    "and 1) from part 1: Implement HMM\n",
    "1. Find Mel-Frequency Cepstral Coefficients (MFCCs) from the raw speech samples. Pick 25 ms worth of speech samples with a 10 ms overlap to find MFCCs. Use the basic 13 element version of MFCC as the feature vector representing 25 ms of speech. You are free to use “Librosa” Python library to find this feature.\n",
    "2. Write a code to implement the likelihood computation using the forward variable after assuming a uniform flat start based initialization with 5 states per HMM and GMM with 3 mixture components per state.\n",
    "3. Write a code to implement the Viterbi algorithm to decode the best state sequence using the existing model\n",
    "4. Use the Baum-Welch re estimation method to train HMMs with examples from class 0 and class 1\n",
    "5. Implement a basic two-class classifier using the HMMs constructed in the previous step. In practise, test samples come from a continuous speech waveform. Herehowever, test your classifier using samples from the database. Your classifier should simply construct the likelihood of the test sample and choose the phone with higher likelihood. Classify the test examples and report the performance. How does the performance change for different number of states per HMM, different number of mixture components per GMM?."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_CTHUVxQKkp"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named librosa",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-76e296de2b34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: No module named librosa"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUCjrp7KQ-rR"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-776b8c198ef0>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-776b8c198ef0>\"\u001b[1;36m, line \u001b[1;32m5\u001b[0m\n\u001b[1;33m    px *= np.exp(-0.5 (x-mean).T @ np.linalg.in)\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def gaussian_pdf(x, mean, covar):\n",
    "    d = len(x)\n",
    "    px = 1/np.sqrt(np.linalg.det(covar) * ((2*np.pi)**d))\n",
    "    px *= np.exp(-0.5*np.matmul(np.matmul(np.transpose(x-mean), np.linalg.inv(covar)), x-mean))\n",
    "    px *= np.exp(-0.5 (x-mean).T @ np.linalg.in)\n",
    "    return px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "6zLpofk3JyaM"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "class dataset():\n",
    "    def __init__(self, folder, sr=8000, window=25, overlap=10, n_mfcc=13):\n",
    "        self.folder = folder\n",
    "        self.files = sorted(os.listdir(folder))\n",
    "        self.sr = sr # sampling rate of given audio files\n",
    "        self.window = int(window*1e-3 * self.sr) # 25ms window\n",
    "        self.overlap = int(overlap*1e-3 * self.sr) # 10ms overlap\n",
    "        self.hop = self.window -  self.overlap\n",
    "        self.n_mfcc = n_mfcc\n",
    "        \n",
    "    def get_features(self, wav_path):\n",
    "        y, self.sr = librosa.core.load(path=wav_path, sr=None, mono=True)\n",
    "        feats = librosa.feature.mfcc(y=y, sr=self.sr, n_mfcc=self.n_mfcc, n_fft=self.window, hop_length=self.hop, n_mels=64) \n",
    "        return feats.T\n",
    "\n",
    "    def __call__(self):\n",
    "        X = np.vstack((self.get_features(os.path.join(self.folder, wav)) for wav in self.files))\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aravind Ganesh\n"
     ]
    }
   ],
   "source": [
    "print(\"Aravind Ganesh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "C1lLh003VnTq"
   },
   "outputs": [],
   "source": [
    "class GMM():\n",
    "    def __init__(self,data, d, K):\n",
    "        self.d = d\n",
    "        self.X = data\n",
    "        self.K = K\n",
    "        self.N = len(X)\n",
    "        #initalization using kmeans\n",
    "        kmeans = Kmeans(X=data, K=K)\n",
    "        kmeans.cluster(n_iter=1)\n",
    "        self.means = kmeans.means\n",
    "        # covariances are initalized by finding covariances of each cluster\n",
    "        mixtures = kmeans.get_clusters()\n",
    "        self.covars = np.array([np.cov(mixture, rowvar=False) for mixture in mixtures])\n",
    "        # inialize mixing coefficients by fractions of number of data points in each cluster\n",
    "        self.w = np.array([np.mean(kmeans.assign_k == k) for k in np.arange(K)])\n",
    "        self.ln_p = self.log_likelihood()\n",
    "    \n",
    "    def log_likelihood(self):\n",
    "        Px = np.sum([np.log(\n",
    "                np.sum(\n",
    "                    [self.w[k] * self.normal_pdf(x, self.means[k], self.covars[k])\n",
    "                     for k in range(self.K)]))\n",
    "                for x in self.X])\n",
    "        return Px\n",
    "    \n",
    "    def normal_pdf(self, x, mu, sigma): # multivariate Gaussian pdf function\n",
    "        n = len(x)\n",
    "        px = 1/np.sqrt(((2*np.pi)**n)*np.linalg.det(sigma))\n",
    "        px *= np.exp(-0.5*np.matmul(np.matmul(np.transpose(x-mu), np.linalg.inv(sigma)),(x-mu)))\n",
    "        return px\n",
    "    \n",
    "    def gamma(self, n, k): # E step: γ(z_nk)\n",
    "        r = self.w[k]*self.normal_pdf(self.X[n], self.means[k], self.covars[k])\n",
    "        r /= np.sum([self.w[i]*self.normal_pdf(self.X[n], self.means[i], self.covars[i]) for i in range(self.K)])\n",
    "        return r\n",
    "\n",
    "    def maximization(self): # M step\n",
    "        Nk = np.array([np.sum([self.gamma(n,k) for n in range(self.N)]) for k in range(self.K)])\n",
    "        # Update means  \n",
    "        means_new = np.array([\n",
    "            (1/Nk[k]) * np.sum([self.gamma(n,k)*self.X[n] for n in range(self.N)], axis=0) for k in range(self.K)\n",
    "        ])\n",
    "        # Update covariance matrices\n",
    "        covars_new = np.array([\n",
    "            (1/Nk[k]) * np.sum([self.gamma(n,k)*np.tensordot((X[n]-means_new[k]), (X[n]-means_new[k]), axes=0) \n",
    "                    for n in range(self.N)], axis=0) \n",
    "        for k in range(self.K)])\n",
    "        # Update mixing coefficients\n",
    "        w_new = Nk/self.N \n",
    "#         self.new_ln_p = self.log_likelihood()\n",
    "        return w_new, means_new, covars_new\n",
    "    \n",
    "    def EM(self, threshold):\n",
    "        count = 0\n",
    "        self.w, self.means, self.covars = self.maximization()\n",
    "        self.new_ln_p = self.log_likelihood()\n",
    "        count +=1\n",
    "        print('Iteration Count: ', count)\n",
    "        print('Log likelihood error: ', (self.new_ln_p - self.ln_p))\n",
    "        print('Log likelihood :', self.ln_p, self.new_ln_p)\n",
    "        while (self.new_ln_p - self.ln_p) > threshold:\n",
    "            self.ln_p = self.new_ln_p\n",
    "            self.w, self.means, self.covars = self.maximization()\n",
    "            self.new_ln_p = self.log_likelihood()\n",
    "            count +=1\n",
    "            print('Iteration Count:', count)\n",
    "            print('Log likelihood error: ', (self.new_ln_p - self.ln_p))\n",
    "            print('Log likelihood :', self.new_ln_p)\n",
    "    \n",
    "    def optimal_params(self):\n",
    "        return self.w, self.means, self.covars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XgPzutWSZ0qX"
   },
   "source": [
    "## HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "both",
    "colab": {},
    "colab_type": "code",
    "id": "krOFAieqVvLQ"
   },
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    def __init__(self, num_states):\n",
    "        self.K = K # states\n",
    "        self.X = trainset\n",
    "        self.M, self.N, self.d = self.X.shape # M:number of sound files, N=states, d=13 \n",
    "        # initializing parameters\n",
    "        self.pi = np.array([1/self.N]*self.N)\n",
    "        self.A = np.triu(np.random.uniform(high=1, low=0, size=(self.K, self.K)))\n",
    "        self.A = np.transpose(self.A.T/np.sum(self.A, axis=1))\n",
    "        #initalization using kmeans\n",
    "        kmeans = Kmeans(X=self.X[0], K=self.K)\n",
    "        kmeans.cluster(n_iter=2)\n",
    "        self.means = kmeans.means # initialize means\n",
    "        # covariances are initalized by finding covariances of each cluster\n",
    "        mixtures = kmeans.get_clusters()\n",
    "        # print(mixtures[0].shape)\n",
    "        self.covars = np.zeros((self.K, self.d, self.d))\n",
    "        for k in range(self.K):\n",
    "            self.covars[k] = np.cov(mixtures[k], rowvar=False)\n",
    "        #intialize arrays\n",
    "        self.alpha = self.beta = self.gamma = self.emission = np.zeros((self.N, self.K)) # shape: (N,k)\n",
    "        self.Q = np.random.normal(0,1)\n",
    "    \n",
    "    def maximization(self, X):\n",
    "        # emission probabilities\n",
    "        for n,x in enumerate(X):\n",
    "            for k in range(self.K):\n",
    "                self.emission[n][k] = gaussian(np.asarray(x), self.means[k], np.asarray(self.covars[k]))\n",
    "        # alpha - forward \n",
    "        self.alpha[0] = self.emission[0] * self.pi\n",
    "        for n in range(1,self.N):\n",
    "            self.alpha[n] = self.emission[n] * np.matmul(self.alpha[n-1], self.A)\n",
    "        # beta - backward\n",
    "        self.beta[self.N-1] = np.ones(self.K) # self.beta.shape: (N,k)\n",
    "        for n in range(self.N-2, -1, -1):\n",
    "            self.beta[n] = np.matmul(self.A, self.emission[n+1]*self.beta[n+1])\n",
    "        self.pX = np.sum(self.alpha[self.N-1])\n",
    "        # gamma \n",
    "        self.gamma = self.alpha * self.beta / self.pX\n",
    "        self.Zeta = np.array([self.emission[n]*(self.alpha[n]*(self.A*self.beta[n]).T).T  for n in range(self.N)]) / self.pX\n",
    "        # update parameters\n",
    "        new_pi = self.gamma[0]/np.sum(self.gamma[0])\n",
    "        new_A = np.sum(self.Zeta, axis=0) / np.sum(np.sum(self.Zeta, axis=0), axis=1)\n",
    "        new_covars, new_means = np.zeros((self.K, self.d, self.d)), np.zeros((self.K, self.d))\n",
    "        for k in range(self.K):\n",
    "            # ck = np.matmul(X[n]-self.means[k], np.transpose(X[n]-self.means[k]))\n",
    "            new_covars[k] = np.sum([self.gamma[n][k]*np.tensordot(X[n]-self.means[k], X[n]-self.means[k], axes=0) for n in range(self.N)], axis=0) / np.sum(self.gamma[:,k], axis=0)\n",
    "            #print(np.linalg.det(self.covars[k]))\n",
    "            new_means[k] = np.sum([self.gamma[n][k]*X[n] for n in range(self.N)], axis=0) / np.sum(self.gamma[:,k], axis=0)\n",
    "            \n",
    "        return [new_pi, new_A, new_means, new_covars]\n",
    "        \n",
    "    def likelihood(self, X):\n",
    "        return np.sum(self.alpha[self.N-1]) # beta(ZN)=1\n",
    "    \n",
    "    def expectation(self):\n",
    "        # expectation step\n",
    "        Q = np.sum(self.gamma[0] * self.pi) + np.sum(self.Zeta*self.A) + np.sum(self.gamma * self.emission)\n",
    "        return Q\n",
    "    \n",
    "    def train(self, threshold):\n",
    "        # EM iterations to train the model\n",
    "        [self.pi, self.A, self.means, self.covars] =  self.maximization(self.X[0])\n",
    "        self.new_Q = self.expectation()\n",
    "        print('log likelihood: ', self.Q)\n",
    "        print('Error:', self.new_Q-self.Q)\n",
    "#         while np.abs(self.new_Q - self.Q) > threshold:\n",
    "        count=1\n",
    "        while np.abs(self.Q - self.new_Q) > threshold:\n",
    "            cnt=0\n",
    "            print(self.Q, self.new_Q)\n",
    "            for X in self.X:\n",
    "                cnt+=1\n",
    "                self.Q = self.new_Q\n",
    "                [self.pi, self.A, self.means, self.covars] =  self.maximization(X)\n",
    "                self.new_Q = self.expectation()\n",
    "                print('batch_count', cnt)\n",
    "                print('log likelihood: ', self.new_Q)\n",
    "                print('Error: ', np.abs(self.new_Q-self.Q))\n",
    "                break\n",
    "            count +=1\n",
    "            print('Epoch:', count)\n",
    "            \n",
    "    def get_params(self):\n",
    "        return {'means':self.means, 'covars':self.covars, 'pi':self.pi, 'transitions':self.A}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VvU9k-tdKSBg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wnjeVG-iKhM0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oe5DMYz-LfD_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CbyGE90EMSHJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HMM.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
