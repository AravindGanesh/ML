{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the back-propagation algorithm to learn the weights of a perceptron with 2 input nodes, 2 hidden nodes and 1 output node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **Python3** in used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters - input all parameter values here\n",
    "input_dim = 2\n",
    "hidden_dim = 2 # dimensions of hidden layers\n",
    "std = 0.01  # train data noise standard deviation\n",
    "w_std = 0.5\n",
    "learn_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "x_inputs = np.array([np.zeros(2), np.ones(2), np.array([1,0]), np.array([0,1])])\n",
    "def generate_trainset(N):\n",
    "    X = np.repeat(x_inputs, N//4, axis=0)\n",
    "    y_or = np.logical_or(X.T[0], X.T[1]).astype(np.float)\n",
    "    # add noise to data\n",
    "    X += np.random.normal(0, std, X.shape)\n",
    "    y_or += np.random.normal(0, std, N)\n",
    "    # shuffle the training data\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    x_train, y_train = X[indices], y_or[indices]\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid( t):\n",
    "    return 1/(1  + np.exp(-t))\n",
    "\n",
    "def dsigmoid( t):\n",
    "    return sigmoid(t)*(1 - sigmoid(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Experiment with N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "x_train, y_train = generate_trainset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/300  Loss:  [0.19238834]\n",
      "Epoch:  2/300  Loss:  [0.1904154]\n",
      "Epoch:  3/300  Loss:  [0.18910905]\n",
      "Epoch:  4/300  Loss:  [0.18831687]\n",
      "Epoch:  5/300  Loss:  [0.18785601]\n",
      "Epoch:  6/300  Loss:  [0.18757744]\n",
      "Epoch:  7/300  Loss:  [0.18738627]\n",
      "Epoch:  8/300  Loss:  [0.18723157]\n",
      "Epoch:  9/300  Loss:  [0.18708879]\n",
      "Epoch:  10/300  Loss:  [0.18694664]\n",
      "Epoch:  11/300  Loss:  [0.18679986]\n",
      "Epoch:  12/300  Loss:  [0.18664569]\n",
      "Epoch:  13/300  Loss:  [0.18648229]\n",
      "Epoch:  14/300  Loss:  [0.18630814]\n",
      "Epoch:  15/300  Loss:  [0.18612162]\n",
      "Epoch:  16/300  Loss:  [0.18592084]\n",
      "Epoch:  17/300  Loss:  [0.1857035]\n",
      "Epoch:  18/300  Loss:  [0.18546669]\n",
      "Epoch:  19/300  Loss:  [0.18520676]\n",
      "Epoch:  20/300  Loss:  [0.18491914]\n",
      "Epoch:  21/300  Loss:  [0.18459805]\n",
      "Epoch:  22/300  Loss:  [0.18423632]\n",
      "Epoch:  23/300  Loss:  [0.18382498]\n",
      "Epoch:  24/300  Loss:  [0.18335296]\n",
      "Epoch:  25/300  Loss:  [0.18280665]\n",
      "Epoch:  26/300  Loss:  [0.18216938]\n",
      "Epoch:  27/300  Loss:  [0.18142094]\n",
      "Epoch:  28/300  Loss:  [0.18053712]\n",
      "Epoch:  29/300  Loss:  [0.17948924]\n",
      "Epoch:  30/300  Loss:  [0.17824403]\n",
      "Epoch:  31/300  Loss:  [0.17676374]\n",
      "Epoch:  32/300  Loss:  [0.17500682]\n",
      "Epoch:  33/300  Loss:  [0.17292931]\n",
      "Epoch:  34/300  Loss:  [0.17048696]\n",
      "Epoch:  35/300  Loss:  [0.167638]\n",
      "Epoch:  36/300  Loss:  [0.16434655]\n",
      "Epoch:  37/300  Loss:  [0.16058614]\n",
      "Epoch:  38/300  Loss:  [0.15634328]\n",
      "Epoch:  39/300  Loss:  [0.15162056]\n",
      "Epoch:  40/300  Loss:  [0.14643925]\n",
      "Epoch:  41/300  Loss:  [0.14084078]\n",
      "Epoch:  42/300  Loss:  [0.1348867]\n",
      "Epoch:  43/300  Loss:  [0.12865654]\n",
      "Epoch:  44/300  Loss:  [0.12224341]\n",
      "Epoch:  45/300  Loss:  [0.11574761]\n",
      "Epoch:  46/300  Loss:  [0.10926928]\n",
      "Epoch:  47/300  Loss:  [0.10290121]\n",
      "Epoch:  48/300  Loss:  [0.09672322]\n",
      "Epoch:  49/300  Loss:  [0.09079865]\n",
      "Epoch:  50/300  Loss:  [0.08517323]\n",
      "Epoch:  51/300  Loss:  [0.07987588]\n",
      "Epoch:  52/300  Loss:  [0.07492077]\n",
      "Epoch:  53/300  Loss:  [0.07031014]\n",
      "Epoch:  54/300  Loss:  [0.06603716]\n",
      "Epoch:  55/300  Loss:  [0.06208855]\n",
      "Epoch:  56/300  Loss:  [0.05844689]\n",
      "Epoch:  57/300  Loss:  [0.05509236]\n",
      "Epoch:  58/300  Loss:  [0.05200409]\n",
      "Epoch:  59/300  Loss:  [0.04916114]\n",
      "Epoch:  60/300  Loss:  [0.04654315]\n",
      "Epoch:  61/300  Loss:  [0.04413072]\n",
      "Epoch:  62/300  Loss:  [0.04190571]\n",
      "Epoch:  63/300  Loss:  [0.03985132]\n",
      "Epoch:  64/300  Loss:  [0.03795213]\n",
      "Epoch:  65/300  Loss:  [0.03619407]\n",
      "Epoch:  66/300  Loss:  [0.03456438]\n",
      "Epoch:  67/300  Loss:  [0.03305148]\n",
      "Epoch:  68/300  Loss:  [0.03164493]\n",
      "Epoch:  69/300  Loss:  [0.03033531]\n",
      "Epoch:  70/300  Loss:  [0.02911413]\n",
      "Epoch:  71/300  Loss:  [0.02797374]\n",
      "Epoch:  72/300  Loss:  [0.02690723]\n",
      "Epoch:  73/300  Loss:  [0.0259084]\n",
      "Epoch:  74/300  Loss:  [0.02497163]\n",
      "Epoch:  75/300  Loss:  [0.02409187]\n",
      "Epoch:  76/300  Loss:  [0.02326453]\n",
      "Epoch:  77/300  Loss:  [0.02248547]\n",
      "Epoch:  78/300  Loss:  [0.02175096]\n",
      "Epoch:  79/300  Loss:  [0.02105758]\n",
      "Epoch:  80/300  Loss:  [0.02040225]\n",
      "Epoch:  81/300  Loss:  [0.01978216]\n",
      "Epoch:  82/300  Loss:  [0.01919476]\n",
      "Epoch:  83/300  Loss:  [0.01863771]\n",
      "Epoch:  84/300  Loss:  [0.01810889]\n",
      "Epoch:  85/300  Loss:  [0.01760635]\n",
      "Epoch:  86/300  Loss:  [0.01712832]\n",
      "Epoch:  87/300  Loss:  [0.01667315]\n",
      "Epoch:  88/300  Loss:  [0.01623935]\n",
      "Epoch:  89/300  Loss:  [0.01582555]\n",
      "Epoch:  90/300  Loss:  [0.01543047]\n",
      "Epoch:  91/300  Loss:  [0.01505295]\n",
      "Epoch:  92/300  Loss:  [0.01469192]\n",
      "Epoch:  93/300  Loss:  [0.01434638]\n",
      "Epoch:  94/300  Loss:  [0.0140154]\n",
      "Epoch:  95/300  Loss:  [0.01369814]\n",
      "Epoch:  96/300  Loss:  [0.01339381]\n",
      "Epoch:  97/300  Loss:  [0.01310168]\n",
      "Epoch:  98/300  Loss:  [0.01282106]\n",
      "Epoch:  99/300  Loss:  [0.01255132]\n",
      "Epoch:  100/300  Loss:  [0.01229188]\n",
      "Epoch:  101/300  Loss:  [0.01204218]\n",
      "Epoch:  102/300  Loss:  [0.01180171]\n",
      "Epoch:  103/300  Loss:  [0.01156999]\n",
      "Epoch:  104/300  Loss:  [0.01134659]\n",
      "Epoch:  105/300  Loss:  [0.01113107]\n",
      "Epoch:  106/300  Loss:  [0.01092305]\n",
      "Epoch:  107/300  Loss:  [0.01072216]\n",
      "Epoch:  108/300  Loss:  [0.01052805]\n",
      "Epoch:  109/300  Loss:  [0.01034041]\n",
      "Epoch:  110/300  Loss:  [0.01015893]\n",
      "Epoch:  111/300  Loss:  [0.00998333]\n",
      "Epoch:  112/300  Loss:  [0.00981333]\n",
      "Epoch:  113/300  Loss:  [0.00964868]\n",
      "Epoch:  114/300  Loss:  [0.00948915]\n",
      "Epoch:  115/300  Loss:  [0.00933451]\n",
      "Epoch:  116/300  Loss:  [0.00918454]\n",
      "Epoch:  117/300  Loss:  [0.00903906]\n",
      "Epoch:  118/300  Loss:  [0.00889785]\n",
      "Epoch:  119/300  Loss:  [0.00876076]\n",
      "Epoch:  120/300  Loss:  [0.0086276]\n",
      "Epoch:  121/300  Loss:  [0.00849822]\n",
      "Epoch:  122/300  Loss:  [0.00837247]\n",
      "Epoch:  123/300  Loss:  [0.00825019]\n",
      "Epoch:  124/300  Loss:  [0.00813126]\n",
      "Epoch:  125/300  Loss:  [0.00801554]\n",
      "Epoch:  126/300  Loss:  [0.00790291]\n",
      "Epoch:  127/300  Loss:  [0.00779325]\n",
      "Epoch:  128/300  Loss:  [0.00768645]\n",
      "Epoch:  129/300  Loss:  [0.00758241]\n",
      "Epoch:  130/300  Loss:  [0.00748101]\n",
      "Epoch:  131/300  Loss:  [0.00738217]\n",
      "Epoch:  132/300  Loss:  [0.00728579]\n",
      "Epoch:  133/300  Loss:  [0.00719179]\n",
      "Epoch:  134/300  Loss:  [0.00710008]\n",
      "Epoch:  135/300  Loss:  [0.00701058]\n",
      "Epoch:  136/300  Loss:  [0.00692321]\n",
      "Epoch:  137/300  Loss:  [0.00683791]\n",
      "Epoch:  138/300  Loss:  [0.0067546]\n",
      "Epoch:  139/300  Loss:  [0.00667321]\n",
      "Epoch:  140/300  Loss:  [0.00659368]\n",
      "Epoch:  141/300  Loss:  [0.00651596]\n",
      "Epoch:  142/300  Loss:  [0.00643998]\n",
      "Epoch:  143/300  Loss:  [0.00636568]\n",
      "Epoch:  144/300  Loss:  [0.00629301]\n",
      "Epoch:  145/300  Loss:  [0.00622193]\n",
      "Epoch:  146/300  Loss:  [0.00615237]\n",
      "Epoch:  147/300  Loss:  [0.0060843]\n",
      "Epoch:  148/300  Loss:  [0.00601767]\n",
      "Epoch:  149/300  Loss:  [0.00595243]\n",
      "Epoch:  150/300  Loss:  [0.00588854]\n",
      "Epoch:  151/300  Loss:  [0.00582597]\n",
      "Epoch:  152/300  Loss:  [0.00576467]\n",
      "Epoch:  153/300  Loss:  [0.0057046]\n",
      "Epoch:  154/300  Loss:  [0.00564573]\n",
      "Epoch:  155/300  Loss:  [0.00558803]\n",
      "Epoch:  156/300  Loss:  [0.00553146]\n",
      "Epoch:  157/300  Loss:  [0.00547599]\n",
      "Epoch:  158/300  Loss:  [0.00542159]\n",
      "Epoch:  159/300  Loss:  [0.00536823]\n",
      "Epoch:  160/300  Loss:  [0.00531588]\n",
      "Epoch:  161/300  Loss:  [0.00526451]\n",
      "Epoch:  162/300  Loss:  [0.0052141]\n",
      "Epoch:  163/300  Loss:  [0.00516461]\n",
      "Epoch:  164/300  Loss:  [0.00511604]\n",
      "Epoch:  165/300  Loss:  [0.00506834]\n",
      "Epoch:  166/300  Loss:  [0.00502151]\n",
      "Epoch:  167/300  Loss:  [0.0049755]\n",
      "Epoch:  168/300  Loss:  [0.00493032]\n",
      "Epoch:  169/300  Loss:  [0.00488593]\n",
      "Epoch:  170/300  Loss:  [0.00484231]\n",
      "Epoch:  171/300  Loss:  [0.00479944]\n",
      "Epoch:  172/300  Loss:  [0.00475731]\n",
      "Epoch:  173/300  Loss:  [0.0047159]\n",
      "Epoch:  174/300  Loss:  [0.00467518]\n",
      "Epoch:  175/300  Loss:  [0.00463515]\n",
      "Epoch:  176/300  Loss:  [0.00459578]\n",
      "Epoch:  177/300  Loss:  [0.00455706]\n",
      "Epoch:  178/300  Loss:  [0.00451898]\n",
      "Epoch:  179/300  Loss:  [0.00448151]\n",
      "Epoch:  180/300  Loss:  [0.00444465]\n",
      "Epoch:  181/300  Loss:  [0.00440838]\n",
      "Epoch:  182/300  Loss:  [0.00437269]\n",
      "Epoch:  183/300  Loss:  [0.00433755]\n",
      "Epoch:  184/300  Loss:  [0.00430297]\n",
      "Epoch:  185/300  Loss:  [0.00426893]\n",
      "Epoch:  186/300  Loss:  [0.00423541]\n",
      "Epoch:  187/300  Loss:  [0.0042024]\n",
      "Epoch:  188/300  Loss:  [0.0041699]\n",
      "Epoch:  189/300  Loss:  [0.00413789]\n",
      "Epoch:  190/300  Loss:  [0.00410635]\n",
      "Epoch:  191/300  Loss:  [0.00407529]\n",
      "Epoch:  192/300  Loss:  [0.00404469]\n",
      "Epoch:  193/300  Loss:  [0.00401453]\n",
      "Epoch:  194/300  Loss:  [0.00398482]\n",
      "Epoch:  195/300  Loss:  [0.00395554]\n",
      "Epoch:  196/300  Loss:  [0.00392668]\n",
      "Epoch:  197/300  Loss:  [0.00389823]\n",
      "Epoch:  198/300  Loss:  [0.00387019]\n",
      "Epoch:  199/300  Loss:  [0.00384254]\n",
      "Epoch:  200/300  Loss:  [0.00381528]\n",
      "Epoch:  201/300  Loss:  [0.0037884]\n",
      "Epoch:  202/300  Loss:  [0.00376189]\n",
      "Epoch:  203/300  Loss:  [0.00373575]\n",
      "Epoch:  204/300  Loss:  [0.00370996]\n",
      "Epoch:  205/300  Loss:  [0.00368452]\n",
      "Epoch:  206/300  Loss:  [0.00365943]\n",
      "Epoch:  207/300  Loss:  [0.00363467]\n",
      "Epoch:  208/300  Loss:  [0.00361025]\n",
      "Epoch:  209/300  Loss:  [0.00358614]\n",
      "Epoch:  210/300  Loss:  [0.00356236]\n",
      "Epoch:  211/300  Loss:  [0.00353888]\n",
      "Epoch:  212/300  Loss:  [0.00351571]\n",
      "Epoch:  213/300  Loss:  [0.00349284]\n",
      "Epoch:  214/300  Loss:  [0.00347026]\n",
      "Epoch:  215/300  Loss:  [0.00344798]\n",
      "Epoch:  216/300  Loss:  [0.00342597]\n",
      "Epoch:  217/300  Loss:  [0.00340424]\n",
      "Epoch:  218/300  Loss:  [0.00338279]\n",
      "Epoch:  219/300  Loss:  [0.0033616]\n",
      "Epoch:  220/300  Loss:  [0.00334067]\n",
      "Epoch:  221/300  Loss:  [0.00332001]\n",
      "Epoch:  222/300  Loss:  [0.00329959]\n",
      "Epoch:  223/300  Loss:  [0.00327943]\n",
      "Epoch:  224/300  Loss:  [0.0032595]\n",
      "Epoch:  225/300  Loss:  [0.00323982]\n",
      "Epoch:  226/300  Loss:  [0.00322038]\n",
      "Epoch:  227/300  Loss:  [0.00320116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  228/300  Loss:  [0.00318218]\n",
      "Epoch:  229/300  Loss:  [0.00316341]\n",
      "Epoch:  230/300  Loss:  [0.00314487]\n",
      "Epoch:  231/300  Loss:  [0.00312654]\n",
      "Epoch:  232/300  Loss:  [0.00310843]\n",
      "Epoch:  233/300  Loss:  [0.00309052]\n",
      "Epoch:  234/300  Loss:  [0.00307282]\n",
      "Epoch:  235/300  Loss:  [0.00305532]\n",
      "Epoch:  236/300  Loss:  [0.00303802]\n",
      "Epoch:  237/300  Loss:  [0.00302092]\n",
      "Epoch:  238/300  Loss:  [0.003004]\n",
      "Epoch:  239/300  Loss:  [0.00298728]\n",
      "Epoch:  240/300  Loss:  [0.00297074]\n",
      "Epoch:  241/300  Loss:  [0.00295438]\n",
      "Epoch:  242/300  Loss:  [0.0029382]\n",
      "Epoch:  243/300  Loss:  [0.0029222]\n",
      "Epoch:  244/300  Loss:  [0.00290638]\n",
      "Epoch:  245/300  Loss:  [0.00289072]\n",
      "Epoch:  246/300  Loss:  [0.00287523]\n",
      "Epoch:  247/300  Loss:  [0.00285991]\n",
      "Epoch:  248/300  Loss:  [0.00284476]\n",
      "Epoch:  249/300  Loss:  [0.00282976]\n",
      "Epoch:  250/300  Loss:  [0.00281492]\n",
      "Epoch:  251/300  Loss:  [0.00280024]\n",
      "Epoch:  252/300  Loss:  [0.00278571]\n",
      "Epoch:  253/300  Loss:  [0.00277133]\n",
      "Epoch:  254/300  Loss:  [0.0027571]\n",
      "Epoch:  255/300  Loss:  [0.00274301]\n",
      "Epoch:  256/300  Loss:  [0.00272907]\n",
      "Epoch:  257/300  Loss:  [0.00271528]\n",
      "Epoch:  258/300  Loss:  [0.00270162]\n",
      "Epoch:  259/300  Loss:  [0.0026881]\n",
      "Epoch:  260/300  Loss:  [0.00267471]\n",
      "Epoch:  261/300  Loss:  [0.00266146]\n",
      "Epoch:  262/300  Loss:  [0.00264835]\n",
      "Epoch:  263/300  Loss:  [0.00263536]\n",
      "Epoch:  264/300  Loss:  [0.0026225]\n",
      "Epoch:  265/300  Loss:  [0.00260976]\n",
      "Epoch:  266/300  Loss:  [0.00259715]\n",
      "Epoch:  267/300  Loss:  [0.00258467]\n",
      "Epoch:  268/300  Loss:  [0.0025723]\n",
      "Epoch:  269/300  Loss:  [0.00256005]\n",
      "Epoch:  270/300  Loss:  [0.00254792]\n",
      "Epoch:  271/300  Loss:  [0.00253591]\n",
      "Epoch:  272/300  Loss:  [0.00252401]\n",
      "Epoch:  273/300  Loss:  [0.00251222]\n",
      "Epoch:  274/300  Loss:  [0.00250054]\n",
      "Epoch:  275/300  Loss:  [0.00248898]\n",
      "Epoch:  276/300  Loss:  [0.00247752]\n",
      "Epoch:  277/300  Loss:  [0.00246617]\n",
      "Epoch:  278/300  Loss:  [0.00245492]\n",
      "Epoch:  279/300  Loss:  [0.00244377]\n",
      "Epoch:  280/300  Loss:  [0.00243273]\n",
      "Epoch:  281/300  Loss:  [0.00242179]\n",
      "Epoch:  282/300  Loss:  [0.00241095]\n",
      "Epoch:  283/300  Loss:  [0.00240021]\n",
      "Epoch:  284/300  Loss:  [0.00238956]\n",
      "Epoch:  285/300  Loss:  [0.00237901]\n",
      "Epoch:  286/300  Loss:  [0.00236855]\n",
      "Epoch:  287/300  Loss:  [0.00235819]\n",
      "Epoch:  288/300  Loss:  [0.00234792]\n",
      "Epoch:  289/300  Loss:  [0.00233774]\n",
      "Epoch:  290/300  Loss:  [0.00232764]\n",
      "Epoch:  291/300  Loss:  [0.00231764]\n",
      "Epoch:  292/300  Loss:  [0.00230772]\n",
      "Epoch:  293/300  Loss:  [0.00229789]\n",
      "Epoch:  294/300  Loss:  [0.00228815]\n",
      "Epoch:  295/300  Loss:  [0.00227849]\n",
      "Epoch:  296/300  Loss:  [0.00226891]\n",
      "Epoch:  297/300  Loss:  [0.00225941]\n",
      "Epoch:  298/300  Loss:  [0.00225]\n",
      "Epoch:  299/300  Loss:  [0.00224066]\n",
      "Epoch:  300/300  Loss:  [0.0022314]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 300 # number of itrations\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0690202 ]\n",
      " [0.97419909]\n",
      " [0.96353925]\n",
      " [0.96327857]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Experiment with N = 100\n",
    "poor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "x_train, y_train = generate_trainset(N)\n",
    "learn_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/250  Loss:  [0.29375708]\n",
      "Epoch:  2/250  Loss:  [0.27919393]\n",
      "Epoch:  3/250  Loss:  [0.26619555]\n",
      "Epoch:  4/250  Loss:  [0.25466529]\n",
      "Epoch:  5/250  Loss:  [0.24448775]\n",
      "Epoch:  6/250  Loss:  [0.23553979]\n",
      "Epoch:  7/250  Loss:  [0.22769773]\n",
      "Epoch:  8/250  Loss:  [0.22084165]\n",
      "Epoch:  9/250  Loss:  [0.21485797]\n",
      "Epoch:  10/250  Loss:  [0.20964097]\n",
      "Epoch:  11/250  Loss:  [0.20509364]\n",
      "Epoch:  12/250  Loss:  [0.20112811]\n",
      "Epoch:  13/250  Loss:  [0.19766572]\n",
      "Epoch:  14/250  Loss:  [0.19463677]\n",
      "Epoch:  15/250  Loss:  [0.19198016]\n",
      "Epoch:  16/250  Loss:  [0.18964271]\n",
      "Epoch:  17/250  Loss:  [0.1875785]\n",
      "Epoch:  18/250  Loss:  [0.18574805]\n",
      "Epoch:  19/250  Loss:  [0.18411764]\n",
      "Epoch:  20/250  Loss:  [0.18265848]\n",
      "Epoch:  21/250  Loss:  [0.18134613]\n",
      "Epoch:  22/250  Loss:  [0.18015982]\n",
      "Epoch:  23/250  Loss:  [0.17908194]\n",
      "Epoch:  24/250  Loss:  [0.17809755]\n",
      "Epoch:  25/250  Loss:  [0.17719397]\n",
      "Epoch:  26/250  Loss:  [0.17636045]\n",
      "Epoch:  27/250  Loss:  [0.17558786]\n",
      "Epoch:  28/250  Loss:  [0.17486842]\n",
      "Epoch:  29/250  Loss:  [0.17419551]\n",
      "Epoch:  30/250  Loss:  [0.1735635]\n",
      "Epoch:  31/250  Loss:  [0.17296755]\n",
      "Epoch:  32/250  Loss:  [0.17240354]\n",
      "Epoch:  33/250  Loss:  [0.17186794]\n",
      "Epoch:  34/250  Loss:  [0.17135769]\n",
      "Epoch:  35/250  Loss:  [0.17087019]\n",
      "Epoch:  36/250  Loss:  [0.17040317]\n",
      "Epoch:  37/250  Loss:  [0.1699547]\n",
      "Epoch:  38/250  Loss:  [0.16952307]\n",
      "Epoch:  39/250  Loss:  [0.16910683]\n",
      "Epoch:  40/250  Loss:  [0.16870469]\n",
      "Epoch:  41/250  Loss:  [0.16831554]\n",
      "Epoch:  42/250  Loss:  [0.16793841]\n",
      "Epoch:  43/250  Loss:  [0.16757244]\n",
      "Epoch:  44/250  Loss:  [0.16721689]\n",
      "Epoch:  45/250  Loss:  [0.16687107]\n",
      "Epoch:  46/250  Loss:  [0.16653442]\n",
      "Epoch:  47/250  Loss:  [0.1662064]\n",
      "Epoch:  48/250  Loss:  [0.16588656]\n",
      "Epoch:  49/250  Loss:  [0.16557446]\n",
      "Epoch:  50/250  Loss:  [0.16526974]\n",
      "Epoch:  51/250  Loss:  [0.16497207]\n",
      "Epoch:  52/250  Loss:  [0.16468112]\n",
      "Epoch:  53/250  Loss:  [0.16439662]\n",
      "Epoch:  54/250  Loss:  [0.16411833]\n",
      "Epoch:  55/250  Loss:  [0.16384599]\n",
      "Epoch:  56/250  Loss:  [0.16357941]\n",
      "Epoch:  57/250  Loss:  [0.16331837]\n",
      "Epoch:  58/250  Loss:  [0.16306269]\n",
      "Epoch:  59/250  Loss:  [0.1628122]\n",
      "Epoch:  60/250  Loss:  [0.16256673]\n",
      "Epoch:  61/250  Loss:  [0.16232614]\n",
      "Epoch:  62/250  Loss:  [0.16209027]\n",
      "Epoch:  63/250  Loss:  [0.16185899]\n",
      "Epoch:  64/250  Loss:  [0.16163217]\n",
      "Epoch:  65/250  Loss:  [0.1614097]\n",
      "Epoch:  66/250  Loss:  [0.16119144]\n",
      "Epoch:  67/250  Loss:  [0.16097729]\n",
      "Epoch:  68/250  Loss:  [0.16076714]\n",
      "Epoch:  69/250  Loss:  [0.16056088]\n",
      "Epoch:  70/250  Loss:  [0.16035843]\n",
      "Epoch:  71/250  Loss:  [0.16015967]\n",
      "Epoch:  72/250  Loss:  [0.15996452]\n",
      "Epoch:  73/250  Loss:  [0.15977289]\n",
      "Epoch:  74/250  Loss:  [0.1595847]\n",
      "Epoch:  75/250  Loss:  [0.15939985]\n",
      "Epoch:  76/250  Loss:  [0.15921828]\n",
      "Epoch:  77/250  Loss:  [0.15903989]\n",
      "Epoch:  78/250  Loss:  [0.15886462]\n",
      "Epoch:  79/250  Loss:  [0.15869239]\n",
      "Epoch:  80/250  Loss:  [0.15852313]\n",
      "Epoch:  81/250  Loss:  [0.15835677]\n",
      "Epoch:  82/250  Loss:  [0.15819325]\n",
      "Epoch:  83/250  Loss:  [0.15803249]\n",
      "Epoch:  84/250  Loss:  [0.15787444]\n",
      "Epoch:  85/250  Loss:  [0.15771904]\n",
      "Epoch:  86/250  Loss:  [0.15756622]\n",
      "Epoch:  87/250  Loss:  [0.15741592]\n",
      "Epoch:  88/250  Loss:  [0.15726809]\n",
      "Epoch:  89/250  Loss:  [0.15712268]\n",
      "Epoch:  90/250  Loss:  [0.15697963]\n",
      "Epoch:  91/250  Loss:  [0.15683889]\n",
      "Epoch:  92/250  Loss:  [0.15670041]\n",
      "Epoch:  93/250  Loss:  [0.15656413]\n",
      "Epoch:  94/250  Loss:  [0.15643002]\n",
      "Epoch:  95/250  Loss:  [0.15629802]\n",
      "Epoch:  96/250  Loss:  [0.15616809]\n",
      "Epoch:  97/250  Loss:  [0.15604019]\n",
      "Epoch:  98/250  Loss:  [0.15591426]\n",
      "Epoch:  99/250  Loss:  [0.15579028]\n",
      "Epoch:  100/250  Loss:  [0.1556682]\n",
      "Epoch:  101/250  Loss:  [0.15554797]\n",
      "Epoch:  102/250  Loss:  [0.15542957]\n",
      "Epoch:  103/250  Loss:  [0.15531295]\n",
      "Epoch:  104/250  Loss:  [0.15519807]\n",
      "Epoch:  105/250  Loss:  [0.15508491]\n",
      "Epoch:  106/250  Loss:  [0.15497342]\n",
      "Epoch:  107/250  Loss:  [0.15486357]\n",
      "Epoch:  108/250  Loss:  [0.15475533]\n",
      "Epoch:  109/250  Loss:  [0.15464867]\n",
      "Epoch:  110/250  Loss:  [0.15454355]\n",
      "Epoch:  111/250  Loss:  [0.15443995]\n",
      "Epoch:  112/250  Loss:  [0.15433783]\n",
      "Epoch:  113/250  Loss:  [0.15423716]\n",
      "Epoch:  114/250  Loss:  [0.15413792]\n",
      "Epoch:  115/250  Loss:  [0.15404008]\n",
      "Epoch:  116/250  Loss:  [0.1539436]\n",
      "Epoch:  117/250  Loss:  [0.15384848]\n",
      "Epoch:  118/250  Loss:  [0.15375467]\n",
      "Epoch:  119/250  Loss:  [0.15366215]\n",
      "Epoch:  120/250  Loss:  [0.1535709]\n",
      "Epoch:  121/250  Loss:  [0.1534809]\n",
      "Epoch:  122/250  Loss:  [0.15339211]\n",
      "Epoch:  123/250  Loss:  [0.15330453]\n",
      "Epoch:  124/250  Loss:  [0.15321812]\n",
      "Epoch:  125/250  Loss:  [0.15313286]\n",
      "Epoch:  126/250  Loss:  [0.15304873]\n",
      "Epoch:  127/250  Loss:  [0.15296572]\n",
      "Epoch:  128/250  Loss:  [0.1528838]\n",
      "Epoch:  129/250  Loss:  [0.15280294]\n",
      "Epoch:  130/250  Loss:  [0.15272314]\n",
      "Epoch:  131/250  Loss:  [0.15264437]\n",
      "Epoch:  132/250  Loss:  [0.15256661]\n",
      "Epoch:  133/250  Loss:  [0.15248984]\n",
      "Epoch:  134/250  Loss:  [0.15241405]\n",
      "Epoch:  135/250  Loss:  [0.15233922]\n",
      "Epoch:  136/250  Loss:  [0.15226534]\n",
      "Epoch:  137/250  Loss:  [0.15219237]\n",
      "Epoch:  138/250  Loss:  [0.15212032]\n",
      "Epoch:  139/250  Loss:  [0.15204916]\n",
      "Epoch:  140/250  Loss:  [0.15197887]\n",
      "Epoch:  141/250  Loss:  [0.15190945]\n",
      "Epoch:  142/250  Loss:  [0.15184087]\n",
      "Epoch:  143/250  Loss:  [0.15177313]\n",
      "Epoch:  144/250  Loss:  [0.1517062]\n",
      "Epoch:  145/250  Loss:  [0.15164007]\n",
      "Epoch:  146/250  Loss:  [0.15157473]\n",
      "Epoch:  147/250  Loss:  [0.15151017]\n",
      "Epoch:  148/250  Loss:  [0.15144637]\n",
      "Epoch:  149/250  Loss:  [0.15138331]\n",
      "Epoch:  150/250  Loss:  [0.151321]\n",
      "Epoch:  151/250  Loss:  [0.1512594]\n",
      "Epoch:  152/250  Loss:  [0.15119852]\n",
      "Epoch:  153/250  Loss:  [0.15113833]\n",
      "Epoch:  154/250  Loss:  [0.15107883]\n",
      "Epoch:  155/250  Loss:  [0.15102001]\n",
      "Epoch:  156/250  Loss:  [0.15096185]\n",
      "Epoch:  157/250  Loss:  [0.15090434]\n",
      "Epoch:  158/250  Loss:  [0.15084748]\n",
      "Epoch:  159/250  Loss:  [0.15079124]\n",
      "Epoch:  160/250  Loss:  [0.15073562]\n",
      "Epoch:  161/250  Loss:  [0.15068062]\n",
      "Epoch:  162/250  Loss:  [0.15062621]\n",
      "Epoch:  163/250  Loss:  [0.15057239]\n",
      "Epoch:  164/250  Loss:  [0.15051915]\n",
      "Epoch:  165/250  Loss:  [0.15046648]\n",
      "Epoch:  166/250  Loss:  [0.15041438]\n",
      "Epoch:  167/250  Loss:  [0.15036282]\n",
      "Epoch:  168/250  Loss:  [0.1503118]\n",
      "Epoch:  169/250  Loss:  [0.15026131]\n",
      "Epoch:  170/250  Loss:  [0.15021135]\n",
      "Epoch:  171/250  Loss:  [0.15016191]\n",
      "Epoch:  172/250  Loss:  [0.15011297]\n",
      "Epoch:  173/250  Loss:  [0.15006452]\n",
      "Epoch:  174/250  Loss:  [0.15001657]\n",
      "Epoch:  175/250  Loss:  [0.1499691]\n",
      "Epoch:  176/250  Loss:  [0.1499221]\n",
      "Epoch:  177/250  Loss:  [0.14987557]\n",
      "Epoch:  178/250  Loss:  [0.14982949]\n",
      "Epoch:  179/250  Loss:  [0.14978387]\n",
      "Epoch:  180/250  Loss:  [0.14973868]\n",
      "Epoch:  181/250  Loss:  [0.14969393]\n",
      "Epoch:  182/250  Loss:  [0.14964961]\n",
      "Epoch:  183/250  Loss:  [0.14960571]\n",
      "Epoch:  184/250  Loss:  [0.14956222]\n",
      "Epoch:  185/250  Loss:  [0.14951914]\n",
      "Epoch:  186/250  Loss:  [0.14947646]\n",
      "Epoch:  187/250  Loss:  [0.14943417]\n",
      "Epoch:  188/250  Loss:  [0.14939226]\n",
      "Epoch:  189/250  Loss:  [0.14935074]\n",
      "Epoch:  190/250  Loss:  [0.14930959]\n",
      "Epoch:  191/250  Loss:  [0.1492688]\n",
      "Epoch:  192/250  Loss:  [0.14922838]\n",
      "Epoch:  193/250  Loss:  [0.1491883]\n",
      "Epoch:  194/250  Loss:  [0.14914858]\n",
      "Epoch:  195/250  Loss:  [0.1491092]\n",
      "Epoch:  196/250  Loss:  [0.14907015]\n",
      "Epoch:  197/250  Loss:  [0.14903143]\n",
      "Epoch:  198/250  Loss:  [0.14899304]\n",
      "Epoch:  199/250  Loss:  [0.14895497]\n",
      "Epoch:  200/250  Loss:  [0.1489172]\n",
      "Epoch:  201/250  Loss:  [0.14887975]\n",
      "Epoch:  202/250  Loss:  [0.14884259]\n",
      "Epoch:  203/250  Loss:  [0.14880573]\n",
      "Epoch:  204/250  Loss:  [0.14876916]\n",
      "Epoch:  205/250  Loss:  [0.14873288]\n",
      "Epoch:  206/250  Loss:  [0.14869688]\n",
      "Epoch:  207/250  Loss:  [0.14866115]\n",
      "Epoch:  208/250  Loss:  [0.14862569]\n",
      "Epoch:  209/250  Loss:  [0.14859049]\n",
      "Epoch:  210/250  Loss:  [0.14855555]\n",
      "Epoch:  211/250  Loss:  [0.14852087]\n",
      "Epoch:  212/250  Loss:  [0.14848644]\n",
      "Epoch:  213/250  Loss:  [0.14845225]\n",
      "Epoch:  214/250  Loss:  [0.1484183]\n",
      "Epoch:  215/250  Loss:  [0.14838458]\n",
      "Epoch:  216/250  Loss:  [0.1483511]\n",
      "Epoch:  217/250  Loss:  [0.14831783]\n",
      "Epoch:  218/250  Loss:  [0.14828479]\n",
      "Epoch:  219/250  Loss:  [0.14825197]\n",
      "Epoch:  220/250  Loss:  [0.14821936]\n",
      "Epoch:  221/250  Loss:  [0.14818695]\n",
      "Epoch:  222/250  Loss:  [0.14815475]\n",
      "Epoch:  223/250  Loss:  [0.14812274]\n",
      "Epoch:  224/250  Loss:  [0.14809093]\n",
      "Epoch:  225/250  Loss:  [0.14805931]\n",
      "Epoch:  226/250  Loss:  [0.14802787]\n",
      "Epoch:  227/250  Loss:  [0.14799661]\n",
      "Epoch:  228/250  Loss:  [0.14796553]\n",
      "Epoch:  229/250  Loss:  [0.14793461]\n",
      "Epoch:  230/250  Loss:  [0.14790387]\n",
      "Epoch:  231/250  Loss:  [0.14787329]\n",
      "Epoch:  232/250  Loss:  [0.14784287]\n",
      "Epoch:  233/250  Loss:  [0.1478126]\n",
      "Epoch:  234/250  Loss:  [0.14778249]\n",
      "Epoch:  235/250  Loss:  [0.14775252]\n",
      "Epoch:  236/250  Loss:  [0.14772269]\n",
      "Epoch:  237/250  Loss:  [0.147693]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  238/250  Loss:  [0.14766345]\n",
      "Epoch:  239/250  Loss:  [0.14763402]\n",
      "Epoch:  240/250  Loss:  [0.14760473]\n",
      "Epoch:  241/250  Loss:  [0.14757555]\n",
      "Epoch:  242/250  Loss:  [0.14754649]\n",
      "Epoch:  243/250  Loss:  [0.14751755]\n",
      "Epoch:  244/250  Loss:  [0.14748872]\n",
      "Epoch:  245/250  Loss:  [0.14746]\n",
      "Epoch:  246/250  Loss:  [0.14743137]\n",
      "Epoch:  247/250  Loss:  [0.14740285]\n",
      "Epoch:  248/250  Loss:  [0.14737442]\n",
      "Epoch:  249/250  Loss:  [0.14734608]\n",
      "Epoch:  250/250  Loss:  [0.14731783]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 250 # number of itrations\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.6746005 ]\n",
      " [0.80680235]\n",
      " [0.77660176]\n",
      " [0.78777763]]\n",
      "[[1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
