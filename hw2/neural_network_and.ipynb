{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the back-propagation algorithm to learn the weights of a perceptron with 2 input nodes, 2 hidden nodes and 1 output node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **Python3** in used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters - input all parameter values here\n",
    "input_dim = 2\n",
    "hidden_dim = 2 # dimensions of hidden layers\n",
    "std = 0.01  # train data noise standard deviation\n",
    "w_std = 0.3\n",
    "learn_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "x_inputs = np.array([np.zeros(2), np.ones(2), np.array([1,0]), np.array([0,1])])\n",
    "def generate_trainset(N):\n",
    "    X = np.repeat(x_inputs, N//4, axis=0)\n",
    "    y_xor = np.logical_xor(X.T[0], X.T[1]).astype(np.float)\n",
    "    # add noise to data\n",
    "    X += np.random.normal(0, std, X.shape)\n",
    "    y_xor += np.random.normal(0, std, N)\n",
    "    # shuffle the training data\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    x_train, y_train = X[indices], y_xor[indices]\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid( t):\n",
    "    return 1/(1  + np.exp(-t))\n",
    "\n",
    "def dsigmoid( t):\n",
    "    return sigmoid(t)*(1 - sigmoid(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Experiment with N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "x_train, y_train = generate_trainset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/300  Loss:  [0.25105842]\n",
      "Epoch:  2/300  Loss:  [0.25026549]\n",
      "Epoch:  3/300  Loss:  [0.25018358]\n",
      "Epoch:  4/300  Loss:  [0.2501714]\n",
      "Epoch:  5/300  Loss:  [0.25016727]\n",
      "Epoch:  6/300  Loss:  [0.25016445]\n",
      "Epoch:  7/300  Loss:  [0.25016199]\n",
      "Epoch:  8/300  Loss:  [0.25015966]\n",
      "Epoch:  9/300  Loss:  [0.25015743]\n",
      "Epoch:  10/300  Loss:  [0.25015528]\n",
      "Epoch:  11/300  Loss:  [0.25015318]\n",
      "Epoch:  12/300  Loss:  [0.25015115]\n",
      "Epoch:  13/300  Loss:  [0.25014916]\n",
      "Epoch:  14/300  Loss:  [0.25014723]\n",
      "Epoch:  15/300  Loss:  [0.25014534]\n",
      "Epoch:  16/300  Loss:  [0.25014349]\n",
      "Epoch:  17/300  Loss:  [0.25014169]\n",
      "Epoch:  18/300  Loss:  [0.25013992]\n",
      "Epoch:  19/300  Loss:  [0.25013818]\n",
      "Epoch:  20/300  Loss:  [0.25013647]\n",
      "Epoch:  21/300  Loss:  [0.25013479]\n",
      "Epoch:  22/300  Loss:  [0.25013314]\n",
      "Epoch:  23/300  Loss:  [0.25013152]\n",
      "Epoch:  24/300  Loss:  [0.25012991]\n",
      "Epoch:  25/300  Loss:  [0.25012833]\n",
      "Epoch:  26/300  Loss:  [0.25012677]\n",
      "Epoch:  27/300  Loss:  [0.25012522]\n",
      "Epoch:  28/300  Loss:  [0.25012369]\n",
      "Epoch:  29/300  Loss:  [0.25012217]\n",
      "Epoch:  30/300  Loss:  [0.25012067]\n",
      "Epoch:  31/300  Loss:  [0.25011918]\n",
      "Epoch:  32/300  Loss:  [0.2501177]\n",
      "Epoch:  33/300  Loss:  [0.25011623]\n",
      "Epoch:  34/300  Loss:  [0.25011477]\n",
      "Epoch:  35/300  Loss:  [0.25011332]\n",
      "Epoch:  36/300  Loss:  [0.25011188]\n",
      "Epoch:  37/300  Loss:  [0.25011045]\n",
      "Epoch:  38/300  Loss:  [0.25010902]\n",
      "Epoch:  39/300  Loss:  [0.25010759]\n",
      "Epoch:  40/300  Loss:  [0.25010618]\n",
      "Epoch:  41/300  Loss:  [0.25010477]\n",
      "Epoch:  42/300  Loss:  [0.25010336]\n",
      "Epoch:  43/300  Loss:  [0.25010196]\n",
      "Epoch:  44/300  Loss:  [0.25010056]\n",
      "Epoch:  45/300  Loss:  [0.25009917]\n",
      "Epoch:  46/300  Loss:  [0.25009778]\n",
      "Epoch:  47/300  Loss:  [0.2500964]\n",
      "Epoch:  48/300  Loss:  [0.25009503]\n",
      "Epoch:  49/300  Loss:  [0.25009365]\n",
      "Epoch:  50/300  Loss:  [0.25009229]\n",
      "Epoch:  51/300  Loss:  [0.25009093]\n",
      "Epoch:  52/300  Loss:  [0.25008958]\n",
      "Epoch:  53/300  Loss:  [0.25008823]\n",
      "Epoch:  54/300  Loss:  [0.25008689]\n",
      "Epoch:  55/300  Loss:  [0.25008556]\n",
      "Epoch:  56/300  Loss:  [0.25008423]\n",
      "Epoch:  57/300  Loss:  [0.25008292]\n",
      "Epoch:  58/300  Loss:  [0.25008161]\n",
      "Epoch:  59/300  Loss:  [0.25008032]\n",
      "Epoch:  60/300  Loss:  [0.25007903]\n",
      "Epoch:  61/300  Loss:  [0.25007776]\n",
      "Epoch:  62/300  Loss:  [0.2500765]\n",
      "Epoch:  63/300  Loss:  [0.25007526]\n",
      "Epoch:  64/300  Loss:  [0.25007402]\n",
      "Epoch:  65/300  Loss:  [0.25007281]\n",
      "Epoch:  66/300  Loss:  [0.25007161]\n",
      "Epoch:  67/300  Loss:  [0.25007043]\n",
      "Epoch:  68/300  Loss:  [0.25006927]\n",
      "Epoch:  69/300  Loss:  [0.25006812]\n",
      "Epoch:  70/300  Loss:  [0.250067]\n",
      "Epoch:  71/300  Loss:  [0.2500659]\n",
      "Epoch:  72/300  Loss:  [0.25006482]\n",
      "Epoch:  73/300  Loss:  [0.25006377]\n",
      "Epoch:  74/300  Loss:  [0.25006274]\n",
      "Epoch:  75/300  Loss:  [0.25006173]\n",
      "Epoch:  76/300  Loss:  [0.25006075]\n",
      "Epoch:  77/300  Loss:  [0.25005979]\n",
      "Epoch:  78/300  Loss:  [0.25005887]\n",
      "Epoch:  79/300  Loss:  [0.25005797]\n",
      "Epoch:  80/300  Loss:  [0.25005709]\n",
      "Epoch:  81/300  Loss:  [0.25005625]\n",
      "Epoch:  82/300  Loss:  [0.25005543]\n",
      "Epoch:  83/300  Loss:  [0.25005465]\n",
      "Epoch:  84/300  Loss:  [0.25005389]\n",
      "Epoch:  85/300  Loss:  [0.25005316]\n",
      "Epoch:  86/300  Loss:  [0.25005246]\n",
      "Epoch:  87/300  Loss:  [0.25005178]\n",
      "Epoch:  88/300  Loss:  [0.25005114]\n",
      "Epoch:  89/300  Loss:  [0.25005051]\n",
      "Epoch:  90/300  Loss:  [0.25004992]\n",
      "Epoch:  91/300  Loss:  [0.25004934]\n",
      "Epoch:  92/300  Loss:  [0.25004879]\n",
      "Epoch:  93/300  Loss:  [0.25004826]\n",
      "Epoch:  94/300  Loss:  [0.25004775]\n",
      "Epoch:  95/300  Loss:  [0.25004725]\n",
      "Epoch:  96/300  Loss:  [0.25004676]\n",
      "Epoch:  97/300  Loss:  [0.25004628]\n",
      "Epoch:  98/300  Loss:  [0.25004581]\n",
      "Epoch:  99/300  Loss:  [0.25004534]\n",
      "Epoch:  100/300  Loss:  [0.25004487]\n",
      "Epoch:  101/300  Loss:  [0.2500444]\n",
      "Epoch:  102/300  Loss:  [0.25004391]\n",
      "Epoch:  103/300  Loss:  [0.25004341]\n",
      "Epoch:  104/300  Loss:  [0.25004288]\n",
      "Epoch:  105/300  Loss:  [0.25004233]\n",
      "Epoch:  106/300  Loss:  [0.25004174]\n",
      "Epoch:  107/300  Loss:  [0.25004111]\n",
      "Epoch:  108/300  Loss:  [0.25004043]\n",
      "Epoch:  109/300  Loss:  [0.25003969]\n",
      "Epoch:  110/300  Loss:  [0.25003888]\n",
      "Epoch:  111/300  Loss:  [0.25003799]\n",
      "Epoch:  112/300  Loss:  [0.25003701]\n",
      "Epoch:  113/300  Loss:  [0.25003593]\n",
      "Epoch:  114/300  Loss:  [0.25003474]\n",
      "Epoch:  115/300  Loss:  [0.25003343]\n",
      "Epoch:  116/300  Loss:  [0.25003197]\n",
      "Epoch:  117/300  Loss:  [0.25003035]\n",
      "Epoch:  118/300  Loss:  [0.25002856]\n",
      "Epoch:  119/300  Loss:  [0.25002658]\n",
      "Epoch:  120/300  Loss:  [0.25002438]\n",
      "Epoch:  121/300  Loss:  [0.25002195]\n",
      "Epoch:  122/300  Loss:  [0.25001926]\n",
      "Epoch:  123/300  Loss:  [0.25001628]\n",
      "Epoch:  124/300  Loss:  [0.250013]\n",
      "Epoch:  125/300  Loss:  [0.25000937]\n",
      "Epoch:  126/300  Loss:  [0.25000536]\n",
      "Epoch:  127/300  Loss:  [0.25000094]\n",
      "Epoch:  128/300  Loss:  [0.24999607]\n",
      "Epoch:  129/300  Loss:  [0.2499907]\n",
      "Epoch:  130/300  Loss:  [0.24998478]\n",
      "Epoch:  131/300  Loss:  [0.24997826]\n",
      "Epoch:  132/300  Loss:  [0.24997107]\n",
      "Epoch:  133/300  Loss:  [0.24996316]\n",
      "Epoch:  134/300  Loss:  [0.24995445]\n",
      "Epoch:  135/300  Loss:  [0.24994485]\n",
      "Epoch:  136/300  Loss:  [0.24993427]\n",
      "Epoch:  137/300  Loss:  [0.24992262]\n",
      "Epoch:  138/300  Loss:  [0.24990977]\n",
      "Epoch:  139/300  Loss:  [0.24989559]\n",
      "Epoch:  140/300  Loss:  [0.24987994]\n",
      "Epoch:  141/300  Loss:  [0.24986265]\n",
      "Epoch:  142/300  Loss:  [0.24984354]\n",
      "Epoch:  143/300  Loss:  [0.24982239]\n",
      "Epoch:  144/300  Loss:  [0.24979895]\n",
      "Epoch:  145/300  Loss:  [0.24977295]\n",
      "Epoch:  146/300  Loss:  [0.24974407]\n",
      "Epoch:  147/300  Loss:  [0.24971195]\n",
      "Epoch:  148/300  Loss:  [0.24967617]\n",
      "Epoch:  149/300  Loss:  [0.24963623]\n",
      "Epoch:  150/300  Loss:  [0.24959159]\n",
      "Epoch:  151/300  Loss:  [0.24954158]\n",
      "Epoch:  152/300  Loss:  [0.24948546]\n",
      "Epoch:  153/300  Loss:  [0.24942233]\n",
      "Epoch:  154/300  Loss:  [0.24935118]\n",
      "Epoch:  155/300  Loss:  [0.24927078]\n",
      "Epoch:  156/300  Loss:  [0.24917973]\n",
      "Epoch:  157/300  Loss:  [0.24907635]\n",
      "Epoch:  158/300  Loss:  [0.24895868]\n",
      "Epoch:  159/300  Loss:  [0.24882441]\n",
      "Epoch:  160/300  Loss:  [0.24867079]\n",
      "Epoch:  161/300  Loss:  [0.2484946]\n",
      "Epoch:  162/300  Loss:  [0.24829202]\n",
      "Epoch:  163/300  Loss:  [0.24805858]\n",
      "Epoch:  164/300  Loss:  [0.24778898]\n",
      "Epoch:  165/300  Loss:  [0.24747705]\n",
      "Epoch:  166/300  Loss:  [0.24711558]\n",
      "Epoch:  167/300  Loss:  [0.24669624]\n",
      "Epoch:  168/300  Loss:  [0.24620946]\n",
      "Epoch:  169/300  Loss:  [0.24564445]\n",
      "Epoch:  170/300  Loss:  [0.24498916]\n",
      "Epoch:  171/300  Loss:  [0.24423046]\n",
      "Epoch:  172/300  Loss:  [0.24335439]\n",
      "Epoch:  173/300  Loss:  [0.24234665]\n",
      "Epoch:  174/300  Loss:  [0.24119316]\n",
      "Epoch:  175/300  Loss:  [0.23988099]\n",
      "Epoch:  176/300  Loss:  [0.23839928]\n",
      "Epoch:  177/300  Loss:  [0.23674028]\n",
      "Epoch:  178/300  Loss:  [0.23490036]\n",
      "Epoch:  179/300  Loss:  [0.23288072]\n",
      "Epoch:  180/300  Loss:  [0.23068782]\n",
      "Epoch:  181/300  Loss:  [0.22833326]\n",
      "Epoch:  182/300  Loss:  [0.22583329]\n",
      "Epoch:  183/300  Loss:  [0.22320783]\n",
      "Epoch:  184/300  Loss:  [0.22047933]\n",
      "Epoch:  185/300  Loss:  [0.21767152]\n",
      "Epoch:  186/300  Loss:  [0.21480826]\n",
      "Epoch:  187/300  Loss:  [0.21191259]\n",
      "Epoch:  188/300  Loss:  [0.20900606]\n",
      "Epoch:  189/300  Loss:  [0.20610822]\n",
      "Epoch:  190/300  Loss:  [0.20323638]\n",
      "Epoch:  191/300  Loss:  [0.2004054]\n",
      "Epoch:  192/300  Loss:  [0.19762769]\n",
      "Epoch:  193/300  Loss:  [0.19491324]\n",
      "Epoch:  194/300  Loss:  [0.19226969]\n",
      "Epoch:  195/300  Loss:  [0.18970256]\n",
      "Epoch:  196/300  Loss:  [0.18721546]\n",
      "Epoch:  197/300  Loss:  [0.18481035]\n",
      "Epoch:  198/300  Loss:  [0.18248785]\n",
      "Epoch:  199/300  Loss:  [0.18024746]\n",
      "Epoch:  200/300  Loss:  [0.17808784]\n",
      "Epoch:  201/300  Loss:  [0.17600699]\n",
      "Epoch:  202/300  Loss:  [0.17400247]\n",
      "Epoch:  203/300  Loss:  [0.17207151]\n",
      "Epoch:  204/300  Loss:  [0.17021114]\n",
      "Epoch:  205/300  Loss:  [0.16841828]\n",
      "Epoch:  206/300  Loss:  [0.16668984]\n",
      "Epoch:  207/300  Loss:  [0.16502272]\n",
      "Epoch:  208/300  Loss:  [0.1634139]\n",
      "Epoch:  209/300  Loss:  [0.16186047]\n",
      "Epoch:  210/300  Loss:  [0.16035962]\n",
      "Epoch:  211/300  Loss:  [0.15890869]\n",
      "Epoch:  212/300  Loss:  [0.15750517]\n",
      "Epoch:  213/300  Loss:  [0.1561467]\n",
      "Epoch:  214/300  Loss:  [0.15483107]\n",
      "Epoch:  215/300  Loss:  [0.15355622]\n",
      "Epoch:  216/300  Loss:  [0.15232022]\n",
      "Epoch:  217/300  Loss:  [0.15112128]\n",
      "Epoch:  218/300  Loss:  [0.14995776]\n",
      "Epoch:  219/300  Loss:  [0.14882809]\n",
      "Epoch:  220/300  Loss:  [0.14773082]\n",
      "Epoch:  221/300  Loss:  [0.14666461]\n",
      "Epoch:  222/300  Loss:  [0.14562817]\n",
      "Epoch:  223/300  Loss:  [0.1446203]\n",
      "Epoch:  224/300  Loss:  [0.14363985]\n",
      "Epoch:  225/300  Loss:  [0.14268575]\n",
      "Epoch:  226/300  Loss:  [0.14175694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  227/300  Loss:  [0.14085244]\n",
      "Epoch:  228/300  Loss:  [0.13997129]\n",
      "Epoch:  229/300  Loss:  [0.13911257]\n",
      "Epoch:  230/300  Loss:  [0.13827539]\n",
      "Epoch:  231/300  Loss:  [0.13745891]\n",
      "Epoch:  232/300  Loss:  [0.1366623]\n",
      "Epoch:  233/300  Loss:  [0.13588479]\n",
      "Epoch:  234/300  Loss:  [0.13512561]\n",
      "Epoch:  235/300  Loss:  [0.13438404]\n",
      "Epoch:  236/300  Loss:  [0.13365939]\n",
      "Epoch:  237/300  Loss:  [0.13295099]\n",
      "Epoch:  238/300  Loss:  [0.1322582]\n",
      "Epoch:  239/300  Loss:  [0.13158043]\n",
      "Epoch:  240/300  Loss:  [0.13091709]\n",
      "Epoch:  241/300  Loss:  [0.13026763]\n",
      "Epoch:  242/300  Loss:  [0.12963153]\n",
      "Epoch:  243/300  Loss:  [0.1290083]\n",
      "Epoch:  244/300  Loss:  [0.12839746]\n",
      "Epoch:  245/300  Loss:  [0.12779855]\n",
      "Epoch:  246/300  Loss:  [0.12721116]\n",
      "Epoch:  247/300  Loss:  [0.12663487]\n",
      "Epoch:  248/300  Loss:  [0.12606929]\n",
      "Epoch:  249/300  Loss:  [0.12551406]\n",
      "Epoch:  250/300  Loss:  [0.12496883]\n",
      "Epoch:  251/300  Loss:  [0.12443326]\n",
      "Epoch:  252/300  Loss:  [0.12390703]\n",
      "Epoch:  253/300  Loss:  [0.12338984]\n",
      "Epoch:  254/300  Loss:  [0.12288139]\n",
      "Epoch:  255/300  Loss:  [0.12238142]\n",
      "Epoch:  256/300  Loss:  [0.12188965]\n",
      "Epoch:  257/300  Loss:  [0.12140583]\n",
      "Epoch:  258/300  Loss:  [0.12092972]\n",
      "Epoch:  259/300  Loss:  [0.12046108]\n",
      "Epoch:  260/300  Loss:  [0.1199997]\n",
      "Epoch:  261/300  Loss:  [0.11954536]\n",
      "Epoch:  262/300  Loss:  [0.11909785]\n",
      "Epoch:  263/300  Loss:  [0.11865698]\n",
      "Epoch:  264/300  Loss:  [0.11822257]\n",
      "Epoch:  265/300  Loss:  [0.11779442]\n",
      "Epoch:  266/300  Loss:  [0.11737237]\n",
      "Epoch:  267/300  Loss:  [0.11695625]\n",
      "Epoch:  268/300  Loss:  [0.1165459]\n",
      "Epoch:  269/300  Loss:  [0.11614116]\n",
      "Epoch:  270/300  Loss:  [0.11574189]\n",
      "Epoch:  271/300  Loss:  [0.11534794]\n",
      "Epoch:  272/300  Loss:  [0.11495917]\n",
      "Epoch:  273/300  Loss:  [0.11457546]\n",
      "Epoch:  274/300  Loss:  [0.11419667]\n",
      "Epoch:  275/300  Loss:  [0.11382268]\n",
      "Epoch:  276/300  Loss:  [0.11345337]\n",
      "Epoch:  277/300  Loss:  [0.11308863]\n",
      "Epoch:  278/300  Loss:  [0.11272834]\n",
      "Epoch:  279/300  Loss:  [0.11237241]\n",
      "Epoch:  280/300  Loss:  [0.11202071]\n",
      "Epoch:  281/300  Loss:  [0.11167317]\n",
      "Epoch:  282/300  Loss:  [0.11132967]\n",
      "Epoch:  283/300  Loss:  [0.11099013]\n",
      "Epoch:  284/300  Loss:  [0.11065446]\n",
      "Epoch:  285/300  Loss:  [0.11032256]\n",
      "Epoch:  286/300  Loss:  [0.10999436]\n",
      "Epoch:  287/300  Loss:  [0.10966978]\n",
      "Epoch:  288/300  Loss:  [0.10934872]\n",
      "Epoch:  289/300  Loss:  [0.10903113]\n",
      "Epoch:  290/300  Loss:  [0.10871692]\n",
      "Epoch:  291/300  Loss:  [0.10840602]\n",
      "Epoch:  292/300  Loss:  [0.10809837]\n",
      "Epoch:  293/300  Loss:  [0.10779389]\n",
      "Epoch:  294/300  Loss:  [0.10749252]\n",
      "Epoch:  295/300  Loss:  [0.10719419]\n",
      "Epoch:  296/300  Loss:  [0.10689886]\n",
      "Epoch:  297/300  Loss:  [0.10660644]\n",
      "Epoch:  298/300  Loss:  [0.1063169]\n",
      "Epoch:  299/300  Loss:  [0.10603017]\n",
      "Epoch:  300/300  Loss:  [0.10574619]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 300 # number of itrations\\\n",
    "# training def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12423437]\n",
      " [0.50977433]\n",
      " [0.72896377]\n",
      " [0.73034588]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Experiment with N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "x_train, y_train = generate_trainset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/500  Loss:  [0.25902529]\n",
      "Epoch:  2/500  Loss:  [0.25820643]\n",
      "Epoch:  3/500  Loss:  [0.25745343]\n",
      "Epoch:  4/500  Loss:  [0.25676196]\n",
      "Epoch:  5/500  Loss:  [0.25612787]\n",
      "Epoch:  6/500  Loss:  [0.2555472]\n",
      "Epoch:  7/500  Loss:  [0.25501614]\n",
      "Epoch:  8/500  Loss:  [0.2545311]\n",
      "Epoch:  9/500  Loss:  [0.25408867]\n",
      "Epoch:  10/500  Loss:  [0.25368561]\n",
      "Epoch:  11/500  Loss:  [0.2533189]\n",
      "Epoch:  12/500  Loss:  [0.25298567]\n",
      "Epoch:  13/500  Loss:  [0.25268326]\n",
      "Epoch:  14/500  Loss:  [0.25240917]\n",
      "Epoch:  15/500  Loss:  [0.25216107]\n",
      "Epoch:  16/500  Loss:  [0.25193679]\n",
      "Epoch:  17/500  Loss:  [0.25173432]\n",
      "Epoch:  18/500  Loss:  [0.25155178]\n",
      "Epoch:  19/500  Loss:  [0.25138745]\n",
      "Epoch:  20/500  Loss:  [0.25123974]\n",
      "Epoch:  21/500  Loss:  [0.25110717]\n",
      "Epoch:  22/500  Loss:  [0.25098838]\n",
      "Epoch:  23/500  Loss:  [0.25088212]\n",
      "Epoch:  24/500  Loss:  [0.25078725]\n",
      "Epoch:  25/500  Loss:  [0.25070271]\n",
      "Epoch:  26/500  Loss:  [0.25062755]\n",
      "Epoch:  27/500  Loss:  [0.25056087]\n",
      "Epoch:  28/500  Loss:  [0.25050186]\n",
      "Epoch:  29/500  Loss:  [0.25044979]\n",
      "Epoch:  30/500  Loss:  [0.25040399]\n",
      "Epoch:  31/500  Loss:  [0.25036384]\n",
      "Epoch:  32/500  Loss:  [0.25032878]\n",
      "Epoch:  33/500  Loss:  [0.2502983]\n",
      "Epoch:  34/500  Loss:  [0.25027194]\n",
      "Epoch:  35/500  Loss:  [0.25024927]\n",
      "Epoch:  36/500  Loss:  [0.25022992]\n",
      "Epoch:  37/500  Loss:  [0.25021353]\n",
      "Epoch:  38/500  Loss:  [0.2501998]\n",
      "Epoch:  39/500  Loss:  [0.25018843]\n",
      "Epoch:  40/500  Loss:  [0.25017918]\n",
      "Epoch:  41/500  Loss:  [0.2501718]\n",
      "Epoch:  42/500  Loss:  [0.25016609]\n",
      "Epoch:  43/500  Loss:  [0.25016185]\n",
      "Epoch:  44/500  Loss:  [0.25015892]\n",
      "Epoch:  45/500  Loss:  [0.25015715]\n",
      "Epoch:  46/500  Loss:  [0.25015638]\n",
      "Epoch:  47/500  Loss:  [0.2501565]\n",
      "Epoch:  48/500  Loss:  [0.2501574]\n",
      "Epoch:  49/500  Loss:  [0.25015898]\n",
      "Epoch:  50/500  Loss:  [0.25016114]\n",
      "Epoch:  51/500  Loss:  [0.2501638]\n",
      "Epoch:  52/500  Loss:  [0.2501669]\n",
      "Epoch:  53/500  Loss:  [0.25017036]\n",
      "Epoch:  54/500  Loss:  [0.25017413]\n",
      "Epoch:  55/500  Loss:  [0.25017817]\n",
      "Epoch:  56/500  Loss:  [0.25018242]\n",
      "Epoch:  57/500  Loss:  [0.25018684]\n",
      "Epoch:  58/500  Loss:  [0.2501914]\n",
      "Epoch:  59/500  Loss:  [0.25019607]\n",
      "Epoch:  60/500  Loss:  [0.25020082]\n",
      "Epoch:  61/500  Loss:  [0.25020563]\n",
      "Epoch:  62/500  Loss:  [0.25021047]\n",
      "Epoch:  63/500  Loss:  [0.25021533]\n",
      "Epoch:  64/500  Loss:  [0.25022019]\n",
      "Epoch:  65/500  Loss:  [0.25022503]\n",
      "Epoch:  66/500  Loss:  [0.25022985]\n",
      "Epoch:  67/500  Loss:  [0.25023464]\n",
      "Epoch:  68/500  Loss:  [0.25023938]\n",
      "Epoch:  69/500  Loss:  [0.25024407]\n",
      "Epoch:  70/500  Loss:  [0.2502487]\n",
      "Epoch:  71/500  Loss:  [0.25025327]\n",
      "Epoch:  72/500  Loss:  [0.25025778]\n",
      "Epoch:  73/500  Loss:  [0.25026222]\n",
      "Epoch:  74/500  Loss:  [0.25026659]\n",
      "Epoch:  75/500  Loss:  [0.25027089]\n",
      "Epoch:  76/500  Loss:  [0.25027511]\n",
      "Epoch:  77/500  Loss:  [0.25027926]\n",
      "Epoch:  78/500  Loss:  [0.25028334]\n",
      "Epoch:  79/500  Loss:  [0.25028735]\n",
      "Epoch:  80/500  Loss:  [0.25029128]\n",
      "Epoch:  81/500  Loss:  [0.25029514]\n",
      "Epoch:  82/500  Loss:  [0.25029893]\n",
      "Epoch:  83/500  Loss:  [0.25030266]\n",
      "Epoch:  84/500  Loss:  [0.25030631]\n",
      "Epoch:  85/500  Loss:  [0.25030991]\n",
      "Epoch:  86/500  Loss:  [0.25031344]\n",
      "Epoch:  87/500  Loss:  [0.2503169]\n",
      "Epoch:  88/500  Loss:  [0.25032031]\n",
      "Epoch:  89/500  Loss:  [0.25032367]\n",
      "Epoch:  90/500  Loss:  [0.25032696]\n",
      "Epoch:  91/500  Loss:  [0.25033021]\n",
      "Epoch:  92/500  Loss:  [0.2503334]\n",
      "Epoch:  93/500  Loss:  [0.25033655]\n",
      "Epoch:  94/500  Loss:  [0.25033965]\n",
      "Epoch:  95/500  Loss:  [0.2503427]\n",
      "Epoch:  96/500  Loss:  [0.25034572]\n",
      "Epoch:  97/500  Loss:  [0.25034869]\n",
      "Epoch:  98/500  Loss:  [0.25035163]\n",
      "Epoch:  99/500  Loss:  [0.25035453]\n",
      "Epoch:  100/500  Loss:  [0.2503574]\n",
      "Epoch:  101/500  Loss:  [0.25036024]\n",
      "Epoch:  102/500  Loss:  [0.25036304]\n",
      "Epoch:  103/500  Loss:  [0.25036582]\n",
      "Epoch:  104/500  Loss:  [0.25036857]\n",
      "Epoch:  105/500  Loss:  [0.2503713]\n",
      "Epoch:  106/500  Loss:  [0.250374]\n",
      "Epoch:  107/500  Loss:  [0.25037668]\n",
      "Epoch:  108/500  Loss:  [0.25037935]\n",
      "Epoch:  109/500  Loss:  [0.25038199]\n",
      "Epoch:  110/500  Loss:  [0.25038462]\n",
      "Epoch:  111/500  Loss:  [0.25038723]\n",
      "Epoch:  112/500  Loss:  [0.25038983]\n",
      "Epoch:  113/500  Loss:  [0.25039242]\n",
      "Epoch:  114/500  Loss:  [0.25039499]\n",
      "Epoch:  115/500  Loss:  [0.25039756]\n",
      "Epoch:  116/500  Loss:  [0.25040011]\n",
      "Epoch:  117/500  Loss:  [0.25040266]\n",
      "Epoch:  118/500  Loss:  [0.2504052]\n",
      "Epoch:  119/500  Loss:  [0.25040773]\n",
      "Epoch:  120/500  Loss:  [0.25041026]\n",
      "Epoch:  121/500  Loss:  [0.25041278]\n",
      "Epoch:  122/500  Loss:  [0.2504153]\n",
      "Epoch:  123/500  Loss:  [0.25041782]\n",
      "Epoch:  124/500  Loss:  [0.25042034]\n",
      "Epoch:  125/500  Loss:  [0.25042285]\n",
      "Epoch:  126/500  Loss:  [0.25042537]\n",
      "Epoch:  127/500  Loss:  [0.25042788]\n",
      "Epoch:  128/500  Loss:  [0.2504304]\n",
      "Epoch:  129/500  Loss:  [0.25043292]\n",
      "Epoch:  130/500  Loss:  [0.25043544]\n",
      "Epoch:  131/500  Loss:  [0.25043796]\n",
      "Epoch:  132/500  Loss:  [0.25044048]\n",
      "Epoch:  133/500  Loss:  [0.25044301]\n",
      "Epoch:  134/500  Loss:  [0.25044555]\n",
      "Epoch:  135/500  Loss:  [0.25044808]\n",
      "Epoch:  136/500  Loss:  [0.25045063]\n",
      "Epoch:  137/500  Loss:  [0.25045318]\n",
      "Epoch:  138/500  Loss:  [0.25045573]\n",
      "Epoch:  139/500  Loss:  [0.25045829]\n",
      "Epoch:  140/500  Loss:  [0.25046085]\n",
      "Epoch:  141/500  Loss:  [0.25046342]\n",
      "Epoch:  142/500  Loss:  [0.250466]\n",
      "Epoch:  143/500  Loss:  [0.25046859]\n",
      "Epoch:  144/500  Loss:  [0.25047118]\n",
      "Epoch:  145/500  Loss:  [0.25047378]\n",
      "Epoch:  146/500  Loss:  [0.25047638]\n",
      "Epoch:  147/500  Loss:  [0.250479]\n",
      "Epoch:  148/500  Loss:  [0.25048162]\n",
      "Epoch:  149/500  Loss:  [0.25048425]\n",
      "Epoch:  150/500  Loss:  [0.25048688]\n",
      "Epoch:  151/500  Loss:  [0.25048953]\n",
      "Epoch:  152/500  Loss:  [0.25049218]\n",
      "Epoch:  153/500  Loss:  [0.25049484]\n",
      "Epoch:  154/500  Loss:  [0.25049751]\n",
      "Epoch:  155/500  Loss:  [0.25050018]\n",
      "Epoch:  156/500  Loss:  [0.25050287]\n",
      "Epoch:  157/500  Loss:  [0.25050556]\n",
      "Epoch:  158/500  Loss:  [0.25050826]\n",
      "Epoch:  159/500  Loss:  [0.25051097]\n",
      "Epoch:  160/500  Loss:  [0.25051368]\n",
      "Epoch:  161/500  Loss:  [0.2505164]\n",
      "Epoch:  162/500  Loss:  [0.25051913]\n",
      "Epoch:  163/500  Loss:  [0.25052187]\n",
      "Epoch:  164/500  Loss:  [0.25052461]\n",
      "Epoch:  165/500  Loss:  [0.25052737]\n",
      "Epoch:  166/500  Loss:  [0.25053012]\n",
      "Epoch:  167/500  Loss:  [0.25053289]\n",
      "Epoch:  168/500  Loss:  [0.25053566]\n",
      "Epoch:  169/500  Loss:  [0.25053844]\n",
      "Epoch:  170/500  Loss:  [0.25054123]\n",
      "Epoch:  171/500  Loss:  [0.25054402]\n",
      "Epoch:  172/500  Loss:  [0.25054682]\n",
      "Epoch:  173/500  Loss:  [0.25054962]\n",
      "Epoch:  174/500  Loss:  [0.25055243]\n",
      "Epoch:  175/500  Loss:  [0.25055524]\n",
      "Epoch:  176/500  Loss:  [0.25055806]\n",
      "Epoch:  177/500  Loss:  [0.25056089]\n",
      "Epoch:  178/500  Loss:  [0.25056372]\n",
      "Epoch:  179/500  Loss:  [0.25056655]\n",
      "Epoch:  180/500  Loss:  [0.25056939]\n",
      "Epoch:  181/500  Loss:  [0.25057223]\n",
      "Epoch:  182/500  Loss:  [0.25057508]\n",
      "Epoch:  183/500  Loss:  [0.25057793]\n",
      "Epoch:  184/500  Loss:  [0.25058078]\n",
      "Epoch:  185/500  Loss:  [0.25058364]\n",
      "Epoch:  186/500  Loss:  [0.2505865]\n",
      "Epoch:  187/500  Loss:  [0.25058936]\n",
      "Epoch:  188/500  Loss:  [0.25059223]\n",
      "Epoch:  189/500  Loss:  [0.25059509]\n",
      "Epoch:  190/500  Loss:  [0.25059796]\n",
      "Epoch:  191/500  Loss:  [0.25060083]\n",
      "Epoch:  192/500  Loss:  [0.2506037]\n",
      "Epoch:  193/500  Loss:  [0.25060657]\n",
      "Epoch:  194/500  Loss:  [0.25060944]\n",
      "Epoch:  195/500  Loss:  [0.25061231]\n",
      "Epoch:  196/500  Loss:  [0.25061518]\n",
      "Epoch:  197/500  Loss:  [0.25061805]\n",
      "Epoch:  198/500  Loss:  [0.25062092]\n",
      "Epoch:  199/500  Loss:  [0.25062378]\n",
      "Epoch:  200/500  Loss:  [0.25062665]\n",
      "Epoch:  201/500  Loss:  [0.25062951]\n",
      "Epoch:  202/500  Loss:  [0.25063238]\n",
      "Epoch:  203/500  Loss:  [0.25063524]\n",
      "Epoch:  204/500  Loss:  [0.25063809]\n",
      "Epoch:  205/500  Loss:  [0.25064095]\n",
      "Epoch:  206/500  Loss:  [0.2506438]\n",
      "Epoch:  207/500  Loss:  [0.25064664]\n",
      "Epoch:  208/500  Loss:  [0.25064948]\n",
      "Epoch:  209/500  Loss:  [0.25065232]\n",
      "Epoch:  210/500  Loss:  [0.25065515]\n",
      "Epoch:  211/500  Loss:  [0.25065798]\n",
      "Epoch:  212/500  Loss:  [0.2506608]\n",
      "Epoch:  213/500  Loss:  [0.25066361]\n",
      "Epoch:  214/500  Loss:  [0.25066642]\n",
      "Epoch:  215/500  Loss:  [0.25066922]\n",
      "Epoch:  216/500  Loss:  [0.25067202]\n",
      "Epoch:  217/500  Loss:  [0.25067481]\n",
      "Epoch:  218/500  Loss:  [0.25067759]\n",
      "Epoch:  219/500  Loss:  [0.25068036]\n",
      "Epoch:  220/500  Loss:  [0.25068312]\n",
      "Epoch:  221/500  Loss:  [0.25068587]\n",
      "Epoch:  222/500  Loss:  [0.25068862]\n",
      "Epoch:  223/500  Loss:  [0.25069135]\n",
      "Epoch:  224/500  Loss:  [0.25069408]\n",
      "Epoch:  225/500  Loss:  [0.25069679]\n",
      "Epoch:  226/500  Loss:  [0.2506995]\n",
      "Epoch:  227/500  Loss:  [0.25070219]\n",
      "Epoch:  228/500  Loss:  [0.25070487]\n",
      "Epoch:  229/500  Loss:  [0.25070755]\n",
      "Epoch:  230/500  Loss:  [0.2507102]\n",
      "Epoch:  231/500  Loss:  [0.25071285]\n",
      "Epoch:  232/500  Loss:  [0.25071548]\n",
      "Epoch:  233/500  Loss:  [0.25071811]\n",
      "Epoch:  234/500  Loss:  [0.25072071]\n",
      "Epoch:  235/500  Loss:  [0.25072331]\n",
      "Epoch:  236/500  Loss:  [0.25072589]\n",
      "Epoch:  237/500  Loss:  [0.25072845]\n",
      "Epoch:  238/500  Loss:  [0.250731]\n",
      "Epoch:  239/500  Loss:  [0.25073354]\n",
      "Epoch:  240/500  Loss:  [0.25073606]\n",
      "Epoch:  241/500  Loss:  [0.25073856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  242/500  Loss:  [0.25074105]\n",
      "Epoch:  243/500  Loss:  [0.25074353]\n",
      "Epoch:  244/500  Loss:  [0.25074598]\n",
      "Epoch:  245/500  Loss:  [0.25074842]\n",
      "Epoch:  246/500  Loss:  [0.25075084]\n",
      "Epoch:  247/500  Loss:  [0.25075325]\n",
      "Epoch:  248/500  Loss:  [0.25075564]\n",
      "Epoch:  249/500  Loss:  [0.25075801]\n",
      "Epoch:  250/500  Loss:  [0.25076036]\n",
      "Epoch:  251/500  Loss:  [0.25076269]\n",
      "Epoch:  252/500  Loss:  [0.250765]\n",
      "Epoch:  253/500  Loss:  [0.25076729]\n",
      "Epoch:  254/500  Loss:  [0.25076957]\n",
      "Epoch:  255/500  Loss:  [0.25077182]\n",
      "Epoch:  256/500  Loss:  [0.25077406]\n",
      "Epoch:  257/500  Loss:  [0.25077627]\n",
      "Epoch:  258/500  Loss:  [0.25077847]\n",
      "Epoch:  259/500  Loss:  [0.25078064]\n",
      "Epoch:  260/500  Loss:  [0.25078279]\n",
      "Epoch:  261/500  Loss:  [0.25078492]\n",
      "Epoch:  262/500  Loss:  [0.25078703]\n",
      "Epoch:  263/500  Loss:  [0.25078912]\n",
      "Epoch:  264/500  Loss:  [0.25079118]\n",
      "Epoch:  265/500  Loss:  [0.25079323]\n",
      "Epoch:  266/500  Loss:  [0.25079525]\n",
      "Epoch:  267/500  Loss:  [0.25079725]\n",
      "Epoch:  268/500  Loss:  [0.25079922]\n",
      "Epoch:  269/500  Loss:  [0.25080117]\n",
      "Epoch:  270/500  Loss:  [0.2508031]\n",
      "Epoch:  271/500  Loss:  [0.25080501]\n",
      "Epoch:  272/500  Loss:  [0.25080689]\n",
      "Epoch:  273/500  Loss:  [0.25080875]\n",
      "Epoch:  274/500  Loss:  [0.25081058]\n",
      "Epoch:  275/500  Loss:  [0.25081239]\n",
      "Epoch:  276/500  Loss:  [0.25081418]\n",
      "Epoch:  277/500  Loss:  [0.25081594]\n",
      "Epoch:  278/500  Loss:  [0.25081767]\n",
      "Epoch:  279/500  Loss:  [0.25081938]\n",
      "Epoch:  280/500  Loss:  [0.25082107]\n",
      "Epoch:  281/500  Loss:  [0.25082273]\n",
      "Epoch:  282/500  Loss:  [0.25082437]\n",
      "Epoch:  283/500  Loss:  [0.25082598]\n",
      "Epoch:  284/500  Loss:  [0.25082756]\n",
      "Epoch:  285/500  Loss:  [0.25082912]\n",
      "Epoch:  286/500  Loss:  [0.25083065]\n",
      "Epoch:  287/500  Loss:  [0.25083216]\n",
      "Epoch:  288/500  Loss:  [0.25083364]\n",
      "Epoch:  289/500  Loss:  [0.2508351]\n",
      "Epoch:  290/500  Loss:  [0.25083653]\n",
      "Epoch:  291/500  Loss:  [0.25083793]\n",
      "Epoch:  292/500  Loss:  [0.25083931]\n",
      "Epoch:  293/500  Loss:  [0.25084066]\n",
      "Epoch:  294/500  Loss:  [0.25084198]\n",
      "Epoch:  295/500  Loss:  [0.25084328]\n",
      "Epoch:  296/500  Loss:  [0.25084455]\n",
      "Epoch:  297/500  Loss:  [0.25084579]\n",
      "Epoch:  298/500  Loss:  [0.25084701]\n",
      "Epoch:  299/500  Loss:  [0.2508482]\n",
      "Epoch:  300/500  Loss:  [0.25084936]\n",
      "Epoch:  301/500  Loss:  [0.25085049]\n",
      "Epoch:  302/500  Loss:  [0.2508516]\n",
      "Epoch:  303/500  Loss:  [0.25085269]\n",
      "Epoch:  304/500  Loss:  [0.25085374]\n",
      "Epoch:  305/500  Loss:  [0.25085477]\n",
      "Epoch:  306/500  Loss:  [0.25085577]\n",
      "Epoch:  307/500  Loss:  [0.25085674]\n",
      "Epoch:  308/500  Loss:  [0.25085769]\n",
      "Epoch:  309/500  Loss:  [0.25085861]\n",
      "Epoch:  310/500  Loss:  [0.2508595]\n",
      "Epoch:  311/500  Loss:  [0.25086037]\n",
      "Epoch:  312/500  Loss:  [0.25086121]\n",
      "Epoch:  313/500  Loss:  [0.25086202]\n",
      "Epoch:  314/500  Loss:  [0.2508628]\n",
      "Epoch:  315/500  Loss:  [0.25086356]\n",
      "Epoch:  316/500  Loss:  [0.25086429]\n",
      "Epoch:  317/500  Loss:  [0.25086499]\n",
      "Epoch:  318/500  Loss:  [0.25086567]\n",
      "Epoch:  319/500  Loss:  [0.25086632]\n",
      "Epoch:  320/500  Loss:  [0.25086695]\n",
      "Epoch:  321/500  Loss:  [0.25086754]\n",
      "Epoch:  322/500  Loss:  [0.25086811]\n",
      "Epoch:  323/500  Loss:  [0.25086866]\n",
      "Epoch:  324/500  Loss:  [0.25086917]\n",
      "Epoch:  325/500  Loss:  [0.25086967]\n",
      "Epoch:  326/500  Loss:  [0.25087013]\n",
      "Epoch:  327/500  Loss:  [0.25087057]\n",
      "Epoch:  328/500  Loss:  [0.25087098]\n",
      "Epoch:  329/500  Loss:  [0.25087137]\n",
      "Epoch:  330/500  Loss:  [0.25087173]\n",
      "Epoch:  331/500  Loss:  [0.25087206]\n",
      "Epoch:  332/500  Loss:  [0.25087237]\n",
      "Epoch:  333/500  Loss:  [0.25087265]\n",
      "Epoch:  334/500  Loss:  [0.25087291]\n",
      "Epoch:  335/500  Loss:  [0.25087314]\n",
      "Epoch:  336/500  Loss:  [0.25087335]\n",
      "Epoch:  337/500  Loss:  [0.25087353]\n",
      "Epoch:  338/500  Loss:  [0.25087369]\n",
      "Epoch:  339/500  Loss:  [0.25087382]\n",
      "Epoch:  340/500  Loss:  [0.25087392]\n",
      "Epoch:  341/500  Loss:  [0.250874]\n",
      "Epoch:  342/500  Loss:  [0.25087406]\n",
      "Epoch:  343/500  Loss:  [0.25087409]\n",
      "Epoch:  344/500  Loss:  [0.2508741]\n",
      "Epoch:  345/500  Loss:  [0.25087409]\n",
      "Epoch:  346/500  Loss:  [0.25087405]\n",
      "Epoch:  347/500  Loss:  [0.25087398]\n",
      "Epoch:  348/500  Loss:  [0.25087389]\n",
      "Epoch:  349/500  Loss:  [0.25087378]\n",
      "Epoch:  350/500  Loss:  [0.25087365]\n",
      "Epoch:  351/500  Loss:  [0.25087349]\n",
      "Epoch:  352/500  Loss:  [0.25087331]\n",
      "Epoch:  353/500  Loss:  [0.2508731]\n",
      "Epoch:  354/500  Loss:  [0.25087287]\n",
      "Epoch:  355/500  Loss:  [0.25087262]\n",
      "Epoch:  356/500  Loss:  [0.25087235]\n",
      "Epoch:  357/500  Loss:  [0.25087205]\n",
      "Epoch:  358/500  Loss:  [0.25087174]\n",
      "Epoch:  359/500  Loss:  [0.2508714]\n",
      "Epoch:  360/500  Loss:  [0.25087103]\n",
      "Epoch:  361/500  Loss:  [0.25087065]\n",
      "Epoch:  362/500  Loss:  [0.25087025]\n",
      "Epoch:  363/500  Loss:  [0.25086982]\n",
      "Epoch:  364/500  Loss:  [0.25086937]\n",
      "Epoch:  365/500  Loss:  [0.2508689]\n",
      "Epoch:  366/500  Loss:  [0.25086841]\n",
      "Epoch:  367/500  Loss:  [0.2508679]\n",
      "Epoch:  368/500  Loss:  [0.25086737]\n",
      "Epoch:  369/500  Loss:  [0.25086682]\n",
      "Epoch:  370/500  Loss:  [0.25086625]\n",
      "Epoch:  371/500  Loss:  [0.25086566]\n",
      "Epoch:  372/500  Loss:  [0.25086505]\n",
      "Epoch:  373/500  Loss:  [0.25086442]\n",
      "Epoch:  374/500  Loss:  [0.25086377]\n",
      "Epoch:  375/500  Loss:  [0.2508631]\n",
      "Epoch:  376/500  Loss:  [0.25086241]\n",
      "Epoch:  377/500  Loss:  [0.2508617]\n",
      "Epoch:  378/500  Loss:  [0.25086098]\n",
      "Epoch:  379/500  Loss:  [0.25086023]\n",
      "Epoch:  380/500  Loss:  [0.25085947]\n",
      "Epoch:  381/500  Loss:  [0.25085869]\n",
      "Epoch:  382/500  Loss:  [0.25085789]\n",
      "Epoch:  383/500  Loss:  [0.25085707]\n",
      "Epoch:  384/500  Loss:  [0.25085624]\n",
      "Epoch:  385/500  Loss:  [0.25085539]\n",
      "Epoch:  386/500  Loss:  [0.25085452]\n",
      "Epoch:  387/500  Loss:  [0.25085363]\n",
      "Epoch:  388/500  Loss:  [0.25085273]\n",
      "Epoch:  389/500  Loss:  [0.25085181]\n",
      "Epoch:  390/500  Loss:  [0.25085088]\n",
      "Epoch:  391/500  Loss:  [0.25084992]\n",
      "Epoch:  392/500  Loss:  [0.25084896]\n",
      "Epoch:  393/500  Loss:  [0.25084797]\n",
      "Epoch:  394/500  Loss:  [0.25084697]\n",
      "Epoch:  395/500  Loss:  [0.25084596]\n",
      "Epoch:  396/500  Loss:  [0.25084493]\n",
      "Epoch:  397/500  Loss:  [0.25084389]\n",
      "Epoch:  398/500  Loss:  [0.25084283]\n",
      "Epoch:  399/500  Loss:  [0.25084175]\n",
      "Epoch:  400/500  Loss:  [0.25084066]\n",
      "Epoch:  401/500  Loss:  [0.25083956]\n",
      "Epoch:  402/500  Loss:  [0.25083844]\n",
      "Epoch:  403/500  Loss:  [0.25083731]\n",
      "Epoch:  404/500  Loss:  [0.25083617]\n",
      "Epoch:  405/500  Loss:  [0.25083501]\n",
      "Epoch:  406/500  Loss:  [0.25083384]\n",
      "Epoch:  407/500  Loss:  [0.25083266]\n",
      "Epoch:  408/500  Loss:  [0.25083146]\n",
      "Epoch:  409/500  Loss:  [0.25083025]\n",
      "Epoch:  410/500  Loss:  [0.25082903]\n",
      "Epoch:  411/500  Loss:  [0.25082779]\n",
      "Epoch:  412/500  Loss:  [0.25082654]\n",
      "Epoch:  413/500  Loss:  [0.25082529]\n",
      "Epoch:  414/500  Loss:  [0.25082401]\n",
      "Epoch:  415/500  Loss:  [0.25082273]\n",
      "Epoch:  416/500  Loss:  [0.25082144]\n",
      "Epoch:  417/500  Loss:  [0.25082014]\n",
      "Epoch:  418/500  Loss:  [0.25081882]\n",
      "Epoch:  419/500  Loss:  [0.25081749]\n",
      "Epoch:  420/500  Loss:  [0.25081616]\n",
      "Epoch:  421/500  Loss:  [0.25081481]\n",
      "Epoch:  422/500  Loss:  [0.25081345]\n",
      "Epoch:  423/500  Loss:  [0.25081209]\n",
      "Epoch:  424/500  Loss:  [0.25081071]\n",
      "Epoch:  425/500  Loss:  [0.25080932]\n",
      "Epoch:  426/500  Loss:  [0.25080793]\n",
      "Epoch:  427/500  Loss:  [0.25080652]\n",
      "Epoch:  428/500  Loss:  [0.25080511]\n",
      "Epoch:  429/500  Loss:  [0.25080368]\n",
      "Epoch:  430/500  Loss:  [0.25080225]\n",
      "Epoch:  431/500  Loss:  [0.25080081]\n",
      "Epoch:  432/500  Loss:  [0.25079936]\n",
      "Epoch:  433/500  Loss:  [0.25079791]\n",
      "Epoch:  434/500  Loss:  [0.25079644]\n",
      "Epoch:  435/500  Loss:  [0.25079497]\n",
      "Epoch:  436/500  Loss:  [0.25079349]\n",
      "Epoch:  437/500  Loss:  [0.250792]\n",
      "Epoch:  438/500  Loss:  [0.25079051]\n",
      "Epoch:  439/500  Loss:  [0.25078901]\n",
      "Epoch:  440/500  Loss:  [0.2507875]\n",
      "Epoch:  441/500  Loss:  [0.25078599]\n",
      "Epoch:  442/500  Loss:  [0.25078447]\n",
      "Epoch:  443/500  Loss:  [0.25078294]\n",
      "Epoch:  444/500  Loss:  [0.25078141]\n",
      "Epoch:  445/500  Loss:  [0.25077987]\n",
      "Epoch:  446/500  Loss:  [0.25077833]\n",
      "Epoch:  447/500  Loss:  [0.25077678]\n",
      "Epoch:  448/500  Loss:  [0.25077523]\n",
      "Epoch:  449/500  Loss:  [0.25077367]\n",
      "Epoch:  450/500  Loss:  [0.2507721]\n",
      "Epoch:  451/500  Loss:  [0.25077053]\n",
      "Epoch:  452/500  Loss:  [0.25076896]\n",
      "Epoch:  453/500  Loss:  [0.25076738]\n",
      "Epoch:  454/500  Loss:  [0.2507658]\n",
      "Epoch:  455/500  Loss:  [0.25076422]\n",
      "Epoch:  456/500  Loss:  [0.25076263]\n",
      "Epoch:  457/500  Loss:  [0.25076104]\n",
      "Epoch:  458/500  Loss:  [0.25075944]\n",
      "Epoch:  459/500  Loss:  [0.25075785]\n",
      "Epoch:  460/500  Loss:  [0.25075624]\n",
      "Epoch:  461/500  Loss:  [0.25075464]\n",
      "Epoch:  462/500  Loss:  [0.25075304]\n",
      "Epoch:  463/500  Loss:  [0.25075143]\n",
      "Epoch:  464/500  Loss:  [0.25074982]\n",
      "Epoch:  465/500  Loss:  [0.2507482]\n",
      "Epoch:  466/500  Loss:  [0.25074659]\n",
      "Epoch:  467/500  Loss:  [0.25074498]\n",
      "Epoch:  468/500  Loss:  [0.25074336]\n",
      "Epoch:  469/500  Loss:  [0.25074174]\n",
      "Epoch:  470/500  Loss:  [0.25074012]\n",
      "Epoch:  471/500  Loss:  [0.2507385]\n",
      "Epoch:  472/500  Loss:  [0.25073688]\n",
      "Epoch:  473/500  Loss:  [0.25073526]\n",
      "Epoch:  474/500  Loss:  [0.25073364]\n",
      "Epoch:  475/500  Loss:  [0.25073202]\n",
      "Epoch:  476/500  Loss:  [0.2507304]\n",
      "Epoch:  477/500  Loss:  [0.25072878]\n",
      "Epoch:  478/500  Loss:  [0.25072717]\n",
      "Epoch:  479/500  Loss:  [0.25072555]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  480/500  Loss:  [0.25072393]\n",
      "Epoch:  481/500  Loss:  [0.25072232]\n",
      "Epoch:  482/500  Loss:  [0.2507207]\n",
      "Epoch:  483/500  Loss:  [0.25071909]\n",
      "Epoch:  484/500  Loss:  [0.25071748]\n",
      "Epoch:  485/500  Loss:  [0.25071587]\n",
      "Epoch:  486/500  Loss:  [0.25071426]\n",
      "Epoch:  487/500  Loss:  [0.25071266]\n",
      "Epoch:  488/500  Loss:  [0.25071106]\n",
      "Epoch:  489/500  Loss:  [0.25070946]\n",
      "Epoch:  490/500  Loss:  [0.25070787]\n",
      "Epoch:  491/500  Loss:  [0.25070627]\n",
      "Epoch:  492/500  Loss:  [0.25070469]\n",
      "Epoch:  493/500  Loss:  [0.2507031]\n",
      "Epoch:  494/500  Loss:  [0.25070152]\n",
      "Epoch:  495/500  Loss:  [0.25069994]\n",
      "Epoch:  496/500  Loss:  [0.25069837]\n",
      "Epoch:  497/500  Loss:  [0.2506968]\n",
      "Epoch:  498/500  Loss:  [0.25069524]\n",
      "Epoch:  499/500  Loss:  [0.25069368]\n",
      "Epoch:  500/500  Loss:  [0.25069213]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 500 # number of itrations\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.48967783]\n",
      " [0.48596564]\n",
      " [0.48581156]\n",
      " [0.48905856]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
