{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the back-propagation algorithm to learn the weights of a perceptron with 2 input nodes, 2 hidden nodes and 1 output node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **Python3** in used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters - input all parameter values here\n",
    "# note: Unless a cell is found with changed values, the training cells the parameters here \n",
    "input_dim = 2\n",
    "hidden_dim = 2 # dimensions of hidden layers\n",
    "std = 0.005  # train data noise standard deviation\n",
    "w_std = 0.5\n",
    "learn_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "x_inputs = np.array([np.zeros(2), np.ones(2), np.array([1,0]), np.array([0,1])])\n",
    "def generate_trainset(N):\n",
    "    X = np.repeat(x_inputs, N//4, axis=0)\n",
    "    y_xor = np.logical_xor(X.T[0], X.T[1]).astype(np.float)\n",
    "    # add noise to data\n",
    "    X += np.random.normal(0, std, X.shape)\n",
    "    y_xor += np.random.normal(0, std, N)\n",
    "    # shuffle the training data\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    x_train, y_train = X[indices], y_xor[indices]\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid( t):\n",
    "    return 1/(1  + np.exp(-t))\n",
    "\n",
    "def dsigmoid( t):\n",
    "    return sigmoid(t)*(1 - sigmoid(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Experiment with N = 1000\n",
    "This gives good performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "x_train, y_train = generate_trainset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/500  Loss:  [0.43267311]\n",
      "Epoch:  2/500  Loss:  [0.41004883]\n",
      "Epoch:  3/500  Loss:  [0.37495991]\n",
      "Epoch:  4/500  Loss:  [0.32487866]\n",
      "Epoch:  5/500  Loss:  [0.27529686]\n",
      "Epoch:  6/500  Loss:  [0.25544857]\n",
      "Epoch:  7/500  Loss:  [0.2533296]\n",
      "Epoch:  8/500  Loss:  [0.25305454]\n",
      "Epoch:  9/500  Loss:  [0.25285668]\n",
      "Epoch:  10/500  Loss:  [0.25267361]\n",
      "Epoch:  11/500  Loss:  [0.25250291]\n",
      "Epoch:  12/500  Loss:  [0.25234365]\n",
      "Epoch:  13/500  Loss:  [0.25219496]\n",
      "Epoch:  14/500  Loss:  [0.252056]\n",
      "Epoch:  15/500  Loss:  [0.25192603]\n",
      "Epoch:  16/500  Loss:  [0.25180436]\n",
      "Epoch:  17/500  Loss:  [0.25169036]\n",
      "Epoch:  18/500  Loss:  [0.25158345]\n",
      "Epoch:  19/500  Loss:  [0.25148309]\n",
      "Epoch:  20/500  Loss:  [0.25138881]\n",
      "Epoch:  21/500  Loss:  [0.25130014]\n",
      "Epoch:  22/500  Loss:  [0.25121667]\n",
      "Epoch:  23/500  Loss:  [0.25113803]\n",
      "Epoch:  24/500  Loss:  [0.25106385]\n",
      "Epoch:  25/500  Loss:  [0.25099382]\n",
      "Epoch:  26/500  Loss:  [0.25092763]\n",
      "Epoch:  27/500  Loss:  [0.25086501]\n",
      "Epoch:  28/500  Loss:  [0.25080568]\n",
      "Epoch:  29/500  Loss:  [0.25074943]\n",
      "Epoch:  30/500  Loss:  [0.25069601]\n",
      "Epoch:  31/500  Loss:  [0.25064523]\n",
      "Epoch:  32/500  Loss:  [0.25059689]\n",
      "Epoch:  33/500  Loss:  [0.2505508]\n",
      "Epoch:  34/500  Loss:  [0.25050681]\n",
      "Epoch:  35/500  Loss:  [0.25046476]\n",
      "Epoch:  36/500  Loss:  [0.25042448]\n",
      "Epoch:  37/500  Loss:  [0.25038585]\n",
      "Epoch:  38/500  Loss:  [0.25034874]\n",
      "Epoch:  39/500  Loss:  [0.25031302]\n",
      "Epoch:  40/500  Loss:  [0.25027856]\n",
      "Epoch:  41/500  Loss:  [0.25024528]\n",
      "Epoch:  42/500  Loss:  [0.25021305]\n",
      "Epoch:  43/500  Loss:  [0.25018177]\n",
      "Epoch:  44/500  Loss:  [0.25015136]\n",
      "Epoch:  45/500  Loss:  [0.25012171]\n",
      "Epoch:  46/500  Loss:  [0.25009275]\n",
      "Epoch:  47/500  Loss:  [0.25006438]\n",
      "Epoch:  48/500  Loss:  [0.25003653]\n",
      "Epoch:  49/500  Loss:  [0.2500091]\n",
      "Epoch:  50/500  Loss:  [0.24998203]\n",
      "Epoch:  51/500  Loss:  [0.24995524]\n",
      "Epoch:  52/500  Loss:  [0.24992866]\n",
      "Epoch:  53/500  Loss:  [0.2499022]\n",
      "Epoch:  54/500  Loss:  [0.2498758]\n",
      "Epoch:  55/500  Loss:  [0.24984937]\n",
      "Epoch:  56/500  Loss:  [0.24982286]\n",
      "Epoch:  57/500  Loss:  [0.24979618]\n",
      "Epoch:  58/500  Loss:  [0.24976926]\n",
      "Epoch:  59/500  Loss:  [0.24974203]\n",
      "Epoch:  60/500  Loss:  [0.2497144]\n",
      "Epoch:  61/500  Loss:  [0.2496863]\n",
      "Epoch:  62/500  Loss:  [0.24965765]\n",
      "Epoch:  63/500  Loss:  [0.24962836]\n",
      "Epoch:  64/500  Loss:  [0.24959836]\n",
      "Epoch:  65/500  Loss:  [0.24956754]\n",
      "Epoch:  66/500  Loss:  [0.24953582]\n",
      "Epoch:  67/500  Loss:  [0.2495031]\n",
      "Epoch:  68/500  Loss:  [0.24946929]\n",
      "Epoch:  69/500  Loss:  [0.24943426]\n",
      "Epoch:  70/500  Loss:  [0.24939792]\n",
      "Epoch:  71/500  Loss:  [0.24936014]\n",
      "Epoch:  72/500  Loss:  [0.2493208]\n",
      "Epoch:  73/500  Loss:  [0.24927976]\n",
      "Epoch:  74/500  Loss:  [0.2492369]\n",
      "Epoch:  75/500  Loss:  [0.24919206]\n",
      "Epoch:  76/500  Loss:  [0.24914508]\n",
      "Epoch:  77/500  Loss:  [0.24909581]\n",
      "Epoch:  78/500  Loss:  [0.24904405]\n",
      "Epoch:  79/500  Loss:  [0.24898962]\n",
      "Epoch:  80/500  Loss:  [0.24893233]\n",
      "Epoch:  81/500  Loss:  [0.24887196]\n",
      "Epoch:  82/500  Loss:  [0.24880828]\n",
      "Epoch:  83/500  Loss:  [0.24874104]\n",
      "Epoch:  84/500  Loss:  [0.24867]\n",
      "Epoch:  85/500  Loss:  [0.24859487]\n",
      "Epoch:  86/500  Loss:  [0.24851536]\n",
      "Epoch:  87/500  Loss:  [0.24843116]\n",
      "Epoch:  88/500  Loss:  [0.24834194]\n",
      "Epoch:  89/500  Loss:  [0.24824734]\n",
      "Epoch:  90/500  Loss:  [0.24814699]\n",
      "Epoch:  91/500  Loss:  [0.24804049]\n",
      "Epoch:  92/500  Loss:  [0.24792743]\n",
      "Epoch:  93/500  Loss:  [0.24780735]\n",
      "Epoch:  94/500  Loss:  [0.24767979]\n",
      "Epoch:  95/500  Loss:  [0.24754424]\n",
      "Epoch:  96/500  Loss:  [0.2474002]\n",
      "Epoch:  97/500  Loss:  [0.2472471]\n",
      "Epoch:  98/500  Loss:  [0.24708437]\n",
      "Epoch:  99/500  Loss:  [0.24691141]\n",
      "Epoch:  100/500  Loss:  [0.2467276]\n",
      "Epoch:  101/500  Loss:  [0.24653229]\n",
      "Epoch:  102/500  Loss:  [0.2463248]\n",
      "Epoch:  103/500  Loss:  [0.24610444]\n",
      "Epoch:  104/500  Loss:  [0.2458705]\n",
      "Epoch:  105/500  Loss:  [0.24562224]\n",
      "Epoch:  106/500  Loss:  [0.24535894]\n",
      "Epoch:  107/500  Loss:  [0.24507984]\n",
      "Epoch:  108/500  Loss:  [0.24478419]\n",
      "Epoch:  109/500  Loss:  [0.24447125]\n",
      "Epoch:  110/500  Loss:  [0.24414028]\n",
      "Epoch:  111/500  Loss:  [0.24379054]\n",
      "Epoch:  112/500  Loss:  [0.24342135]\n",
      "Epoch:  113/500  Loss:  [0.24303202]\n",
      "Epoch:  114/500  Loss:  [0.24262194]\n",
      "Epoch:  115/500  Loss:  [0.2421905]\n",
      "Epoch:  116/500  Loss:  [0.24173717]\n",
      "Epoch:  117/500  Loss:  [0.24126149]\n",
      "Epoch:  118/500  Loss:  [0.24076307]\n",
      "Epoch:  119/500  Loss:  [0.24024157]\n",
      "Epoch:  120/500  Loss:  [0.23969677]\n",
      "Epoch:  121/500  Loss:  [0.23912853]\n",
      "Epoch:  122/500  Loss:  [0.23853681]\n",
      "Epoch:  123/500  Loss:  [0.23792167]\n",
      "Epoch:  124/500  Loss:  [0.23728328]\n",
      "Epoch:  125/500  Loss:  [0.23662191]\n",
      "Epoch:  126/500  Loss:  [0.23593794]\n",
      "Epoch:  127/500  Loss:  [0.23523186]\n",
      "Epoch:  128/500  Loss:  [0.23450425]\n",
      "Epoch:  129/500  Loss:  [0.23375581]\n",
      "Epoch:  130/500  Loss:  [0.23298731]\n",
      "Epoch:  131/500  Loss:  [0.23219963]\n",
      "Epoch:  132/500  Loss:  [0.23139369]\n",
      "Epoch:  133/500  Loss:  [0.23057051]\n",
      "Epoch:  134/500  Loss:  [0.22973116]\n",
      "Epoch:  135/500  Loss:  [0.22887676]\n",
      "Epoch:  136/500  Loss:  [0.22800845]\n",
      "Epoch:  137/500  Loss:  [0.22712742]\n",
      "Epoch:  138/500  Loss:  [0.22623487]\n",
      "Epoch:  139/500  Loss:  [0.225332]\n",
      "Epoch:  140/500  Loss:  [0.22442001]\n",
      "Epoch:  141/500  Loss:  [0.22350008]\n",
      "Epoch:  142/500  Loss:  [0.22257339]\n",
      "Epoch:  143/500  Loss:  [0.22164107]\n",
      "Epoch:  144/500  Loss:  [0.22070421]\n",
      "Epoch:  145/500  Loss:  [0.21976389]\n",
      "Epoch:  146/500  Loss:  [0.21882112]\n",
      "Epoch:  147/500  Loss:  [0.21787685]\n",
      "Epoch:  148/500  Loss:  [0.21693199]\n",
      "Epoch:  149/500  Loss:  [0.2159874]\n",
      "Epoch:  150/500  Loss:  [0.21504387]\n",
      "Epoch:  151/500  Loss:  [0.21410212]\n",
      "Epoch:  152/500  Loss:  [0.21316284]\n",
      "Epoch:  153/500  Loss:  [0.21222662]\n",
      "Epoch:  154/500  Loss:  [0.21129403]\n",
      "Epoch:  155/500  Loss:  [0.21036554]\n",
      "Epoch:  156/500  Loss:  [0.2094416]\n",
      "Epoch:  157/500  Loss:  [0.20852257]\n",
      "Epoch:  158/500  Loss:  [0.20760878]\n",
      "Epoch:  159/500  Loss:  [0.20670049]\n",
      "Epoch:  160/500  Loss:  [0.20579793]\n",
      "Epoch:  161/500  Loss:  [0.20490126]\n",
      "Epoch:  162/500  Loss:  [0.20401061]\n",
      "Epoch:  163/500  Loss:  [0.20312605]\n",
      "Epoch:  164/500  Loss:  [0.20224762]\n",
      "Epoch:  165/500  Loss:  [0.20137533]\n",
      "Epoch:  166/500  Loss:  [0.20050914]\n",
      "Epoch:  167/500  Loss:  [0.19964899]\n",
      "Epoch:  168/500  Loss:  [0.19879476]\n",
      "Epoch:  169/500  Loss:  [0.19794635]\n",
      "Epoch:  170/500  Loss:  [0.19710358]\n",
      "Epoch:  171/500  Loss:  [0.19626629]\n",
      "Epoch:  172/500  Loss:  [0.19543428]\n",
      "Epoch:  173/500  Loss:  [0.19460732]\n",
      "Epoch:  174/500  Loss:  [0.19378518]\n",
      "Epoch:  175/500  Loss:  [0.1929676]\n",
      "Epoch:  176/500  Loss:  [0.19215432]\n",
      "Epoch:  177/500  Loss:  [0.19134506]\n",
      "Epoch:  178/500  Loss:  [0.19053953]\n",
      "Epoch:  179/500  Loss:  [0.18973742]\n",
      "Epoch:  180/500  Loss:  [0.18893845]\n",
      "Epoch:  181/500  Loss:  [0.18814229]\n",
      "Epoch:  182/500  Loss:  [0.18734864]\n",
      "Epoch:  183/500  Loss:  [0.18655717]\n",
      "Epoch:  184/500  Loss:  [0.18576758]\n",
      "Epoch:  185/500  Loss:  [0.18497953]\n",
      "Epoch:  186/500  Loss:  [0.18419273]\n",
      "Epoch:  187/500  Loss:  [0.18340685]\n",
      "Epoch:  188/500  Loss:  [0.18262159]\n",
      "Epoch:  189/500  Loss:  [0.18183664]\n",
      "Epoch:  190/500  Loss:  [0.18105171]\n",
      "Epoch:  191/500  Loss:  [0.1802665]\n",
      "Epoch:  192/500  Loss:  [0.17948072]\n",
      "Epoch:  193/500  Loss:  [0.17869409]\n",
      "Epoch:  194/500  Loss:  [0.17790635]\n",
      "Epoch:  195/500  Loss:  [0.17711721]\n",
      "Epoch:  196/500  Loss:  [0.17632642]\n",
      "Epoch:  197/500  Loss:  [0.17553371]\n",
      "Epoch:  198/500  Loss:  [0.17473883]\n",
      "Epoch:  199/500  Loss:  [0.17394152]\n",
      "Epoch:  200/500  Loss:  [0.17314152]\n",
      "Epoch:  201/500  Loss:  [0.17233858]\n",
      "Epoch:  202/500  Loss:  [0.17153242]\n",
      "Epoch:  203/500  Loss:  [0.17072277]\n",
      "Epoch:  204/500  Loss:  [0.16990936]\n",
      "Epoch:  205/500  Loss:  [0.16909187]\n",
      "Epoch:  206/500  Loss:  [0.16827]\n",
      "Epoch:  207/500  Loss:  [0.1674434]\n",
      "Epoch:  208/500  Loss:  [0.16661171]\n",
      "Epoch:  209/500  Loss:  [0.16577454]\n",
      "Epoch:  210/500  Loss:  [0.16493146]\n",
      "Epoch:  211/500  Loss:  [0.164082]\n",
      "Epoch:  212/500  Loss:  [0.16322566]\n",
      "Epoch:  213/500  Loss:  [0.1623619]\n",
      "Epoch:  214/500  Loss:  [0.16149012]\n",
      "Epoch:  215/500  Loss:  [0.16060966]\n",
      "Epoch:  216/500  Loss:  [0.15971983]\n",
      "Epoch:  217/500  Loss:  [0.15881986]\n",
      "Epoch:  218/500  Loss:  [0.15790892]\n",
      "Epoch:  219/500  Loss:  [0.15698613]\n",
      "Epoch:  220/500  Loss:  [0.15605052]\n",
      "Epoch:  221/500  Loss:  [0.15510109]\n",
      "Epoch:  222/500  Loss:  [0.15413672]\n",
      "Epoch:  223/500  Loss:  [0.15315626]\n",
      "Epoch:  224/500  Loss:  [0.15215848]\n",
      "Epoch:  225/500  Loss:  [0.15114205]\n",
      "Epoch:  226/500  Loss:  [0.15010562]\n",
      "Epoch:  227/500  Loss:  [0.14904773]\n",
      "Epoch:  228/500  Loss:  [0.14796688]\n",
      "Epoch:  229/500  Loss:  [0.1468615]\n",
      "Epoch:  230/500  Loss:  [0.14572998]\n",
      "Epoch:  231/500  Loss:  [0.14457065]\n",
      "Epoch:  232/500  Loss:  [0.14338183]\n",
      "Epoch:  233/500  Loss:  [0.1421618]\n",
      "Epoch:  234/500  Loss:  [0.14090882]\n",
      "Epoch:  235/500  Loss:  [0.13962119]\n",
      "Epoch:  236/500  Loss:  [0.13829722]\n",
      "Epoch:  237/500  Loss:  [0.13693526]\n",
      "Epoch:  238/500  Loss:  [0.13553376]\n",
      "Epoch:  239/500  Loss:  [0.13409125]\n",
      "Epoch:  240/500  Loss:  [0.1326064]\n",
      "Epoch:  241/500  Loss:  [0.13107804]\n",
      "Epoch:  242/500  Loss:  [0.12950519]\n",
      "Epoch:  243/500  Loss:  [0.12788711]\n",
      "Epoch:  244/500  Loss:  [0.1262233]\n",
      "Epoch:  245/500  Loss:  [0.1245136]\n",
      "Epoch:  246/500  Loss:  [0.12275815]\n",
      "Epoch:  247/500  Loss:  [0.12095745]\n",
      "Epoch:  248/500  Loss:  [0.11911243]\n",
      "Epoch:  249/500  Loss:  [0.11722439]\n",
      "Epoch:  250/500  Loss:  [0.11529508]\n",
      "Epoch:  251/500  Loss:  [0.11332668]\n",
      "Epoch:  252/500  Loss:  [0.11132183]\n",
      "Epoch:  253/500  Loss:  [0.10928357]\n",
      "Epoch:  254/500  Loss:  [0.10721536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  255/500  Loss:  [0.10512106]\n",
      "Epoch:  256/500  Loss:  [0.10300485]\n",
      "Epoch:  257/500  Loss:  [0.10087121]\n",
      "Epoch:  258/500  Loss:  [0.09872486]\n",
      "Epoch:  259/500  Loss:  [0.09657069]\n",
      "Epoch:  260/500  Loss:  [0.09441372]\n",
      "Epoch:  261/500  Loss:  [0.09225897]\n",
      "Epoch:  262/500  Loss:  [0.09011147]\n",
      "Epoch:  263/500  Loss:  [0.08797613]\n",
      "Epoch:  264/500  Loss:  [0.0858577]\n",
      "Epoch:  265/500  Loss:  [0.08376072]\n",
      "Epoch:  266/500  Loss:  [0.08168943]\n",
      "Epoch:  267/500  Loss:  [0.07964779]\n",
      "Epoch:  268/500  Loss:  [0.07763939]\n",
      "Epoch:  269/500  Loss:  [0.07566747]\n",
      "Epoch:  270/500  Loss:  [0.07373484]\n",
      "Epoch:  271/500  Loss:  [0.07184397]\n",
      "Epoch:  272/500  Loss:  [0.0699969]\n",
      "Epoch:  273/500  Loss:  [0.06819529]\n",
      "Epoch:  274/500  Loss:  [0.06644045]\n",
      "Epoch:  275/500  Loss:  [0.06473332]\n",
      "Epoch:  276/500  Loss:  [0.06307453]\n",
      "Epoch:  277/500  Loss:  [0.06146438]\n",
      "Epoch:  278/500  Loss:  [0.05990293]\n",
      "Epoch:  279/500  Loss:  [0.05838999]\n",
      "Epoch:  280/500  Loss:  [0.05692513]\n",
      "Epoch:  281/500  Loss:  [0.05550775]\n",
      "Epoch:  282/500  Loss:  [0.05413709]\n",
      "Epoch:  283/500  Loss:  [0.05281225]\n",
      "Epoch:  284/500  Loss:  [0.05153221]\n",
      "Epoch:  285/500  Loss:  [0.05029588]\n",
      "Epoch:  286/500  Loss:  [0.04910207]\n",
      "Epoch:  287/500  Loss:  [0.04794956]\n",
      "Epoch:  288/500  Loss:  [0.04683707]\n",
      "Epoch:  289/500  Loss:  [0.04576333]\n",
      "Epoch:  290/500  Loss:  [0.04472702]\n",
      "Epoch:  291/500  Loss:  [0.04372684]\n",
      "Epoch:  292/500  Loss:  [0.04276149]\n",
      "Epoch:  293/500  Loss:  [0.04182969]\n",
      "Epoch:  294/500  Loss:  [0.04093017]\n",
      "Epoch:  295/500  Loss:  [0.0400617]\n",
      "Epoch:  296/500  Loss:  [0.03922307]\n",
      "Epoch:  297/500  Loss:  [0.0384131]\n",
      "Epoch:  298/500  Loss:  [0.03763065]\n",
      "Epoch:  299/500  Loss:  [0.03687462]\n",
      "Epoch:  300/500  Loss:  [0.03614393]\n",
      "Epoch:  301/500  Loss:  [0.03543756]\n",
      "Epoch:  302/500  Loss:  [0.03475451]\n",
      "Epoch:  303/500  Loss:  [0.03409384]\n",
      "Epoch:  304/500  Loss:  [0.03345462]\n",
      "Epoch:  305/500  Loss:  [0.03283598]\n",
      "Epoch:  306/500  Loss:  [0.03223708]\n",
      "Epoch:  307/500  Loss:  [0.03165712]\n",
      "Epoch:  308/500  Loss:  [0.03109532]\n",
      "Epoch:  309/500  Loss:  [0.03055094]\n",
      "Epoch:  310/500  Loss:  [0.03002328]\n",
      "Epoch:  311/500  Loss:  [0.02951167]\n",
      "Epoch:  312/500  Loss:  [0.02901546]\n",
      "Epoch:  313/500  Loss:  [0.02853404]\n",
      "Epoch:  314/500  Loss:  [0.02806682]\n",
      "Epoch:  315/500  Loss:  [0.02761325]\n",
      "Epoch:  316/500  Loss:  [0.02717279]\n",
      "Epoch:  317/500  Loss:  [0.02674492]\n",
      "Epoch:  318/500  Loss:  [0.02632916]\n",
      "Epoch:  319/500  Loss:  [0.02592506]\n",
      "Epoch:  320/500  Loss:  [0.02553215]\n",
      "Epoch:  321/500  Loss:  [0.02515002]\n",
      "Epoch:  322/500  Loss:  [0.02477826]\n",
      "Epoch:  323/500  Loss:  [0.02441649]\n",
      "Epoch:  324/500  Loss:  [0.02406434]\n",
      "Epoch:  325/500  Loss:  [0.02372145]\n",
      "Epoch:  326/500  Loss:  [0.02338749]\n",
      "Epoch:  327/500  Loss:  [0.02306214]\n",
      "Epoch:  328/500  Loss:  [0.02274508]\n",
      "Epoch:  329/500  Loss:  [0.02243603]\n",
      "Epoch:  330/500  Loss:  [0.0221347]\n",
      "Epoch:  331/500  Loss:  [0.02184081]\n",
      "Epoch:  332/500  Loss:  [0.02155413]\n",
      "Epoch:  333/500  Loss:  [0.02127439]\n",
      "Epoch:  334/500  Loss:  [0.02100136]\n",
      "Epoch:  335/500  Loss:  [0.02073481]\n",
      "Epoch:  336/500  Loss:  [0.02047453]\n",
      "Epoch:  337/500  Loss:  [0.02022031]\n",
      "Epoch:  338/500  Loss:  [0.01997196]\n",
      "Epoch:  339/500  Loss:  [0.01972927]\n",
      "Epoch:  340/500  Loss:  [0.01949206]\n",
      "Epoch:  341/500  Loss:  [0.01926017]\n",
      "Epoch:  342/500  Loss:  [0.01903342]\n",
      "Epoch:  343/500  Loss:  [0.01881165]\n",
      "Epoch:  344/500  Loss:  [0.0185947]\n",
      "Epoch:  345/500  Loss:  [0.01838243]\n",
      "Epoch:  346/500  Loss:  [0.01817469]\n",
      "Epoch:  347/500  Loss:  [0.01797134]\n",
      "Epoch:  348/500  Loss:  [0.01777225]\n",
      "Epoch:  349/500  Loss:  [0.0175773]\n",
      "Epoch:  350/500  Loss:  [0.01738635]\n",
      "Epoch:  351/500  Loss:  [0.0171993]\n",
      "Epoch:  352/500  Loss:  [0.01701603]\n",
      "Epoch:  353/500  Loss:  [0.01683643]\n",
      "Epoch:  354/500  Loss:  [0.01666039]\n",
      "Epoch:  355/500  Loss:  [0.01648781]\n",
      "Epoch:  356/500  Loss:  [0.0163186]\n",
      "Epoch:  357/500  Loss:  [0.01615266]\n",
      "Epoch:  358/500  Loss:  [0.0159899]\n",
      "Epoch:  359/500  Loss:  [0.01583023]\n",
      "Epoch:  360/500  Loss:  [0.01567357]\n",
      "Epoch:  361/500  Loss:  [0.01551983]\n",
      "Epoch:  362/500  Loss:  [0.01536894]\n",
      "Epoch:  363/500  Loss:  [0.01522083]\n",
      "Epoch:  364/500  Loss:  [0.01507541]\n",
      "Epoch:  365/500  Loss:  [0.01493262]\n",
      "Epoch:  366/500  Loss:  [0.01479238]\n",
      "Epoch:  367/500  Loss:  [0.01465464]\n",
      "Epoch:  368/500  Loss:  [0.01451932]\n",
      "Epoch:  369/500  Loss:  [0.01438638]\n",
      "Epoch:  370/500  Loss:  [0.01425573]\n",
      "Epoch:  371/500  Loss:  [0.01412734]\n",
      "Epoch:  372/500  Loss:  [0.01400114]\n",
      "Epoch:  373/500  Loss:  [0.01387708]\n",
      "Epoch:  374/500  Loss:  [0.0137551]\n",
      "Epoch:  375/500  Loss:  [0.01363516]\n",
      "Epoch:  376/500  Loss:  [0.0135172]\n",
      "Epoch:  377/500  Loss:  [0.01340118]\n",
      "Epoch:  378/500  Loss:  [0.01328706]\n",
      "Epoch:  379/500  Loss:  [0.01317478]\n",
      "Epoch:  380/500  Loss:  [0.0130643]\n",
      "Epoch:  381/500  Loss:  [0.01295559]\n",
      "Epoch:  382/500  Loss:  [0.01284859]\n",
      "Epoch:  383/500  Loss:  [0.01274328]\n",
      "Epoch:  384/500  Loss:  [0.01263961]\n",
      "Epoch:  385/500  Loss:  [0.01253755]\n",
      "Epoch:  386/500  Loss:  [0.01243705]\n",
      "Epoch:  387/500  Loss:  [0.01233809]\n",
      "Epoch:  388/500  Loss:  [0.01224063]\n",
      "Epoch:  389/500  Loss:  [0.01214464]\n",
      "Epoch:  390/500  Loss:  [0.01205008]\n",
      "Epoch:  391/500  Loss:  [0.01195693]\n",
      "Epoch:  392/500  Loss:  [0.01186515]\n",
      "Epoch:  393/500  Loss:  [0.01177471]\n",
      "Epoch:  394/500  Loss:  [0.01168559]\n",
      "Epoch:  395/500  Loss:  [0.01159775]\n",
      "Epoch:  396/500  Loss:  [0.01151118]\n",
      "Epoch:  397/500  Loss:  [0.01142584]\n",
      "Epoch:  398/500  Loss:  [0.01134171]\n",
      "Epoch:  399/500  Loss:  [0.01125876]\n",
      "Epoch:  400/500  Loss:  [0.01117697]\n",
      "Epoch:  401/500  Loss:  [0.01109631]\n",
      "Epoch:  402/500  Loss:  [0.01101677]\n",
      "Epoch:  403/500  Loss:  [0.01093832]\n",
      "Epoch:  404/500  Loss:  [0.01086094]\n",
      "Epoch:  405/500  Loss:  [0.0107846]\n",
      "Epoch:  406/500  Loss:  [0.01070929]\n",
      "Epoch:  407/500  Loss:  [0.01063498]\n",
      "Epoch:  408/500  Loss:  [0.01056167]\n",
      "Epoch:  409/500  Loss:  [0.01048931]\n",
      "Epoch:  410/500  Loss:  [0.01041791]\n",
      "Epoch:  411/500  Loss:  [0.01034744]\n",
      "Epoch:  412/500  Loss:  [0.01027788]\n",
      "Epoch:  413/500  Loss:  [0.01020922]\n",
      "Epoch:  414/500  Loss:  [0.01014143]\n",
      "Epoch:  415/500  Loss:  [0.01007451]\n",
      "Epoch:  416/500  Loss:  [0.01000843]\n",
      "Epoch:  417/500  Loss:  [0.00994319]\n",
      "Epoch:  418/500  Loss:  [0.00987876]\n",
      "Epoch:  419/500  Loss:  [0.00981512]\n",
      "Epoch:  420/500  Loss:  [0.00975228]\n",
      "Epoch:  421/500  Loss:  [0.00969021]\n",
      "Epoch:  422/500  Loss:  [0.00962889]\n",
      "Epoch:  423/500  Loss:  [0.00956832]\n",
      "Epoch:  424/500  Loss:  [0.00950848]\n",
      "Epoch:  425/500  Loss:  [0.00944935]\n",
      "Epoch:  426/500  Loss:  [0.00939094]\n",
      "Epoch:  427/500  Loss:  [0.00933321]\n",
      "Epoch:  428/500  Loss:  [0.00927617]\n",
      "Epoch:  429/500  Loss:  [0.00921979]\n",
      "Epoch:  430/500  Loss:  [0.00916408]\n",
      "Epoch:  431/500  Loss:  [0.00910901]\n",
      "Epoch:  432/500  Loss:  [0.00905457]\n",
      "Epoch:  433/500  Loss:  [0.00900076]\n",
      "Epoch:  434/500  Loss:  [0.00894757]\n",
      "Epoch:  435/500  Loss:  [0.00889498]\n",
      "Epoch:  436/500  Loss:  [0.00884298]\n",
      "Epoch:  437/500  Loss:  [0.00879157]\n",
      "Epoch:  438/500  Loss:  [0.00874073]\n",
      "Epoch:  439/500  Loss:  [0.00869045]\n",
      "Epoch:  440/500  Loss:  [0.00864073]\n",
      "Epoch:  441/500  Loss:  [0.00859156]\n",
      "Epoch:  442/500  Loss:  [0.00854293]\n",
      "Epoch:  443/500  Loss:  [0.00849482]\n",
      "Epoch:  444/500  Loss:  [0.00844724]\n",
      "Epoch:  445/500  Loss:  [0.00840017]\n",
      "Epoch:  446/500  Loss:  [0.0083536]\n",
      "Epoch:  447/500  Loss:  [0.00830753]\n",
      "Epoch:  448/500  Loss:  [0.00826195]\n",
      "Epoch:  449/500  Loss:  [0.00821685]\n",
      "Epoch:  450/500  Loss:  [0.00817223]\n",
      "Epoch:  451/500  Loss:  [0.00812807]\n",
      "Epoch:  452/500  Loss:  [0.00808437]\n",
      "Epoch:  453/500  Loss:  [0.00804112]\n",
      "Epoch:  454/500  Loss:  [0.00799831]\n",
      "Epoch:  455/500  Loss:  [0.00795595]\n",
      "Epoch:  456/500  Loss:  [0.00791402]\n",
      "Epoch:  457/500  Loss:  [0.00787251]\n",
      "Epoch:  458/500  Loss:  [0.00783142]\n",
      "Epoch:  459/500  Loss:  [0.00779075]\n",
      "Epoch:  460/500  Loss:  [0.00775048]\n",
      "Epoch:  461/500  Loss:  [0.00771061]\n",
      "Epoch:  462/500  Loss:  [0.00767114]\n",
      "Epoch:  463/500  Loss:  [0.00763206]\n",
      "Epoch:  464/500  Loss:  [0.00759336]\n",
      "Epoch:  465/500  Loss:  [0.00755504]\n",
      "Epoch:  466/500  Loss:  [0.00751709]\n",
      "Epoch:  467/500  Loss:  [0.00747951]\n",
      "Epoch:  468/500  Loss:  [0.00744229]\n",
      "Epoch:  469/500  Loss:  [0.00740543]\n",
      "Epoch:  470/500  Loss:  [0.00736892]\n",
      "Epoch:  471/500  Loss:  [0.00733276]\n",
      "Epoch:  472/500  Loss:  [0.00729694]\n",
      "Epoch:  473/500  Loss:  [0.00726145]\n",
      "Epoch:  474/500  Loss:  [0.0072263]\n",
      "Epoch:  475/500  Loss:  [0.00719148]\n",
      "Epoch:  476/500  Loss:  [0.00715698]\n",
      "Epoch:  477/500  Loss:  [0.0071228]\n",
      "Epoch:  478/500  Loss:  [0.00708894]\n",
      "Epoch:  479/500  Loss:  [0.00705538]\n",
      "Epoch:  480/500  Loss:  [0.00702214]\n",
      "Epoch:  481/500  Loss:  [0.00698919]\n",
      "Epoch:  482/500  Loss:  [0.00695654]\n",
      "Epoch:  483/500  Loss:  [0.00692419]\n",
      "Epoch:  484/500  Loss:  [0.00689213]\n",
      "Epoch:  485/500  Loss:  [0.00686035]\n",
      "Epoch:  486/500  Loss:  [0.00682886]\n",
      "Epoch:  487/500  Loss:  [0.00679764]\n",
      "Epoch:  488/500  Loss:  [0.00676671]\n",
      "Epoch:  489/500  Loss:  [0.00673604]\n",
      "Epoch:  490/500  Loss:  [0.00670564]\n",
      "Epoch:  491/500  Loss:  [0.00667551]\n",
      "Epoch:  492/500  Loss:  [0.00664563]\n",
      "Epoch:  493/500  Loss:  [0.00661602]\n",
      "Epoch:  494/500  Loss:  [0.00658666]\n",
      "Epoch:  495/500  Loss:  [0.00655755]\n",
      "Epoch:  496/500  Loss:  [0.00652869]\n",
      "Epoch:  497/500  Loss:  [0.00650008]\n",
      "Epoch:  498/500  Loss:  [0.00647171]\n",
      "Epoch:  499/500  Loss:  [0.00644357]\n",
      "Epoch:  500/500  Loss:  [0.00641568]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))# [1,1], [1,0], [0,0], [0,1]\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 500 # number of itrations\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10077908]\n",
      " [0.05632131]\n",
      " [0.92554713]\n",
      " [0.9188792 ]]\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Experiment with N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "x_train, y_train = generate_trainset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/1000  Loss:  [0.26374113]\n",
      "Epoch:  2/1000  Loss:  [0.26211801]\n",
      "Epoch:  3/1000  Loss:  [0.26067316]\n",
      "Epoch:  4/1000  Loss:  [0.25939107]\n",
      "Epoch:  5/1000  Loss:  [0.2582568]\n",
      "Epoch:  6/1000  Loss:  [0.25725609]\n",
      "Epoch:  7/1000  Loss:  [0.25637555]\n",
      "Epoch:  8/1000  Loss:  [0.25560266]\n",
      "Epoch:  9/1000  Loss:  [0.25492585]\n",
      "Epoch:  10/1000  Loss:  [0.25433452]\n",
      "Epoch:  11/1000  Loss:  [0.25381899]\n",
      "Epoch:  12/1000  Loss:  [0.25337049]\n",
      "Epoch:  13/1000  Loss:  [0.25298111]\n",
      "Epoch:  14/1000  Loss:  [0.25264376]\n",
      "Epoch:  15/1000  Loss:  [0.25235207]\n",
      "Epoch:  16/1000  Loss:  [0.25210039]\n",
      "Epoch:  17/1000  Loss:  [0.25188371]\n",
      "Epoch:  18/1000  Loss:  [0.25169756]\n",
      "Epoch:  19/1000  Loss:  [0.25153801]\n",
      "Epoch:  20/1000  Loss:  [0.25140162]\n",
      "Epoch:  21/1000  Loss:  [0.25128532]\n",
      "Epoch:  22/1000  Loss:  [0.25118644]\n",
      "Epoch:  23/1000  Loss:  [0.25110265]\n",
      "Epoch:  24/1000  Loss:  [0.2510319]\n",
      "Epoch:  25/1000  Loss:  [0.25097239]\n",
      "Epoch:  26/1000  Loss:  [0.25092257]\n",
      "Epoch:  27/1000  Loss:  [0.25088108]\n",
      "Epoch:  28/1000  Loss:  [0.25084673]\n",
      "Epoch:  29/1000  Loss:  [0.25081852]\n",
      "Epoch:  30/1000  Loss:  [0.25079553]\n",
      "Epoch:  31/1000  Loss:  [0.25077701]\n",
      "Epoch:  32/1000  Loss:  [0.25076228]\n",
      "Epoch:  33/1000  Loss:  [0.25075078]\n",
      "Epoch:  34/1000  Loss:  [0.250742]\n",
      "Epoch:  35/1000  Loss:  [0.25073552]\n",
      "Epoch:  36/1000  Loss:  [0.25073097]\n",
      "Epoch:  37/1000  Loss:  [0.25072803]\n",
      "Epoch:  38/1000  Loss:  [0.25072642]\n",
      "Epoch:  39/1000  Loss:  [0.25072593]\n",
      "Epoch:  40/1000  Loss:  [0.25072635]\n",
      "Epoch:  41/1000  Loss:  [0.25072751]\n",
      "Epoch:  42/1000  Loss:  [0.25072927]\n",
      "Epoch:  43/1000  Loss:  [0.2507315]\n",
      "Epoch:  44/1000  Loss:  [0.2507341]\n",
      "Epoch:  45/1000  Loss:  [0.25073699]\n",
      "Epoch:  46/1000  Loss:  [0.2507401]\n",
      "Epoch:  47/1000  Loss:  [0.25074335]\n",
      "Epoch:  48/1000  Loss:  [0.2507467]\n",
      "Epoch:  49/1000  Loss:  [0.25075012]\n",
      "Epoch:  50/1000  Loss:  [0.25075356]\n",
      "Epoch:  51/1000  Loss:  [0.25075699]\n",
      "Epoch:  52/1000  Loss:  [0.2507604]\n",
      "Epoch:  53/1000  Loss:  [0.25076376]\n",
      "Epoch:  54/1000  Loss:  [0.25076706]\n",
      "Epoch:  55/1000  Loss:  [0.25077029]\n",
      "Epoch:  56/1000  Loss:  [0.25077345]\n",
      "Epoch:  57/1000  Loss:  [0.25077652]\n",
      "Epoch:  58/1000  Loss:  [0.25077951]\n",
      "Epoch:  59/1000  Loss:  [0.25078241]\n",
      "Epoch:  60/1000  Loss:  [0.25078521]\n",
      "Epoch:  61/1000  Loss:  [0.25078793]\n",
      "Epoch:  62/1000  Loss:  [0.25079056]\n",
      "Epoch:  63/1000  Loss:  [0.2507931]\n",
      "Epoch:  64/1000  Loss:  [0.25079555]\n",
      "Epoch:  65/1000  Loss:  [0.25079793]\n",
      "Epoch:  66/1000  Loss:  [0.25080022]\n",
      "Epoch:  67/1000  Loss:  [0.25080244]\n",
      "Epoch:  68/1000  Loss:  [0.25080459]\n",
      "Epoch:  69/1000  Loss:  [0.25080667]\n",
      "Epoch:  70/1000  Loss:  [0.25080869]\n",
      "Epoch:  71/1000  Loss:  [0.25081064]\n",
      "Epoch:  72/1000  Loss:  [0.25081254]\n",
      "Epoch:  73/1000  Loss:  [0.25081438]\n",
      "Epoch:  74/1000  Loss:  [0.25081618]\n",
      "Epoch:  75/1000  Loss:  [0.25081793]\n",
      "Epoch:  76/1000  Loss:  [0.25081963]\n",
      "Epoch:  77/1000  Loss:  [0.2508213]\n",
      "Epoch:  78/1000  Loss:  [0.25082293]\n",
      "Epoch:  79/1000  Loss:  [0.25082452]\n",
      "Epoch:  80/1000  Loss:  [0.25082609]\n",
      "Epoch:  81/1000  Loss:  [0.25082762]\n",
      "Epoch:  82/1000  Loss:  [0.25082913]\n",
      "Epoch:  83/1000  Loss:  [0.25083062]\n",
      "Epoch:  84/1000  Loss:  [0.25083208]\n",
      "Epoch:  85/1000  Loss:  [0.25083353]\n",
      "Epoch:  86/1000  Loss:  [0.25083496]\n",
      "Epoch:  87/1000  Loss:  [0.25083637]\n",
      "Epoch:  88/1000  Loss:  [0.25083778]\n",
      "Epoch:  89/1000  Loss:  [0.25083917]\n",
      "Epoch:  90/1000  Loss:  [0.25084055]\n",
      "Epoch:  91/1000  Loss:  [0.25084192]\n",
      "Epoch:  92/1000  Loss:  [0.25084329]\n",
      "Epoch:  93/1000  Loss:  [0.25084465]\n",
      "Epoch:  94/1000  Loss:  [0.25084601]\n",
      "Epoch:  95/1000  Loss:  [0.25084737]\n",
      "Epoch:  96/1000  Loss:  [0.25084872]\n",
      "Epoch:  97/1000  Loss:  [0.25085007]\n",
      "Epoch:  98/1000  Loss:  [0.25085143]\n",
      "Epoch:  99/1000  Loss:  [0.25085278]\n",
      "Epoch:  100/1000  Loss:  [0.25085414]\n",
      "Epoch:  101/1000  Loss:  [0.2508555]\n",
      "Epoch:  102/1000  Loss:  [0.25085686]\n",
      "Epoch:  103/1000  Loss:  [0.25085823]\n",
      "Epoch:  104/1000  Loss:  [0.25085961]\n",
      "Epoch:  105/1000  Loss:  [0.25086099]\n",
      "Epoch:  106/1000  Loss:  [0.25086237]\n",
      "Epoch:  107/1000  Loss:  [0.25086376]\n",
      "Epoch:  108/1000  Loss:  [0.25086516]\n",
      "Epoch:  109/1000  Loss:  [0.25086657]\n",
      "Epoch:  110/1000  Loss:  [0.25086798]\n",
      "Epoch:  111/1000  Loss:  [0.2508694]\n",
      "Epoch:  112/1000  Loss:  [0.25087084]\n",
      "Epoch:  113/1000  Loss:  [0.25087228]\n",
      "Epoch:  114/1000  Loss:  [0.25087372]\n",
      "Epoch:  115/1000  Loss:  [0.25087518]\n",
      "Epoch:  116/1000  Loss:  [0.25087665]\n",
      "Epoch:  117/1000  Loss:  [0.25087813]\n",
      "Epoch:  118/1000  Loss:  [0.25087962]\n",
      "Epoch:  119/1000  Loss:  [0.25088112]\n",
      "Epoch:  120/1000  Loss:  [0.25088262]\n",
      "Epoch:  121/1000  Loss:  [0.25088414]\n",
      "Epoch:  122/1000  Loss:  [0.25088567]\n",
      "Epoch:  123/1000  Loss:  [0.25088721]\n",
      "Epoch:  124/1000  Loss:  [0.25088877]\n",
      "Epoch:  125/1000  Loss:  [0.25089033]\n",
      "Epoch:  126/1000  Loss:  [0.2508919]\n",
      "Epoch:  127/1000  Loss:  [0.25089348]\n",
      "Epoch:  128/1000  Loss:  [0.25089508]\n",
      "Epoch:  129/1000  Loss:  [0.25089669]\n",
      "Epoch:  130/1000  Loss:  [0.2508983]\n",
      "Epoch:  131/1000  Loss:  [0.25089993]\n",
      "Epoch:  132/1000  Loss:  [0.25090157]\n",
      "Epoch:  133/1000  Loss:  [0.25090322]\n",
      "Epoch:  134/1000  Loss:  [0.25090488]\n",
      "Epoch:  135/1000  Loss:  [0.25090656]\n",
      "Epoch:  136/1000  Loss:  [0.25090824]\n",
      "Epoch:  137/1000  Loss:  [0.25090994]\n",
      "Epoch:  138/1000  Loss:  [0.25091164]\n",
      "Epoch:  139/1000  Loss:  [0.25091336]\n",
      "Epoch:  140/1000  Loss:  [0.25091509]\n",
      "Epoch:  141/1000  Loss:  [0.25091683]\n",
      "Epoch:  142/1000  Loss:  [0.25091857]\n",
      "Epoch:  143/1000  Loss:  [0.25092033]\n",
      "Epoch:  144/1000  Loss:  [0.25092211]\n",
      "Epoch:  145/1000  Loss:  [0.25092389]\n",
      "Epoch:  146/1000  Loss:  [0.25092568]\n",
      "Epoch:  147/1000  Loss:  [0.25092748]\n",
      "Epoch:  148/1000  Loss:  [0.25092929]\n",
      "Epoch:  149/1000  Loss:  [0.25093112]\n",
      "Epoch:  150/1000  Loss:  [0.25093295]\n",
      "Epoch:  151/1000  Loss:  [0.25093479]\n",
      "Epoch:  152/1000  Loss:  [0.25093665]\n",
      "Epoch:  153/1000  Loss:  [0.25093851]\n",
      "Epoch:  154/1000  Loss:  [0.25094038]\n",
      "Epoch:  155/1000  Loss:  [0.25094227]\n",
      "Epoch:  156/1000  Loss:  [0.25094416]\n",
      "Epoch:  157/1000  Loss:  [0.25094606]\n",
      "Epoch:  158/1000  Loss:  [0.25094797]\n",
      "Epoch:  159/1000  Loss:  [0.25094989]\n",
      "Epoch:  160/1000  Loss:  [0.25095182]\n",
      "Epoch:  161/1000  Loss:  [0.25095376]\n",
      "Epoch:  162/1000  Loss:  [0.25095571]\n",
      "Epoch:  163/1000  Loss:  [0.25095766]\n",
      "Epoch:  164/1000  Loss:  [0.25095963]\n",
      "Epoch:  165/1000  Loss:  [0.2509616]\n",
      "Epoch:  166/1000  Loss:  [0.25096359]\n",
      "Epoch:  167/1000  Loss:  [0.25096558]\n",
      "Epoch:  168/1000  Loss:  [0.25096758]\n",
      "Epoch:  169/1000  Loss:  [0.25096958]\n",
      "Epoch:  170/1000  Loss:  [0.2509716]\n",
      "Epoch:  171/1000  Loss:  [0.25097362]\n",
      "Epoch:  172/1000  Loss:  [0.25097565]\n",
      "Epoch:  173/1000  Loss:  [0.25097769]\n",
      "Epoch:  174/1000  Loss:  [0.25097973]\n",
      "Epoch:  175/1000  Loss:  [0.25098179]\n",
      "Epoch:  176/1000  Loss:  [0.25098385]\n",
      "Epoch:  177/1000  Loss:  [0.25098592]\n",
      "Epoch:  178/1000  Loss:  [0.25098799]\n",
      "Epoch:  179/1000  Loss:  [0.25099007]\n",
      "Epoch:  180/1000  Loss:  [0.25099216]\n",
      "Epoch:  181/1000  Loss:  [0.25099425]\n",
      "Epoch:  182/1000  Loss:  [0.25099635]\n",
      "Epoch:  183/1000  Loss:  [0.25099846]\n",
      "Epoch:  184/1000  Loss:  [0.25100057]\n",
      "Epoch:  185/1000  Loss:  [0.25100269]\n",
      "Epoch:  186/1000  Loss:  [0.25100482]\n",
      "Epoch:  187/1000  Loss:  [0.25100695]\n",
      "Epoch:  188/1000  Loss:  [0.25100908]\n",
      "Epoch:  189/1000  Loss:  [0.25101123]\n",
      "Epoch:  190/1000  Loss:  [0.25101337]\n",
      "Epoch:  191/1000  Loss:  [0.25101552]\n",
      "Epoch:  192/1000  Loss:  [0.25101768]\n",
      "Epoch:  193/1000  Loss:  [0.25101984]\n",
      "Epoch:  194/1000  Loss:  [0.25102201]\n",
      "Epoch:  195/1000  Loss:  [0.25102418]\n",
      "Epoch:  196/1000  Loss:  [0.25102635]\n",
      "Epoch:  197/1000  Loss:  [0.25102853]\n",
      "Epoch:  198/1000  Loss:  [0.25103072]\n",
      "Epoch:  199/1000  Loss:  [0.2510329]\n",
      "Epoch:  200/1000  Loss:  [0.25103509]\n",
      "Epoch:  201/1000  Loss:  [0.25103729]\n",
      "Epoch:  202/1000  Loss:  [0.25103949]\n",
      "Epoch:  203/1000  Loss:  [0.25104169]\n",
      "Epoch:  204/1000  Loss:  [0.25104389]\n",
      "Epoch:  205/1000  Loss:  [0.2510461]\n",
      "Epoch:  206/1000  Loss:  [0.25104831]\n",
      "Epoch:  207/1000  Loss:  [0.25105052]\n",
      "Epoch:  208/1000  Loss:  [0.25105274]\n",
      "Epoch:  209/1000  Loss:  [0.25105496]\n",
      "Epoch:  210/1000  Loss:  [0.25105718]\n",
      "Epoch:  211/1000  Loss:  [0.2510594]\n",
      "Epoch:  212/1000  Loss:  [0.25106162]\n",
      "Epoch:  213/1000  Loss:  [0.25106385]\n",
      "Epoch:  214/1000  Loss:  [0.25106608]\n",
      "Epoch:  215/1000  Loss:  [0.2510683]\n",
      "Epoch:  216/1000  Loss:  [0.25107053]\n",
      "Epoch:  217/1000  Loss:  [0.25107276]\n",
      "Epoch:  218/1000  Loss:  [0.251075]\n",
      "Epoch:  219/1000  Loss:  [0.25107723]\n",
      "Epoch:  220/1000  Loss:  [0.25107946]\n",
      "Epoch:  221/1000  Loss:  [0.2510817]\n",
      "Epoch:  222/1000  Loss:  [0.25108393]\n",
      "Epoch:  223/1000  Loss:  [0.25108616]\n",
      "Epoch:  224/1000  Loss:  [0.2510884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  225/1000  Loss:  [0.25109063]\n",
      "Epoch:  226/1000  Loss:  [0.25109287]\n",
      "Epoch:  227/1000  Loss:  [0.2510951]\n",
      "Epoch:  228/1000  Loss:  [0.25109733]\n",
      "Epoch:  229/1000  Loss:  [0.25109956]\n",
      "Epoch:  230/1000  Loss:  [0.25110179]\n",
      "Epoch:  231/1000  Loss:  [0.25110402]\n",
      "Epoch:  232/1000  Loss:  [0.25110625]\n",
      "Epoch:  233/1000  Loss:  [0.25110848]\n",
      "Epoch:  234/1000  Loss:  [0.2511107]\n",
      "Epoch:  235/1000  Loss:  [0.25111292]\n",
      "Epoch:  236/1000  Loss:  [0.25111515]\n",
      "Epoch:  237/1000  Loss:  [0.25111736]\n",
      "Epoch:  238/1000  Loss:  [0.25111958]\n",
      "Epoch:  239/1000  Loss:  [0.25112179]\n",
      "Epoch:  240/1000  Loss:  [0.25112401]\n",
      "Epoch:  241/1000  Loss:  [0.25112621]\n",
      "Epoch:  242/1000  Loss:  [0.25112842]\n",
      "Epoch:  243/1000  Loss:  [0.25113062]\n",
      "Epoch:  244/1000  Loss:  [0.25113282]\n",
      "Epoch:  245/1000  Loss:  [0.25113502]\n",
      "Epoch:  246/1000  Loss:  [0.25113721]\n",
      "Epoch:  247/1000  Loss:  [0.2511394]\n",
      "Epoch:  248/1000  Loss:  [0.25114158]\n",
      "Epoch:  249/1000  Loss:  [0.25114376]\n",
      "Epoch:  250/1000  Loss:  [0.25114594]\n",
      "Epoch:  251/1000  Loss:  [0.25114811]\n",
      "Epoch:  252/1000  Loss:  [0.25115028]\n",
      "Epoch:  253/1000  Loss:  [0.25115244]\n",
      "Epoch:  254/1000  Loss:  [0.25115459]\n",
      "Epoch:  255/1000  Loss:  [0.25115675]\n",
      "Epoch:  256/1000  Loss:  [0.25115889]\n",
      "Epoch:  257/1000  Loss:  [0.25116103]\n",
      "Epoch:  258/1000  Loss:  [0.25116317]\n",
      "Epoch:  259/1000  Loss:  [0.2511653]\n",
      "Epoch:  260/1000  Loss:  [0.25116742]\n",
      "Epoch:  261/1000  Loss:  [0.25116954]\n",
      "Epoch:  262/1000  Loss:  [0.25117165]\n",
      "Epoch:  263/1000  Loss:  [0.25117376]\n",
      "Epoch:  264/1000  Loss:  [0.25117586]\n",
      "Epoch:  265/1000  Loss:  [0.25117795]\n",
      "Epoch:  266/1000  Loss:  [0.25118003]\n",
      "Epoch:  267/1000  Loss:  [0.25118211]\n",
      "Epoch:  268/1000  Loss:  [0.25118418]\n",
      "Epoch:  269/1000  Loss:  [0.25118625]\n",
      "Epoch:  270/1000  Loss:  [0.2511883]\n",
      "Epoch:  271/1000  Loss:  [0.25119035]\n",
      "Epoch:  272/1000  Loss:  [0.25119239]\n",
      "Epoch:  273/1000  Loss:  [0.25119442]\n",
      "Epoch:  274/1000  Loss:  [0.25119645]\n",
      "Epoch:  275/1000  Loss:  [0.25119846]\n",
      "Epoch:  276/1000  Loss:  [0.25120047]\n",
      "Epoch:  277/1000  Loss:  [0.25120247]\n",
      "Epoch:  278/1000  Loss:  [0.25120446]\n",
      "Epoch:  279/1000  Loss:  [0.25120644]\n",
      "Epoch:  280/1000  Loss:  [0.25120841]\n",
      "Epoch:  281/1000  Loss:  [0.25121037]\n",
      "Epoch:  282/1000  Loss:  [0.25121233]\n",
      "Epoch:  283/1000  Loss:  [0.25121427]\n",
      "Epoch:  284/1000  Loss:  [0.2512162]\n",
      "Epoch:  285/1000  Loss:  [0.25121813]\n",
      "Epoch:  286/1000  Loss:  [0.25122004]\n",
      "Epoch:  287/1000  Loss:  [0.25122195]\n",
      "Epoch:  288/1000  Loss:  [0.25122384]\n",
      "Epoch:  289/1000  Loss:  [0.25122572]\n",
      "Epoch:  290/1000  Loss:  [0.2512276]\n",
      "Epoch:  291/1000  Loss:  [0.25122946]\n",
      "Epoch:  292/1000  Loss:  [0.25123131]\n",
      "Epoch:  293/1000  Loss:  [0.25123315]\n",
      "Epoch:  294/1000  Loss:  [0.25123498]\n",
      "Epoch:  295/1000  Loss:  [0.2512368]\n",
      "Epoch:  296/1000  Loss:  [0.2512386]\n",
      "Epoch:  297/1000  Loss:  [0.2512404]\n",
      "Epoch:  298/1000  Loss:  [0.25124218]\n",
      "Epoch:  299/1000  Loss:  [0.25124395]\n",
      "Epoch:  300/1000  Loss:  [0.25124571]\n",
      "Epoch:  301/1000  Loss:  [0.25124745]\n",
      "Epoch:  302/1000  Loss:  [0.25124919]\n",
      "Epoch:  303/1000  Loss:  [0.25125091]\n",
      "Epoch:  304/1000  Loss:  [0.25125262]\n",
      "Epoch:  305/1000  Loss:  [0.25125431]\n",
      "Epoch:  306/1000  Loss:  [0.251256]\n",
      "Epoch:  307/1000  Loss:  [0.25125767]\n",
      "Epoch:  308/1000  Loss:  [0.25125932]\n",
      "Epoch:  309/1000  Loss:  [0.25126097]\n",
      "Epoch:  310/1000  Loss:  [0.2512626]\n",
      "Epoch:  311/1000  Loss:  [0.25126422]\n",
      "Epoch:  312/1000  Loss:  [0.25126582]\n",
      "Epoch:  313/1000  Loss:  [0.25126741]\n",
      "Epoch:  314/1000  Loss:  [0.25126898]\n",
      "Epoch:  315/1000  Loss:  [0.25127054]\n",
      "Epoch:  316/1000  Loss:  [0.25127209]\n",
      "Epoch:  317/1000  Loss:  [0.25127362]\n",
      "Epoch:  318/1000  Loss:  [0.25127514]\n",
      "Epoch:  319/1000  Loss:  [0.25127665]\n",
      "Epoch:  320/1000  Loss:  [0.25127814]\n",
      "Epoch:  321/1000  Loss:  [0.25127961]\n",
      "Epoch:  322/1000  Loss:  [0.25128107]\n",
      "Epoch:  323/1000  Loss:  [0.25128251]\n",
      "Epoch:  324/1000  Loss:  [0.25128394]\n",
      "Epoch:  325/1000  Loss:  [0.25128536]\n",
      "Epoch:  326/1000  Loss:  [0.25128675]\n",
      "Epoch:  327/1000  Loss:  [0.25128814]\n",
      "Epoch:  328/1000  Loss:  [0.2512895]\n",
      "Epoch:  329/1000  Loss:  [0.25129086]\n",
      "Epoch:  330/1000  Loss:  [0.25129219]\n",
      "Epoch:  331/1000  Loss:  [0.25129351]\n",
      "Epoch:  332/1000  Loss:  [0.25129482]\n",
      "Epoch:  333/1000  Loss:  [0.2512961]\n",
      "Epoch:  334/1000  Loss:  [0.25129737]\n",
      "Epoch:  335/1000  Loss:  [0.25129863]\n",
      "Epoch:  336/1000  Loss:  [0.25129987]\n",
      "Epoch:  337/1000  Loss:  [0.25130109]\n",
      "Epoch:  338/1000  Loss:  [0.25130229]\n",
      "Epoch:  339/1000  Loss:  [0.25130348]\n",
      "Epoch:  340/1000  Loss:  [0.25130465]\n",
      "Epoch:  341/1000  Loss:  [0.25130581]\n",
      "Epoch:  342/1000  Loss:  [0.25130694]\n",
      "Epoch:  343/1000  Loss:  [0.25130806]\n",
      "Epoch:  344/1000  Loss:  [0.25130917]\n",
      "Epoch:  345/1000  Loss:  [0.25131025]\n",
      "Epoch:  346/1000  Loss:  [0.25131132]\n",
      "Epoch:  347/1000  Loss:  [0.25131237]\n",
      "Epoch:  348/1000  Loss:  [0.2513134]\n",
      "Epoch:  349/1000  Loss:  [0.25131442]\n",
      "Epoch:  350/1000  Loss:  [0.25131541]\n",
      "Epoch:  351/1000  Loss:  [0.25131639]\n",
      "Epoch:  352/1000  Loss:  [0.25131735]\n",
      "Epoch:  353/1000  Loss:  [0.2513183]\n",
      "Epoch:  354/1000  Loss:  [0.25131922]\n",
      "Epoch:  355/1000  Loss:  [0.25132013]\n",
      "Epoch:  356/1000  Loss:  [0.25132102]\n",
      "Epoch:  357/1000  Loss:  [0.25132189]\n",
      "Epoch:  358/1000  Loss:  [0.25132274]\n",
      "Epoch:  359/1000  Loss:  [0.25132357]\n",
      "Epoch:  360/1000  Loss:  [0.25132439]\n",
      "Epoch:  361/1000  Loss:  [0.25132518]\n",
      "Epoch:  362/1000  Loss:  [0.25132596]\n",
      "Epoch:  363/1000  Loss:  [0.25132672]\n",
      "Epoch:  364/1000  Loss:  [0.25132746]\n",
      "Epoch:  365/1000  Loss:  [0.25132818]\n",
      "Epoch:  366/1000  Loss:  [0.25132888]\n",
      "Epoch:  367/1000  Loss:  [0.25132956]\n",
      "Epoch:  368/1000  Loss:  [0.25133023]\n",
      "Epoch:  369/1000  Loss:  [0.25133087]\n",
      "Epoch:  370/1000  Loss:  [0.2513315]\n",
      "Epoch:  371/1000  Loss:  [0.25133211]\n",
      "Epoch:  372/1000  Loss:  [0.25133269]\n",
      "Epoch:  373/1000  Loss:  [0.25133326]\n",
      "Epoch:  374/1000  Loss:  [0.25133381]\n",
      "Epoch:  375/1000  Loss:  [0.25133434]\n",
      "Epoch:  376/1000  Loss:  [0.25133485]\n",
      "Epoch:  377/1000  Loss:  [0.25133534]\n",
      "Epoch:  378/1000  Loss:  [0.25133581]\n",
      "Epoch:  379/1000  Loss:  [0.25133626]\n",
      "Epoch:  380/1000  Loss:  [0.25133669]\n",
      "Epoch:  381/1000  Loss:  [0.2513371]\n",
      "Epoch:  382/1000  Loss:  [0.2513375]\n",
      "Epoch:  383/1000  Loss:  [0.25133787]\n",
      "Epoch:  384/1000  Loss:  [0.25133822]\n",
      "Epoch:  385/1000  Loss:  [0.25133855]\n",
      "Epoch:  386/1000  Loss:  [0.25133887]\n",
      "Epoch:  387/1000  Loss:  [0.25133916]\n",
      "Epoch:  388/1000  Loss:  [0.25133943]\n",
      "Epoch:  389/1000  Loss:  [0.25133969]\n",
      "Epoch:  390/1000  Loss:  [0.25133992]\n",
      "Epoch:  391/1000  Loss:  [0.25134014]\n",
      "Epoch:  392/1000  Loss:  [0.25134033]\n",
      "Epoch:  393/1000  Loss:  [0.25134051]\n",
      "Epoch:  394/1000  Loss:  [0.25134066]\n",
      "Epoch:  395/1000  Loss:  [0.2513408]\n",
      "Epoch:  396/1000  Loss:  [0.25134091]\n",
      "Epoch:  397/1000  Loss:  [0.25134101]\n",
      "Epoch:  398/1000  Loss:  [0.25134108]\n",
      "Epoch:  399/1000  Loss:  [0.25134114]\n",
      "Epoch:  400/1000  Loss:  [0.25134117]\n",
      "Epoch:  401/1000  Loss:  [0.25134119]\n",
      "Epoch:  402/1000  Loss:  [0.25134118]\n",
      "Epoch:  403/1000  Loss:  [0.25134116]\n",
      "Epoch:  404/1000  Loss:  [0.25134111]\n",
      "Epoch:  405/1000  Loss:  [0.25134105]\n",
      "Epoch:  406/1000  Loss:  [0.25134097]\n",
      "Epoch:  407/1000  Loss:  [0.25134086]\n",
      "Epoch:  408/1000  Loss:  [0.25134074]\n",
      "Epoch:  409/1000  Loss:  [0.2513406]\n",
      "Epoch:  410/1000  Loss:  [0.25134043]\n",
      "Epoch:  411/1000  Loss:  [0.25134025]\n",
      "Epoch:  412/1000  Loss:  [0.25134005]\n",
      "Epoch:  413/1000  Loss:  [0.25133983]\n",
      "Epoch:  414/1000  Loss:  [0.25133959]\n",
      "Epoch:  415/1000  Loss:  [0.25133932]\n",
      "Epoch:  416/1000  Loss:  [0.25133904]\n",
      "Epoch:  417/1000  Loss:  [0.25133874]\n",
      "Epoch:  418/1000  Loss:  [0.25133842]\n",
      "Epoch:  419/1000  Loss:  [0.25133808]\n",
      "Epoch:  420/1000  Loss:  [0.25133773]\n",
      "Epoch:  421/1000  Loss:  [0.25133735]\n",
      "Epoch:  422/1000  Loss:  [0.25133695]\n",
      "Epoch:  423/1000  Loss:  [0.25133653]\n",
      "Epoch:  424/1000  Loss:  [0.2513361]\n",
      "Epoch:  425/1000  Loss:  [0.25133564]\n",
      "Epoch:  426/1000  Loss:  [0.25133517]\n",
      "Epoch:  427/1000  Loss:  [0.25133468]\n",
      "Epoch:  428/1000  Loss:  [0.25133416]\n",
      "Epoch:  429/1000  Loss:  [0.25133363]\n",
      "Epoch:  430/1000  Loss:  [0.25133308]\n",
      "Epoch:  431/1000  Loss:  [0.25133251]\n",
      "Epoch:  432/1000  Loss:  [0.25133193]\n",
      "Epoch:  433/1000  Loss:  [0.25133132]\n",
      "Epoch:  434/1000  Loss:  [0.25133069]\n",
      "Epoch:  435/1000  Loss:  [0.25133005]\n",
      "Epoch:  436/1000  Loss:  [0.25132939]\n",
      "Epoch:  437/1000  Loss:  [0.25132871]\n",
      "Epoch:  438/1000  Loss:  [0.25132801]\n",
      "Epoch:  439/1000  Loss:  [0.25132729]\n",
      "Epoch:  440/1000  Loss:  [0.25132656]\n",
      "Epoch:  441/1000  Loss:  [0.25132581]\n",
      "Epoch:  442/1000  Loss:  [0.25132504]\n",
      "Epoch:  443/1000  Loss:  [0.25132425]\n",
      "Epoch:  444/1000  Loss:  [0.25132344]\n",
      "Epoch:  445/1000  Loss:  [0.25132262]\n",
      "Epoch:  446/1000  Loss:  [0.25132177]\n",
      "Epoch:  447/1000  Loss:  [0.25132091]\n",
      "Epoch:  448/1000  Loss:  [0.25132004]\n",
      "Epoch:  449/1000  Loss:  [0.25131914]\n",
      "Epoch:  450/1000  Loss:  [0.25131823]\n",
      "Epoch:  451/1000  Loss:  [0.2513173]\n",
      "Epoch:  452/1000  Loss:  [0.25131636]\n",
      "Epoch:  453/1000  Loss:  [0.25131539]\n",
      "Epoch:  454/1000  Loss:  [0.25131441]\n",
      "Epoch:  455/1000  Loss:  [0.25131342]\n",
      "Epoch:  456/1000  Loss:  [0.25131241]\n",
      "Epoch:  457/1000  Loss:  [0.25131138]\n",
      "Epoch:  458/1000  Loss:  [0.25131033]\n",
      "Epoch:  459/1000  Loss:  [0.25130927]\n",
      "Epoch:  460/1000  Loss:  [0.25130819]\n",
      "Epoch:  461/1000  Loss:  [0.2513071]\n",
      "Epoch:  462/1000  Loss:  [0.25130599]\n",
      "Epoch:  463/1000  Loss:  [0.25130486]\n",
      "Epoch:  464/1000  Loss:  [0.25130372]\n",
      "Epoch:  465/1000  Loss:  [0.25130256]\n",
      "Epoch:  466/1000  Loss:  [0.25130139]\n",
      "Epoch:  467/1000  Loss:  [0.2513002]\n",
      "Epoch:  468/1000  Loss:  [0.251299]\n",
      "Epoch:  469/1000  Loss:  [0.25129778]\n",
      "Epoch:  470/1000  Loss:  [0.25129655]\n",
      "Epoch:  471/1000  Loss:  [0.2512953]\n",
      "Epoch:  472/1000  Loss:  [0.25129404]\n",
      "Epoch:  473/1000  Loss:  [0.25129276]\n",
      "Epoch:  474/1000  Loss:  [0.25129147]\n",
      "Epoch:  475/1000  Loss:  [0.25129017]\n",
      "Epoch:  476/1000  Loss:  [0.25128885]\n",
      "Epoch:  477/1000  Loss:  [0.25128752]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  478/1000  Loss:  [0.25128617]\n",
      "Epoch:  479/1000  Loss:  [0.25128481]\n",
      "Epoch:  480/1000  Loss:  [0.25128343]\n",
      "Epoch:  481/1000  Loss:  [0.25128205]\n",
      "Epoch:  482/1000  Loss:  [0.25128065]\n",
      "Epoch:  483/1000  Loss:  [0.25127923]\n",
      "Epoch:  484/1000  Loss:  [0.25127781]\n",
      "Epoch:  485/1000  Loss:  [0.25127637]\n",
      "Epoch:  486/1000  Loss:  [0.25127492]\n",
      "Epoch:  487/1000  Loss:  [0.25127345]\n",
      "Epoch:  488/1000  Loss:  [0.25127198]\n",
      "Epoch:  489/1000  Loss:  [0.25127049]\n",
      "Epoch:  490/1000  Loss:  [0.25126899]\n",
      "Epoch:  491/1000  Loss:  [0.25126747]\n",
      "Epoch:  492/1000  Loss:  [0.25126595]\n",
      "Epoch:  493/1000  Loss:  [0.25126442]\n",
      "Epoch:  494/1000  Loss:  [0.25126287]\n",
      "Epoch:  495/1000  Loss:  [0.25126131]\n",
      "Epoch:  496/1000  Loss:  [0.25125974]\n",
      "Epoch:  497/1000  Loss:  [0.25125816]\n",
      "Epoch:  498/1000  Loss:  [0.25125657]\n",
      "Epoch:  499/1000  Loss:  [0.25125497]\n",
      "Epoch:  500/1000  Loss:  [0.25125336]\n",
      "Epoch:  501/1000  Loss:  [0.25125174]\n",
      "Epoch:  502/1000  Loss:  [0.25125011]\n",
      "Epoch:  503/1000  Loss:  [0.25124847]\n",
      "Epoch:  504/1000  Loss:  [0.25124682]\n",
      "Epoch:  505/1000  Loss:  [0.25124516]\n",
      "Epoch:  506/1000  Loss:  [0.25124349]\n",
      "Epoch:  507/1000  Loss:  [0.25124181]\n",
      "Epoch:  508/1000  Loss:  [0.25124012]\n",
      "Epoch:  509/1000  Loss:  [0.25123843]\n",
      "Epoch:  510/1000  Loss:  [0.25123672]\n",
      "Epoch:  511/1000  Loss:  [0.25123501]\n",
      "Epoch:  512/1000  Loss:  [0.25123329]\n",
      "Epoch:  513/1000  Loss:  [0.25123156]\n",
      "Epoch:  514/1000  Loss:  [0.25122982]\n",
      "Epoch:  515/1000  Loss:  [0.25122808]\n",
      "Epoch:  516/1000  Loss:  [0.25122632]\n",
      "Epoch:  517/1000  Loss:  [0.25122456]\n",
      "Epoch:  518/1000  Loss:  [0.2512228]\n",
      "Epoch:  519/1000  Loss:  [0.25122102]\n",
      "Epoch:  520/1000  Loss:  [0.25121924]\n",
      "Epoch:  521/1000  Loss:  [0.25121746]\n",
      "Epoch:  522/1000  Loss:  [0.25121566]\n",
      "Epoch:  523/1000  Loss:  [0.25121386]\n",
      "Epoch:  524/1000  Loss:  [0.25121206]\n",
      "Epoch:  525/1000  Loss:  [0.25121024]\n",
      "Epoch:  526/1000  Loss:  [0.25120843]\n",
      "Epoch:  527/1000  Loss:  [0.2512066]\n",
      "Epoch:  528/1000  Loss:  [0.25120477]\n",
      "Epoch:  529/1000  Loss:  [0.25120294]\n",
      "Epoch:  530/1000  Loss:  [0.2512011]\n",
      "Epoch:  531/1000  Loss:  [0.25119926]\n",
      "Epoch:  532/1000  Loss:  [0.25119741]\n",
      "Epoch:  533/1000  Loss:  [0.25119556]\n",
      "Epoch:  534/1000  Loss:  [0.2511937]\n",
      "Epoch:  535/1000  Loss:  [0.25119184]\n",
      "Epoch:  536/1000  Loss:  [0.25118998]\n",
      "Epoch:  537/1000  Loss:  [0.25118811]\n",
      "Epoch:  538/1000  Loss:  [0.25118624]\n",
      "Epoch:  539/1000  Loss:  [0.25118436]\n",
      "Epoch:  540/1000  Loss:  [0.25118248]\n",
      "Epoch:  541/1000  Loss:  [0.2511806]\n",
      "Epoch:  542/1000  Loss:  [0.25117872]\n",
      "Epoch:  543/1000  Loss:  [0.25117683]\n",
      "Epoch:  544/1000  Loss:  [0.25117494]\n",
      "Epoch:  545/1000  Loss:  [0.25117305]\n",
      "Epoch:  546/1000  Loss:  [0.25117116]\n",
      "Epoch:  547/1000  Loss:  [0.25116927]\n",
      "Epoch:  548/1000  Loss:  [0.25116737]\n",
      "Epoch:  549/1000  Loss:  [0.25116547]\n",
      "Epoch:  550/1000  Loss:  [0.25116357]\n",
      "Epoch:  551/1000  Loss:  [0.25116167]\n",
      "Epoch:  552/1000  Loss:  [0.25115977]\n",
      "Epoch:  553/1000  Loss:  [0.25115787]\n",
      "Epoch:  554/1000  Loss:  [0.25115597]\n",
      "Epoch:  555/1000  Loss:  [0.25115406]\n",
      "Epoch:  556/1000  Loss:  [0.25115216]\n",
      "Epoch:  557/1000  Loss:  [0.25115026]\n",
      "Epoch:  558/1000  Loss:  [0.25114835]\n",
      "Epoch:  559/1000  Loss:  [0.25114645]\n",
      "Epoch:  560/1000  Loss:  [0.25114455]\n",
      "Epoch:  561/1000  Loss:  [0.25114264]\n",
      "Epoch:  562/1000  Loss:  [0.25114074]\n",
      "Epoch:  563/1000  Loss:  [0.25113884]\n",
      "Epoch:  564/1000  Loss:  [0.25113694]\n",
      "Epoch:  565/1000  Loss:  [0.25113505]\n",
      "Epoch:  566/1000  Loss:  [0.25113315]\n",
      "Epoch:  567/1000  Loss:  [0.25113126]\n",
      "Epoch:  568/1000  Loss:  [0.25112936]\n",
      "Epoch:  569/1000  Loss:  [0.25112747]\n",
      "Epoch:  570/1000  Loss:  [0.25112559]\n",
      "Epoch:  571/1000  Loss:  [0.2511237]\n",
      "Epoch:  572/1000  Loss:  [0.25112182]\n",
      "Epoch:  573/1000  Loss:  [0.25111994]\n",
      "Epoch:  574/1000  Loss:  [0.25111806]\n",
      "Epoch:  575/1000  Loss:  [0.25111618]\n",
      "Epoch:  576/1000  Loss:  [0.25111431]\n",
      "Epoch:  577/1000  Loss:  [0.25111244]\n",
      "Epoch:  578/1000  Loss:  [0.25111058]\n",
      "Epoch:  579/1000  Loss:  [0.25110872]\n",
      "Epoch:  580/1000  Loss:  [0.25110686]\n",
      "Epoch:  581/1000  Loss:  [0.25110501]\n",
      "Epoch:  582/1000  Loss:  [0.25110316]\n",
      "Epoch:  583/1000  Loss:  [0.25110132]\n",
      "Epoch:  584/1000  Loss:  [0.25109948]\n",
      "Epoch:  585/1000  Loss:  [0.25109764]\n",
      "Epoch:  586/1000  Loss:  [0.25109581]\n",
      "Epoch:  587/1000  Loss:  [0.25109398]\n",
      "Epoch:  588/1000  Loss:  [0.25109216]\n",
      "Epoch:  589/1000  Loss:  [0.25109035]\n",
      "Epoch:  590/1000  Loss:  [0.25108854]\n",
      "Epoch:  591/1000  Loss:  [0.25108674]\n",
      "Epoch:  592/1000  Loss:  [0.25108494]\n",
      "Epoch:  593/1000  Loss:  [0.25108314]\n",
      "Epoch:  594/1000  Loss:  [0.25108136]\n",
      "Epoch:  595/1000  Loss:  [0.25107958]\n",
      "Epoch:  596/1000  Loss:  [0.2510778]\n",
      "Epoch:  597/1000  Loss:  [0.25107604]\n",
      "Epoch:  598/1000  Loss:  [0.25107427]\n",
      "Epoch:  599/1000  Loss:  [0.25107252]\n",
      "Epoch:  600/1000  Loss:  [0.25107077]\n",
      "Epoch:  601/1000  Loss:  [0.25106903]\n",
      "Epoch:  602/1000  Loss:  [0.2510673]\n",
      "Epoch:  603/1000  Loss:  [0.25106557]\n",
      "Epoch:  604/1000  Loss:  [0.25106385]\n",
      "Epoch:  605/1000  Loss:  [0.25106214]\n",
      "Epoch:  606/1000  Loss:  [0.25106044]\n",
      "Epoch:  607/1000  Loss:  [0.25105874]\n",
      "Epoch:  608/1000  Loss:  [0.25105706]\n",
      "Epoch:  609/1000  Loss:  [0.25105538]\n",
      "Epoch:  610/1000  Loss:  [0.2510537]\n",
      "Epoch:  611/1000  Loss:  [0.25105204]\n",
      "Epoch:  612/1000  Loss:  [0.25105039]\n",
      "Epoch:  613/1000  Loss:  [0.25104874]\n",
      "Epoch:  614/1000  Loss:  [0.2510471]\n",
      "Epoch:  615/1000  Loss:  [0.25104547]\n",
      "Epoch:  616/1000  Loss:  [0.25104385]\n",
      "Epoch:  617/1000  Loss:  [0.25104224]\n",
      "Epoch:  618/1000  Loss:  [0.25104064]\n",
      "Epoch:  619/1000  Loss:  [0.25103905]\n",
      "Epoch:  620/1000  Loss:  [0.25103747]\n",
      "Epoch:  621/1000  Loss:  [0.25103589]\n",
      "Epoch:  622/1000  Loss:  [0.25103433]\n",
      "Epoch:  623/1000  Loss:  [0.25103278]\n",
      "Epoch:  624/1000  Loss:  [0.25103123]\n",
      "Epoch:  625/1000  Loss:  [0.2510297]\n",
      "Epoch:  626/1000  Loss:  [0.25102817]\n",
      "Epoch:  627/1000  Loss:  [0.25102666]\n",
      "Epoch:  628/1000  Loss:  [0.25102516]\n",
      "Epoch:  629/1000  Loss:  [0.25102366]\n",
      "Epoch:  630/1000  Loss:  [0.25102218]\n",
      "Epoch:  631/1000  Loss:  [0.25102071]\n",
      "Epoch:  632/1000  Loss:  [0.25101925]\n",
      "Epoch:  633/1000  Loss:  [0.2510178]\n",
      "Epoch:  634/1000  Loss:  [0.25101636]\n",
      "Epoch:  635/1000  Loss:  [0.25101493]\n",
      "Epoch:  636/1000  Loss:  [0.25101351]\n",
      "Epoch:  637/1000  Loss:  [0.2510121]\n",
      "Epoch:  638/1000  Loss:  [0.25101071]\n",
      "Epoch:  639/1000  Loss:  [0.25100932]\n",
      "Epoch:  640/1000  Loss:  [0.25100795]\n",
      "Epoch:  641/1000  Loss:  [0.25100659]\n",
      "Epoch:  642/1000  Loss:  [0.25100524]\n",
      "Epoch:  643/1000  Loss:  [0.2510039]\n",
      "Epoch:  644/1000  Loss:  [0.25100257]\n",
      "Epoch:  645/1000  Loss:  [0.25100126]\n",
      "Epoch:  646/1000  Loss:  [0.25099996]\n",
      "Epoch:  647/1000  Loss:  [0.25099866]\n",
      "Epoch:  648/1000  Loss:  [0.25099739]\n",
      "Epoch:  649/1000  Loss:  [0.25099612]\n",
      "Epoch:  650/1000  Loss:  [0.25099486]\n",
      "Epoch:  651/1000  Loss:  [0.25099362]\n",
      "Epoch:  652/1000  Loss:  [0.25099239]\n",
      "Epoch:  653/1000  Loss:  [0.25099117]\n",
      "Epoch:  654/1000  Loss:  [0.25098997]\n",
      "Epoch:  655/1000  Loss:  [0.25098878]\n",
      "Epoch:  656/1000  Loss:  [0.2509876]\n",
      "Epoch:  657/1000  Loss:  [0.25098643]\n",
      "Epoch:  658/1000  Loss:  [0.25098528]\n",
      "Epoch:  659/1000  Loss:  [0.25098413]\n",
      "Epoch:  660/1000  Loss:  [0.25098301]\n",
      "Epoch:  661/1000  Loss:  [0.25098189]\n",
      "Epoch:  662/1000  Loss:  [0.25098079]\n",
      "Epoch:  663/1000  Loss:  [0.2509797]\n",
      "Epoch:  664/1000  Loss:  [0.25097862]\n",
      "Epoch:  665/1000  Loss:  [0.25097756]\n",
      "Epoch:  666/1000  Loss:  [0.25097651]\n",
      "Epoch:  667/1000  Loss:  [0.25097547]\n",
      "Epoch:  668/1000  Loss:  [0.25097444]\n",
      "Epoch:  669/1000  Loss:  [0.25097343]\n",
      "Epoch:  670/1000  Loss:  [0.25097244]\n",
      "Epoch:  671/1000  Loss:  [0.25097145]\n",
      "Epoch:  672/1000  Loss:  [0.25097048]\n",
      "Epoch:  673/1000  Loss:  [0.25096953]\n",
      "Epoch:  674/1000  Loss:  [0.25096858]\n",
      "Epoch:  675/1000  Loss:  [0.25096765]\n",
      "Epoch:  676/1000  Loss:  [0.25096674]\n",
      "Epoch:  677/1000  Loss:  [0.25096583]\n",
      "Epoch:  678/1000  Loss:  [0.25096495]\n",
      "Epoch:  679/1000  Loss:  [0.25096407]\n",
      "Epoch:  680/1000  Loss:  [0.25096321]\n",
      "Epoch:  681/1000  Loss:  [0.25096236]\n",
      "Epoch:  682/1000  Loss:  [0.25096153]\n",
      "Epoch:  683/1000  Loss:  [0.25096071]\n",
      "Epoch:  684/1000  Loss:  [0.2509599]\n",
      "Epoch:  685/1000  Loss:  [0.25095911]\n",
      "Epoch:  686/1000  Loss:  [0.25095834]\n",
      "Epoch:  687/1000  Loss:  [0.25095757]\n",
      "Epoch:  688/1000  Loss:  [0.25095682]\n",
      "Epoch:  689/1000  Loss:  [0.25095609]\n",
      "Epoch:  690/1000  Loss:  [0.25095536]\n",
      "Epoch:  691/1000  Loss:  [0.25095466]\n",
      "Epoch:  692/1000  Loss:  [0.25095396]\n",
      "Epoch:  693/1000  Loss:  [0.25095328]\n",
      "Epoch:  694/1000  Loss:  [0.25095262]\n",
      "Epoch:  695/1000  Loss:  [0.25095197]\n",
      "Epoch:  696/1000  Loss:  [0.25095133]\n",
      "Epoch:  697/1000  Loss:  [0.25095071]\n",
      "Epoch:  698/1000  Loss:  [0.2509501]\n",
      "Epoch:  699/1000  Loss:  [0.25094951]\n",
      "Epoch:  700/1000  Loss:  [0.25094893]\n",
      "Epoch:  701/1000  Loss:  [0.25094836]\n",
      "Epoch:  702/1000  Loss:  [0.25094781]\n",
      "Epoch:  703/1000  Loss:  [0.25094727]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  704/1000  Loss:  [0.25094675]\n",
      "Epoch:  705/1000  Loss:  [0.25094624]\n",
      "Epoch:  706/1000  Loss:  [0.25094575]\n",
      "Epoch:  707/1000  Loss:  [0.25094527]\n",
      "Epoch:  708/1000  Loss:  [0.2509448]\n",
      "Epoch:  709/1000  Loss:  [0.25094435]\n",
      "Epoch:  710/1000  Loss:  [0.25094391]\n",
      "Epoch:  711/1000  Loss:  [0.25094349]\n",
      "Epoch:  712/1000  Loss:  [0.25094308]\n",
      "Epoch:  713/1000  Loss:  [0.25094269]\n",
      "Epoch:  714/1000  Loss:  [0.2509423]\n",
      "Epoch:  715/1000  Loss:  [0.25094194]\n",
      "Epoch:  716/1000  Loss:  [0.25094159]\n",
      "Epoch:  717/1000  Loss:  [0.25094125]\n",
      "Epoch:  718/1000  Loss:  [0.25094093]\n",
      "Epoch:  719/1000  Loss:  [0.25094062]\n",
      "Epoch:  720/1000  Loss:  [0.25094032]\n",
      "Epoch:  721/1000  Loss:  [0.25094004]\n",
      "Epoch:  722/1000  Loss:  [0.25093977]\n",
      "Epoch:  723/1000  Loss:  [0.25093952]\n",
      "Epoch:  724/1000  Loss:  [0.25093928]\n",
      "Epoch:  725/1000  Loss:  [0.25093906]\n",
      "Epoch:  726/1000  Loss:  [0.25093885]\n",
      "Epoch:  727/1000  Loss:  [0.25093865]\n",
      "Epoch:  728/1000  Loss:  [0.25093847]\n",
      "Epoch:  729/1000  Loss:  [0.2509383]\n",
      "Epoch:  730/1000  Loss:  [0.25093815]\n",
      "Epoch:  731/1000  Loss:  [0.25093801]\n",
      "Epoch:  732/1000  Loss:  [0.25093788]\n",
      "Epoch:  733/1000  Loss:  [0.25093777]\n",
      "Epoch:  734/1000  Loss:  [0.25093767]\n",
      "Epoch:  735/1000  Loss:  [0.25093759]\n",
      "Epoch:  736/1000  Loss:  [0.25093752]\n",
      "Epoch:  737/1000  Loss:  [0.25093746]\n",
      "Epoch:  738/1000  Loss:  [0.25093742]\n",
      "Epoch:  739/1000  Loss:  [0.25093739]\n",
      "Epoch:  740/1000  Loss:  [0.25093738]\n",
      "Epoch:  741/1000  Loss:  [0.25093737]\n",
      "Epoch:  742/1000  Loss:  [0.25093739]\n",
      "Epoch:  743/1000  Loss:  [0.25093741]\n",
      "Epoch:  744/1000  Loss:  [0.25093745]\n",
      "Epoch:  745/1000  Loss:  [0.25093751]\n",
      "Epoch:  746/1000  Loss:  [0.25093757]\n",
      "Epoch:  747/1000  Loss:  [0.25093765]\n",
      "Epoch:  748/1000  Loss:  [0.25093775]\n",
      "Epoch:  749/1000  Loss:  [0.25093785]\n",
      "Epoch:  750/1000  Loss:  [0.25093797]\n",
      "Epoch:  751/1000  Loss:  [0.25093811]\n",
      "Epoch:  752/1000  Loss:  [0.25093825]\n",
      "Epoch:  753/1000  Loss:  [0.25093841]\n",
      "Epoch:  754/1000  Loss:  [0.25093859]\n",
      "Epoch:  755/1000  Loss:  [0.25093877]\n",
      "Epoch:  756/1000  Loss:  [0.25093897]\n",
      "Epoch:  757/1000  Loss:  [0.25093918]\n",
      "Epoch:  758/1000  Loss:  [0.25093941]\n",
      "Epoch:  759/1000  Loss:  [0.25093965]\n",
      "Epoch:  760/1000  Loss:  [0.2509399]\n",
      "Epoch:  761/1000  Loss:  [0.25094016]\n",
      "Epoch:  762/1000  Loss:  [0.25094044]\n",
      "Epoch:  763/1000  Loss:  [0.25094072]\n",
      "Epoch:  764/1000  Loss:  [0.25094103]\n",
      "Epoch:  765/1000  Loss:  [0.25094134]\n",
      "Epoch:  766/1000  Loss:  [0.25094167]\n",
      "Epoch:  767/1000  Loss:  [0.25094201]\n",
      "Epoch:  768/1000  Loss:  [0.25094236]\n",
      "Epoch:  769/1000  Loss:  [0.25094272]\n",
      "Epoch:  770/1000  Loss:  [0.2509431]\n",
      "Epoch:  771/1000  Loss:  [0.25094348]\n",
      "Epoch:  772/1000  Loss:  [0.25094388]\n",
      "Epoch:  773/1000  Loss:  [0.25094429]\n",
      "Epoch:  774/1000  Loss:  [0.25094472]\n",
      "Epoch:  775/1000  Loss:  [0.25094515]\n",
      "Epoch:  776/1000  Loss:  [0.2509456]\n",
      "Epoch:  777/1000  Loss:  [0.25094606]\n",
      "Epoch:  778/1000  Loss:  [0.25094653]\n",
      "Epoch:  779/1000  Loss:  [0.25094701]\n",
      "Epoch:  780/1000  Loss:  [0.25094751]\n",
      "Epoch:  781/1000  Loss:  [0.25094801]\n",
      "Epoch:  782/1000  Loss:  [0.25094853]\n",
      "Epoch:  783/1000  Loss:  [0.25094905]\n",
      "Epoch:  784/1000  Loss:  [0.25094959]\n",
      "Epoch:  785/1000  Loss:  [0.25095014]\n",
      "Epoch:  786/1000  Loss:  [0.2509507]\n",
      "Epoch:  787/1000  Loss:  [0.25095127]\n",
      "Epoch:  788/1000  Loss:  [0.25095185]\n",
      "Epoch:  789/1000  Loss:  [0.25095244]\n",
      "Epoch:  790/1000  Loss:  [0.25095305]\n",
      "Epoch:  791/1000  Loss:  [0.25095366]\n",
      "Epoch:  792/1000  Loss:  [0.25095428]\n",
      "Epoch:  793/1000  Loss:  [0.25095491]\n",
      "Epoch:  794/1000  Loss:  [0.25095556]\n",
      "Epoch:  795/1000  Loss:  [0.25095621]\n",
      "Epoch:  796/1000  Loss:  [0.25095687]\n",
      "Epoch:  797/1000  Loss:  [0.25095755]\n",
      "Epoch:  798/1000  Loss:  [0.25095823]\n",
      "Epoch:  799/1000  Loss:  [0.25095892]\n",
      "Epoch:  800/1000  Loss:  [0.25095962]\n",
      "Epoch:  801/1000  Loss:  [0.25096033]\n",
      "Epoch:  802/1000  Loss:  [0.25096105]\n",
      "Epoch:  803/1000  Loss:  [0.25096178]\n",
      "Epoch:  804/1000  Loss:  [0.25096252]\n",
      "Epoch:  805/1000  Loss:  [0.25096326]\n",
      "Epoch:  806/1000  Loss:  [0.25096402]\n",
      "Epoch:  807/1000  Loss:  [0.25096478]\n",
      "Epoch:  808/1000  Loss:  [0.25096555]\n",
      "Epoch:  809/1000  Loss:  [0.25096633]\n",
      "Epoch:  810/1000  Loss:  [0.25096712]\n",
      "Epoch:  811/1000  Loss:  [0.25096791]\n",
      "Epoch:  812/1000  Loss:  [0.25096871]\n",
      "Epoch:  813/1000  Loss:  [0.25096952]\n",
      "Epoch:  814/1000  Loss:  [0.25097034]\n",
      "Epoch:  815/1000  Loss:  [0.25097117]\n",
      "Epoch:  816/1000  Loss:  [0.250972]\n",
      "Epoch:  817/1000  Loss:  [0.25097284]\n",
      "Epoch:  818/1000  Loss:  [0.25097368]\n",
      "Epoch:  819/1000  Loss:  [0.25097454]\n",
      "Epoch:  820/1000  Loss:  [0.2509754]\n",
      "Epoch:  821/1000  Loss:  [0.25097626]\n",
      "Epoch:  822/1000  Loss:  [0.25097713]\n",
      "Epoch:  823/1000  Loss:  [0.25097801]\n",
      "Epoch:  824/1000  Loss:  [0.25097889]\n",
      "Epoch:  825/1000  Loss:  [0.25097978]\n",
      "Epoch:  826/1000  Loss:  [0.25098067]\n",
      "Epoch:  827/1000  Loss:  [0.25098157]\n",
      "Epoch:  828/1000  Loss:  [0.25098247]\n",
      "Epoch:  829/1000  Loss:  [0.25098338]\n",
      "Epoch:  830/1000  Loss:  [0.25098429]\n",
      "Epoch:  831/1000  Loss:  [0.25098521]\n",
      "Epoch:  832/1000  Loss:  [0.25098613]\n",
      "Epoch:  833/1000  Loss:  [0.25098706]\n",
      "Epoch:  834/1000  Loss:  [0.25098798]\n",
      "Epoch:  835/1000  Loss:  [0.25098891]\n",
      "Epoch:  836/1000  Loss:  [0.25098985]\n",
      "Epoch:  837/1000  Loss:  [0.25099079]\n",
      "Epoch:  838/1000  Loss:  [0.25099173]\n",
      "Epoch:  839/1000  Loss:  [0.25099267]\n",
      "Epoch:  840/1000  Loss:  [0.25099361]\n",
      "Epoch:  841/1000  Loss:  [0.25099456]\n",
      "Epoch:  842/1000  Loss:  [0.25099551]\n",
      "Epoch:  843/1000  Loss:  [0.25099646]\n",
      "Epoch:  844/1000  Loss:  [0.2509974]\n",
      "Epoch:  845/1000  Loss:  [0.25099836]\n",
      "Epoch:  846/1000  Loss:  [0.25099931]\n",
      "Epoch:  847/1000  Loss:  [0.25100026]\n",
      "Epoch:  848/1000  Loss:  [0.25100121]\n",
      "Epoch:  849/1000  Loss:  [0.25100216]\n",
      "Epoch:  850/1000  Loss:  [0.25100311]\n",
      "Epoch:  851/1000  Loss:  [0.25100406]\n",
      "Epoch:  852/1000  Loss:  [0.251005]\n",
      "Epoch:  853/1000  Loss:  [0.25100595]\n",
      "Epoch:  854/1000  Loss:  [0.25100689]\n",
      "Epoch:  855/1000  Loss:  [0.25100783]\n",
      "Epoch:  856/1000  Loss:  [0.25100877]\n",
      "Epoch:  857/1000  Loss:  [0.2510097]\n",
      "Epoch:  858/1000  Loss:  [0.25101063]\n",
      "Epoch:  859/1000  Loss:  [0.25101156]\n",
      "Epoch:  860/1000  Loss:  [0.25101248]\n",
      "Epoch:  861/1000  Loss:  [0.2510134]\n",
      "Epoch:  862/1000  Loss:  [0.25101431]\n",
      "Epoch:  863/1000  Loss:  [0.25101522]\n",
      "Epoch:  864/1000  Loss:  [0.25101611]\n",
      "Epoch:  865/1000  Loss:  [0.25101701]\n",
      "Epoch:  866/1000  Loss:  [0.25101789]\n",
      "Epoch:  867/1000  Loss:  [0.25101877]\n",
      "Epoch:  868/1000  Loss:  [0.25101964]\n",
      "Epoch:  869/1000  Loss:  [0.2510205]\n",
      "Epoch:  870/1000  Loss:  [0.25102136]\n",
      "Epoch:  871/1000  Loss:  [0.2510222]\n",
      "Epoch:  872/1000  Loss:  [0.25102303]\n",
      "Epoch:  873/1000  Loss:  [0.25102385]\n",
      "Epoch:  874/1000  Loss:  [0.25102467]\n",
      "Epoch:  875/1000  Loss:  [0.25102546]\n",
      "Epoch:  876/1000  Loss:  [0.25102625]\n",
      "Epoch:  877/1000  Loss:  [0.25102702]\n",
      "Epoch:  878/1000  Loss:  [0.25102778]\n",
      "Epoch:  879/1000  Loss:  [0.25102853]\n",
      "Epoch:  880/1000  Loss:  [0.25102926]\n",
      "Epoch:  881/1000  Loss:  [0.25102997]\n",
      "Epoch:  882/1000  Loss:  [0.25103067]\n",
      "Epoch:  883/1000  Loss:  [0.25103135]\n",
      "Epoch:  884/1000  Loss:  [0.25103202]\n",
      "Epoch:  885/1000  Loss:  [0.25103266]\n",
      "Epoch:  886/1000  Loss:  [0.25103329]\n",
      "Epoch:  887/1000  Loss:  [0.25103389]\n",
      "Epoch:  888/1000  Loss:  [0.25103448]\n",
      "Epoch:  889/1000  Loss:  [0.25103504]\n",
      "Epoch:  890/1000  Loss:  [0.25103558]\n",
      "Epoch:  891/1000  Loss:  [0.2510361]\n",
      "Epoch:  892/1000  Loss:  [0.25103659]\n",
      "Epoch:  893/1000  Loss:  [0.25103706]\n",
      "Epoch:  894/1000  Loss:  [0.2510375]\n",
      "Epoch:  895/1000  Loss:  [0.25103791]\n",
      "Epoch:  896/1000  Loss:  [0.2510383]\n",
      "Epoch:  897/1000  Loss:  [0.25103865]\n",
      "Epoch:  898/1000  Loss:  [0.25103898]\n",
      "Epoch:  899/1000  Loss:  [0.25103927]\n",
      "Epoch:  900/1000  Loss:  [0.25103953]\n",
      "Epoch:  901/1000  Loss:  [0.25103976]\n",
      "Epoch:  902/1000  Loss:  [0.25103995]\n",
      "Epoch:  903/1000  Loss:  [0.2510401]\n",
      "Epoch:  904/1000  Loss:  [0.25104022]\n",
      "Epoch:  905/1000  Loss:  [0.2510403]\n",
      "Epoch:  906/1000  Loss:  [0.25104033]\n",
      "Epoch:  907/1000  Loss:  [0.25104033]\n",
      "Epoch:  908/1000  Loss:  [0.25104028]\n",
      "Epoch:  909/1000  Loss:  [0.25104019]\n",
      "Epoch:  910/1000  Loss:  [0.25104005]\n",
      "Epoch:  911/1000  Loss:  [0.25103987]\n",
      "Epoch:  912/1000  Loss:  [0.25103963]\n",
      "Epoch:  913/1000  Loss:  [0.25103934]\n",
      "Epoch:  914/1000  Loss:  [0.251039]\n",
      "Epoch:  915/1000  Loss:  [0.25103861]\n",
      "Epoch:  916/1000  Loss:  [0.25103815]\n",
      "Epoch:  917/1000  Loss:  [0.25103764]\n",
      "Epoch:  918/1000  Loss:  [0.25103707]\n",
      "Epoch:  919/1000  Loss:  [0.25103644]\n",
      "Epoch:  920/1000  Loss:  [0.25103574]\n",
      "Epoch:  921/1000  Loss:  [0.25103497]\n",
      "Epoch:  922/1000  Loss:  [0.25103414]\n",
      "Epoch:  923/1000  Loss:  [0.25103323]\n",
      "Epoch:  924/1000  Loss:  [0.25103225]\n",
      "Epoch:  925/1000  Loss:  [0.2510312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  926/1000  Loss:  [0.25103006]\n",
      "Epoch:  927/1000  Loss:  [0.25102885]\n",
      "Epoch:  928/1000  Loss:  [0.25102755]\n",
      "Epoch:  929/1000  Loss:  [0.25102616]\n",
      "Epoch:  930/1000  Loss:  [0.25102468]\n",
      "Epoch:  931/1000  Loss:  [0.25102311]\n",
      "Epoch:  932/1000  Loss:  [0.25102144]\n",
      "Epoch:  933/1000  Loss:  [0.25101968]\n",
      "Epoch:  934/1000  Loss:  [0.25101781]\n",
      "Epoch:  935/1000  Loss:  [0.25101583]\n",
      "Epoch:  936/1000  Loss:  [0.25101375]\n",
      "Epoch:  937/1000  Loss:  [0.25101155]\n",
      "Epoch:  938/1000  Loss:  [0.25100923]\n",
      "Epoch:  939/1000  Loss:  [0.25100679]\n",
      "Epoch:  940/1000  Loss:  [0.25100423]\n",
      "Epoch:  941/1000  Loss:  [0.25100154]\n",
      "Epoch:  942/1000  Loss:  [0.25099871]\n",
      "Epoch:  943/1000  Loss:  [0.25099575]\n",
      "Epoch:  944/1000  Loss:  [0.25099264]\n",
      "Epoch:  945/1000  Loss:  [0.25098939]\n",
      "Epoch:  946/1000  Loss:  [0.25098598]\n",
      "Epoch:  947/1000  Loss:  [0.25098242]\n",
      "Epoch:  948/1000  Loss:  [0.25097869]\n",
      "Epoch:  949/1000  Loss:  [0.25097479]\n",
      "Epoch:  950/1000  Loss:  [0.25097071]\n",
      "Epoch:  951/1000  Loss:  [0.25096646]\n",
      "Epoch:  952/1000  Loss:  [0.25096202]\n",
      "Epoch:  953/1000  Loss:  [0.25095738]\n",
      "Epoch:  954/1000  Loss:  [0.25095255]\n",
      "Epoch:  955/1000  Loss:  [0.2509475]\n",
      "Epoch:  956/1000  Loss:  [0.25094225]\n",
      "Epoch:  957/1000  Loss:  [0.25093677]\n",
      "Epoch:  958/1000  Loss:  [0.25093106]\n",
      "Epoch:  959/1000  Loss:  [0.25092511]\n",
      "Epoch:  960/1000  Loss:  [0.25091892]\n",
      "Epoch:  961/1000  Loss:  [0.25091248]\n",
      "Epoch:  962/1000  Loss:  [0.25090577]\n",
      "Epoch:  963/1000  Loss:  [0.25089879]\n",
      "Epoch:  964/1000  Loss:  [0.25089152]\n",
      "Epoch:  965/1000  Loss:  [0.25088397]\n",
      "Epoch:  966/1000  Loss:  [0.25087611]\n",
      "Epoch:  967/1000  Loss:  [0.25086794]\n",
      "Epoch:  968/1000  Loss:  [0.25085944]\n",
      "Epoch:  969/1000  Loss:  [0.25085061]\n",
      "Epoch:  970/1000  Loss:  [0.25084144]\n",
      "Epoch:  971/1000  Loss:  [0.2508319]\n",
      "Epoch:  972/1000  Loss:  [0.25082199]\n",
      "Epoch:  973/1000  Loss:  [0.25081169]\n",
      "Epoch:  974/1000  Loss:  [0.25080099]\n",
      "Epoch:  975/1000  Loss:  [0.25078988]\n",
      "Epoch:  976/1000  Loss:  [0.25077834]\n",
      "Epoch:  977/1000  Loss:  [0.25076635]\n",
      "Epoch:  978/1000  Loss:  [0.2507539]\n",
      "Epoch:  979/1000  Loss:  [0.25074098]\n",
      "Epoch:  980/1000  Loss:  [0.25072755]\n",
      "Epoch:  981/1000  Loss:  [0.25071362]\n",
      "Epoch:  982/1000  Loss:  [0.25069915]\n",
      "Epoch:  983/1000  Loss:  [0.25068413]\n",
      "Epoch:  984/1000  Loss:  [0.25066854]\n",
      "Epoch:  985/1000  Loss:  [0.25065236]\n",
      "Epoch:  986/1000  Loss:  [0.25063556]\n",
      "Epoch:  987/1000  Loss:  [0.25061813]\n",
      "Epoch:  988/1000  Loss:  [0.25060003]\n",
      "Epoch:  989/1000  Loss:  [0.25058125]\n",
      "Epoch:  990/1000  Loss:  [0.25056176]\n",
      "Epoch:  991/1000  Loss:  [0.25054153]\n",
      "Epoch:  992/1000  Loss:  [0.25052053]\n",
      "Epoch:  993/1000  Loss:  [0.25049875]\n",
      "Epoch:  994/1000  Loss:  [0.25047614]\n",
      "Epoch:  995/1000  Loss:  [0.25045268]\n",
      "Epoch:  996/1000  Loss:  [0.25042834]\n",
      "Epoch:  997/1000  Loss:  [0.25040308]\n",
      "Epoch:  998/1000  Loss:  [0.25037686]\n",
      "Epoch:  999/1000  Loss:  [0.25034966]\n",
      "Epoch:  1000/1000  Loss:  [0.25032144]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 1000 # number of itrations\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.42433357]\n",
      " [0.55641165]\n",
      " [0.49047291]\n",
      " [0.50132611]]\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### experiment with 6 hidden nodes and N = 1000\n",
    "This gives good performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "x_train, y_train = generate_trainset(N)\n",
    "hidden_dim = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/300  Loss:  [0.25050003]\n",
      "Epoch:  2/300  Loss:  [0.25035521]\n",
      "Epoch:  3/300  Loss:  [0.25029759]\n",
      "Epoch:  4/300  Loss:  [0.2502416]\n",
      "Epoch:  5/300  Loss:  [0.25018718]\n",
      "Epoch:  6/300  Loss:  [0.2501341]\n",
      "Epoch:  7/300  Loss:  [0.25008217]\n",
      "Epoch:  8/300  Loss:  [0.25003121]\n",
      "Epoch:  9/300  Loss:  [0.24998106]\n",
      "Epoch:  10/300  Loss:  [0.24993154]\n",
      "Epoch:  11/300  Loss:  [0.2498825]\n",
      "Epoch:  12/300  Loss:  [0.24983378]\n",
      "Epoch:  13/300  Loss:  [0.24978524]\n",
      "Epoch:  14/300  Loss:  [0.24973671]\n",
      "Epoch:  15/300  Loss:  [0.24968807]\n",
      "Epoch:  16/300  Loss:  [0.24963916]\n",
      "Epoch:  17/300  Loss:  [0.24958986]\n",
      "Epoch:  18/300  Loss:  [0.24954002]\n",
      "Epoch:  19/300  Loss:  [0.2494895]\n",
      "Epoch:  20/300  Loss:  [0.24943818]\n",
      "Epoch:  21/300  Loss:  [0.2493859]\n",
      "Epoch:  22/300  Loss:  [0.24933254]\n",
      "Epoch:  23/300  Loss:  [0.24927796]\n",
      "Epoch:  24/300  Loss:  [0.24922202]\n",
      "Epoch:  25/300  Loss:  [0.24916458]\n",
      "Epoch:  26/300  Loss:  [0.24910548]\n",
      "Epoch:  27/300  Loss:  [0.24904459]\n",
      "Epoch:  28/300  Loss:  [0.24898175]\n",
      "Epoch:  29/300  Loss:  [0.2489168]\n",
      "Epoch:  30/300  Loss:  [0.24884958]\n",
      "Epoch:  31/300  Loss:  [0.24877993]\n",
      "Epoch:  32/300  Loss:  [0.24870767]\n",
      "Epoch:  33/300  Loss:  [0.24863262]\n",
      "Epoch:  34/300  Loss:  [0.24855459]\n",
      "Epoch:  35/300  Loss:  [0.24847339]\n",
      "Epoch:  36/300  Loss:  [0.24838881]\n",
      "Epoch:  37/300  Loss:  [0.24830064]\n",
      "Epoch:  38/300  Loss:  [0.24820864]\n",
      "Epoch:  39/300  Loss:  [0.24811259]\n",
      "Epoch:  40/300  Loss:  [0.24801222]\n",
      "Epoch:  41/300  Loss:  [0.24790729]\n",
      "Epoch:  42/300  Loss:  [0.24779751]\n",
      "Epoch:  43/300  Loss:  [0.24768259]\n",
      "Epoch:  44/300  Loss:  [0.24756222]\n",
      "Epoch:  45/300  Loss:  [0.24743609]\n",
      "Epoch:  46/300  Loss:  [0.24730383]\n",
      "Epoch:  47/300  Loss:  [0.24716511]\n",
      "Epoch:  48/300  Loss:  [0.24701953]\n",
      "Epoch:  49/300  Loss:  [0.24686669]\n",
      "Epoch:  50/300  Loss:  [0.24670617]\n",
      "Epoch:  51/300  Loss:  [0.24653753]\n",
      "Epoch:  52/300  Loss:  [0.24636029]\n",
      "Epoch:  53/300  Loss:  [0.24617396]\n",
      "Epoch:  54/300  Loss:  [0.24597802]\n",
      "Epoch:  55/300  Loss:  [0.24577193]\n",
      "Epoch:  56/300  Loss:  [0.24555512]\n",
      "Epoch:  57/300  Loss:  [0.24532698]\n",
      "Epoch:  58/300  Loss:  [0.24508691]\n",
      "Epoch:  59/300  Loss:  [0.24483424]\n",
      "Epoch:  60/300  Loss:  [0.24456831]\n",
      "Epoch:  61/300  Loss:  [0.24428841]\n",
      "Epoch:  62/300  Loss:  [0.24399383]\n",
      "Epoch:  63/300  Loss:  [0.24368381]\n",
      "Epoch:  64/300  Loss:  [0.24335759]\n",
      "Epoch:  65/300  Loss:  [0.24301439]\n",
      "Epoch:  66/300  Loss:  [0.24265341]\n",
      "Epoch:  67/300  Loss:  [0.24227384]\n",
      "Epoch:  68/300  Loss:  [0.24187485]\n",
      "Epoch:  69/300  Loss:  [0.24145563]\n",
      "Epoch:  70/300  Loss:  [0.24101534]\n",
      "Epoch:  71/300  Loss:  [0.24055316]\n",
      "Epoch:  72/300  Loss:  [0.24006828]\n",
      "Epoch:  73/300  Loss:  [0.23955989]\n",
      "Epoch:  74/300  Loss:  [0.2390272]\n",
      "Epoch:  75/300  Loss:  [0.23846945]\n",
      "Epoch:  76/300  Loss:  [0.23788589]\n",
      "Epoch:  77/300  Loss:  [0.2372758]\n",
      "Epoch:  78/300  Loss:  [0.23663847]\n",
      "Epoch:  79/300  Loss:  [0.23597324]\n",
      "Epoch:  80/300  Loss:  [0.23527946]\n",
      "Epoch:  81/300  Loss:  [0.23455648]\n",
      "Epoch:  82/300  Loss:  [0.23380368]\n",
      "Epoch:  83/300  Loss:  [0.23302046]\n",
      "Epoch:  84/300  Loss:  [0.23220617]\n",
      "Epoch:  85/300  Loss:  [0.23136018]\n",
      "Epoch:  86/300  Loss:  [0.23048179]\n",
      "Epoch:  87/300  Loss:  [0.22957029]\n",
      "Epoch:  88/300  Loss:  [0.22862485]\n",
      "Epoch:  89/300  Loss:  [0.22764458]\n",
      "Epoch:  90/300  Loss:  [0.22662846]\n",
      "Epoch:  91/300  Loss:  [0.22557534]\n",
      "Epoch:  92/300  Loss:  [0.22448389]\n",
      "Epoch:  93/300  Loss:  [0.2233526]\n",
      "Epoch:  94/300  Loss:  [0.22217975]\n",
      "Epoch:  95/300  Loss:  [0.22096337]\n",
      "Epoch:  96/300  Loss:  [0.21970123]\n",
      "Epoch:  97/300  Loss:  [0.21839083]\n",
      "Epoch:  98/300  Loss:  [0.21702935]\n",
      "Epoch:  99/300  Loss:  [0.21561366]\n",
      "Epoch:  100/300  Loss:  [0.21414032]\n",
      "Epoch:  101/300  Loss:  [0.21260555]\n",
      "Epoch:  102/300  Loss:  [0.21100525]\n",
      "Epoch:  103/300  Loss:  [0.20933501]\n",
      "Epoch:  104/300  Loss:  [0.20759013]\n",
      "Epoch:  105/300  Loss:  [0.20576566]\n",
      "Epoch:  106/300  Loss:  [0.20385644]\n",
      "Epoch:  107/300  Loss:  [0.2018572]\n",
      "Epoch:  108/300  Loss:  [0.19976258]\n",
      "Epoch:  109/300  Loss:  [0.1975673]\n",
      "Epoch:  110/300  Loss:  [0.19526625]\n",
      "Epoch:  111/300  Loss:  [0.19285465]\n",
      "Epoch:  112/300  Loss:  [0.1903282]\n",
      "Epoch:  113/300  Loss:  [0.18768327]\n",
      "Epoch:  114/300  Loss:  [0.18491711]\n",
      "Epoch:  115/300  Loss:  [0.18202801]\n",
      "Epoch:  116/300  Loss:  [0.17901551]\n",
      "Epoch:  117/300  Loss:  [0.17588057]\n",
      "Epoch:  118/300  Loss:  [0.1726257]\n",
      "Epoch:  119/300  Loss:  [0.16925505]\n",
      "Epoch:  120/300  Loss:  [0.16577448]\n",
      "Epoch:  121/300  Loss:  [0.16219153]\n",
      "Epoch:  122/300  Loss:  [0.15851534]\n",
      "Epoch:  123/300  Loss:  [0.15475658]\n",
      "Epoch:  124/300  Loss:  [0.15092723]\n",
      "Epoch:  125/300  Loss:  [0.14704038]\n",
      "Epoch:  126/300  Loss:  [0.14311002]\n",
      "Epoch:  127/300  Loss:  [0.13915072]\n",
      "Epoch:  128/300  Loss:  [0.13517735]\n",
      "Epoch:  129/300  Loss:  [0.1312048]\n",
      "Epoch:  130/300  Loss:  [0.1272477]\n",
      "Epoch:  131/300  Loss:  [0.12332009]\n",
      "Epoch:  132/300  Loss:  [0.11943522]\n",
      "Epoch:  133/300  Loss:  [0.11560532]\n",
      "Epoch:  134/300  Loss:  [0.11184144]\n",
      "Epoch:  135/300  Loss:  [0.10815334]\n",
      "Epoch:  136/300  Loss:  [0.10454938]\n",
      "Epoch:  137/300  Loss:  [0.10103657]\n",
      "Epoch:  138/300  Loss:  [0.09762053]\n",
      "Epoch:  139/300  Loss:  [0.09430561]\n",
      "Epoch:  140/300  Loss:  [0.09109492]\n",
      "Epoch:  141/300  Loss:  [0.08799044]\n",
      "Epoch:  142/300  Loss:  [0.08499317]\n",
      "Epoch:  143/300  Loss:  [0.08210324]\n",
      "Epoch:  144/300  Loss:  [0.07931998]\n",
      "Epoch:  145/300  Loss:  [0.07664211]\n",
      "Epoch:  146/300  Loss:  [0.0740678]\n",
      "Epoch:  147/300  Loss:  [0.07159479]\n",
      "Epoch:  148/300  Loss:  [0.06922045]\n",
      "Epoch:  149/300  Loss:  [0.06694193]\n",
      "Epoch:  150/300  Loss:  [0.06475613]\n",
      "Epoch:  151/300  Loss:  [0.06265987]\n",
      "Epoch:  152/300  Loss:  [0.06064985]\n",
      "Epoch:  153/300  Loss:  [0.05872274]\n",
      "Epoch:  154/300  Loss:  [0.05687521]\n",
      "Epoch:  155/300  Loss:  [0.05510395]\n",
      "Epoch:  156/300  Loss:  [0.05340572]\n",
      "Epoch:  157/300  Loss:  [0.05177731]\n",
      "Epoch:  158/300  Loss:  [0.05021564]\n",
      "Epoch:  159/300  Loss:  [0.04871768]\n",
      "Epoch:  160/300  Loss:  [0.04728053]\n",
      "Epoch:  161/300  Loss:  [0.0459014]\n",
      "Epoch:  162/300  Loss:  [0.04457758]\n",
      "Epoch:  163/300  Loss:  [0.0433065]\n",
      "Epoch:  164/300  Loss:  [0.0420857]\n",
      "Epoch:  165/300  Loss:  [0.04091282]\n",
      "Epoch:  166/300  Loss:  [0.03978561]\n",
      "Epoch:  167/300  Loss:  [0.03870194]\n",
      "Epoch:  168/300  Loss:  [0.03765977]\n",
      "Epoch:  169/300  Loss:  [0.03665716]\n",
      "Epoch:  170/300  Loss:  [0.03569228]\n",
      "Epoch:  171/300  Loss:  [0.03476338]\n",
      "Epoch:  172/300  Loss:  [0.0338688]\n",
      "Epoch:  173/300  Loss:  [0.03300698]\n",
      "Epoch:  174/300  Loss:  [0.03217641]\n",
      "Epoch:  175/300  Loss:  [0.03137568]\n",
      "Epoch:  176/300  Loss:  [0.03060346]\n",
      "Epoch:  177/300  Loss:  [0.02985847]\n",
      "Epoch:  178/300  Loss:  [0.02913951]\n",
      "Epoch:  179/300  Loss:  [0.02844542]\n",
      "Epoch:  180/300  Loss:  [0.02777513]\n",
      "Epoch:  181/300  Loss:  [0.02712761]\n",
      "Epoch:  182/300  Loss:  [0.02650187]\n",
      "Epoch:  183/300  Loss:  [0.025897]\n",
      "Epoch:  184/300  Loss:  [0.02531211]\n",
      "Epoch:  185/300  Loss:  [0.02474637]\n",
      "Epoch:  186/300  Loss:  [0.02419898]\n",
      "Epoch:  187/300  Loss:  [0.0236692]\n",
      "Epoch:  188/300  Loss:  [0.0231563]\n",
      "Epoch:  189/300  Loss:  [0.0226596]\n",
      "Epoch:  190/300  Loss:  [0.02217847]\n",
      "Epoch:  191/300  Loss:  [0.02171227]\n",
      "Epoch:  192/300  Loss:  [0.02126043]\n",
      "Epoch:  193/300  Loss:  [0.02082238]\n",
      "Epoch:  194/300  Loss:  [0.0203976]\n",
      "Epoch:  195/300  Loss:  [0.01998558]\n",
      "Epoch:  196/300  Loss:  [0.01958584]\n",
      "Epoch:  197/300  Loss:  [0.01919791]\n",
      "Epoch:  198/300  Loss:  [0.01882135]\n",
      "Epoch:  199/300  Loss:  [0.01845576]\n",
      "Epoch:  200/300  Loss:  [0.01810072]\n",
      "Epoch:  201/300  Loss:  [0.01775585]\n",
      "Epoch:  202/300  Loss:  [0.01742079]\n",
      "Epoch:  203/300  Loss:  [0.01709519]\n",
      "Epoch:  204/300  Loss:  [0.01677872]\n",
      "Epoch:  205/300  Loss:  [0.01647104]\n",
      "Epoch:  206/300  Loss:  [0.01617187]\n",
      "Epoch:  207/300  Loss:  [0.0158809]\n",
      "Epoch:  208/300  Loss:  [0.01559785]\n",
      "Epoch:  209/300  Loss:  [0.01532246]\n",
      "Epoch:  210/300  Loss:  [0.01505446]\n",
      "Epoch:  211/300  Loss:  [0.01479361]\n",
      "Epoch:  212/300  Loss:  [0.01453966]\n",
      "Epoch:  213/300  Loss:  [0.0142924]\n",
      "Epoch:  214/300  Loss:  [0.0140516]\n",
      "Epoch:  215/300  Loss:  [0.01381705]\n",
      "Epoch:  216/300  Loss:  [0.01358855]\n",
      "Epoch:  217/300  Loss:  [0.0133659]\n",
      "Epoch:  218/300  Loss:  [0.01314892]\n",
      "Epoch:  219/300  Loss:  [0.01293742]\n",
      "Epoch:  220/300  Loss:  [0.01273124]\n",
      "Epoch:  221/300  Loss:  [0.0125302]\n",
      "Epoch:  222/300  Loss:  [0.01233414]\n",
      "Epoch:  223/300  Loss:  [0.01214292]\n",
      "Epoch:  224/300  Loss:  [0.01195638]\n",
      "Epoch:  225/300  Loss:  [0.01177437]\n",
      "Epoch:  226/300  Loss:  [0.01159676]\n",
      "Epoch:  227/300  Loss:  [0.01142342]\n",
      "Epoch:  228/300  Loss:  [0.01125421]\n",
      "Epoch:  229/300  Loss:  [0.01108902]\n",
      "Epoch:  230/300  Loss:  [0.01092772]\n",
      "Epoch:  231/300  Loss:  [0.01077019]\n",
      "Epoch:  232/300  Loss:  [0.01061633]\n",
      "Epoch:  233/300  Loss:  [0.01046603]\n",
      "Epoch:  234/300  Loss:  [0.01031917]\n",
      "Epoch:  235/300  Loss:  [0.01017567]\n",
      "Epoch:  236/300  Loss:  [0.01003543]\n",
      "Epoch:  237/300  Loss:  [0.00989834]\n",
      "Epoch:  238/300  Loss:  [0.00976433]\n",
      "Epoch:  239/300  Loss:  [0.0096333]\n",
      "Epoch:  240/300  Loss:  [0.00950516]\n",
      "Epoch:  241/300  Loss:  [0.00937984]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  242/300  Loss:  [0.00925726]\n",
      "Epoch:  243/300  Loss:  [0.00913734]\n",
      "Epoch:  244/300  Loss:  [0.00902]\n",
      "Epoch:  245/300  Loss:  [0.00890518]\n",
      "Epoch:  246/300  Loss:  [0.0087928]\n",
      "Epoch:  247/300  Loss:  [0.0086828]\n",
      "Epoch:  248/300  Loss:  [0.00857511]\n",
      "Epoch:  249/300  Loss:  [0.00846967]\n",
      "Epoch:  250/300  Loss:  [0.00836642]\n",
      "Epoch:  251/300  Loss:  [0.0082653]\n",
      "Epoch:  252/300  Loss:  [0.00816625]\n",
      "Epoch:  253/300  Loss:  [0.00806922]\n",
      "Epoch:  254/300  Loss:  [0.00797415]\n",
      "Epoch:  255/300  Loss:  [0.00788099]\n",
      "Epoch:  256/300  Loss:  [0.00778969]\n",
      "Epoch:  257/300  Loss:  [0.00770021]\n",
      "Epoch:  258/300  Loss:  [0.00761249]\n",
      "Epoch:  259/300  Loss:  [0.00752649]\n",
      "Epoch:  260/300  Loss:  [0.00744216]\n",
      "Epoch:  261/300  Loss:  [0.00735947]\n",
      "Epoch:  262/300  Loss:  [0.00727837]\n",
      "Epoch:  263/300  Loss:  [0.00719881]\n",
      "Epoch:  264/300  Loss:  [0.00712077]\n",
      "Epoch:  265/300  Loss:  [0.0070442]\n",
      "Epoch:  266/300  Loss:  [0.00696907]\n",
      "Epoch:  267/300  Loss:  [0.00689533]\n",
      "Epoch:  268/300  Loss:  [0.00682296]\n",
      "Epoch:  269/300  Loss:  [0.00675192]\n",
      "Epoch:  270/300  Loss:  [0.00668218]\n",
      "Epoch:  271/300  Loss:  [0.00661371]\n",
      "Epoch:  272/300  Loss:  [0.00654647]\n",
      "Epoch:  273/300  Loss:  [0.00648044]\n",
      "Epoch:  274/300  Loss:  [0.00641559]\n",
      "Epoch:  275/300  Loss:  [0.00635188]\n",
      "Epoch:  276/300  Loss:  [0.0062893]\n",
      "Epoch:  277/300  Loss:  [0.00622781]\n",
      "Epoch:  278/300  Loss:  [0.00616739]\n",
      "Epoch:  279/300  Loss:  [0.00610802]\n",
      "Epoch:  280/300  Loss:  [0.00604966]\n",
      "Epoch:  281/300  Loss:  [0.0059923]\n",
      "Epoch:  282/300  Loss:  [0.00593591]\n",
      "Epoch:  283/300  Loss:  [0.00588048]\n",
      "Epoch:  284/300  Loss:  [0.00582597]\n",
      "Epoch:  285/300  Loss:  [0.00577237]\n",
      "Epoch:  286/300  Loss:  [0.00571965]\n",
      "Epoch:  287/300  Loss:  [0.00566781]\n",
      "Epoch:  288/300  Loss:  [0.0056168]\n",
      "Epoch:  289/300  Loss:  [0.00556663]\n",
      "Epoch:  290/300  Loss:  [0.00551727]\n",
      "Epoch:  291/300  Loss:  [0.0054687]\n",
      "Epoch:  292/300  Loss:  [0.00542091]\n",
      "Epoch:  293/300  Loss:  [0.00537387]\n",
      "Epoch:  294/300  Loss:  [0.00532757]\n",
      "Epoch:  295/300  Loss:  [0.005282]\n",
      "Epoch:  296/300  Loss:  [0.00523714]\n",
      "Epoch:  297/300  Loss:  [0.00519297]\n",
      "Epoch:  298/300  Loss:  [0.00514949]\n",
      "Epoch:  299/300  Loss:  [0.00510666]\n",
      "Epoch:  300/300  Loss:  [0.00506449]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 300 # number of itrations\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07929603]\n",
      " [0.06154217]\n",
      " [0.93455219]\n",
      " [0.92543788]]\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass NN(object):\\n    def __init__(self, hidden_dim=2, learn_rate=0.01):\\n        self.learn_rate = learn_rate\\n        self.input_dim = 2\\n        self.hidden_dim = hidden_dim\\n        self.output_dim = 1\\n        # initialize all weights and biases to zeros\\n#         self.A = np.zeros((self.hidden_dim, self.input_dim))  # input to hidden layer weights\\n#         self.B = np.zeros(self.hidden_dim)                     # hidden layer to output weights \\n#         self.a0 = np.zeros(hidden_dim) # input to hidden layer bias\\n#         self.b0 = 0 # hidden layer to output bias\\n        self.A = np.random.normal(0, 1, (self.hidden_dim, self.input_dim))\\n        self.B = np.random.normal(0, 1, self.hidden_dim)\\n        self.a0 = np.random.normal(0, 1, self.hidden_dim)\\n        self.b0 = np.random.normal(0, 1, 1)\\n        \\n    def sigmoid(self, t):\\n        return 1/(1  + np.exp(-t))\\n    \\n    def dsigmoid(self, t):\\n        return self.sigmoid(t)*(1 - self.sigmoid(t))\\n    \\n    def hidden_layer(self, x):\\n        self.z = self.sigmoid(np.dot(self.A, x) + self.a0) \\n        return self.z\\n    \\n    def forward_pass(self, x):\\n        self.y_hat = self.sigmoid(np.dot(self.B, self.hidden_layer(x)) + self.b0)\\n        return self.y_hat\\n    \\n    def back_propogate(self, X, Y, Y_hat):\\n        SSE_a, SSE_b   = np.zeros_like(self.A), np.zeros_like(self.B)\\n        z_bias, y_bias = np.zeros_like(self.a0), np.zeros_like(self.b0)\\n        for i, x in enumerate(X):\\n            z = self.hidden_layer(x)\\n            y_error = Y[i] - Y_hat[i]\\n            y_delta = -2* y_error * self.dsigmoid(np.dot(self.B, z) + self.b0)\\n            s = self.dsigmoid(np.dot(self.A,x) + self.a0) * self.B * y_delta\\n            SSE_b += y_delta*z\\n            SSE_a += np.tensordot(x,s, axes=0)\\n            y_bias += y_delta\\n            z_bias += s\\n        # update the weights and biases\\n        self.A -= self.learn_rate * SSE_a\\n        self.B -= self.learn_rate * SSE_b\\n        self.a0 -= self.learn_rate * s\\n        self.b0 -= self.learn_rate * y_delta\\n        \\n    def train(self, x_train, y_train, epochs, shuffle=True): \\n        if shuffle:\\n            indices = np.arange(N)\\n            np.random.shuffle(indices)\\n            x_train, y_train = x_train[indices], y_train[indices]\\n\\n        epoch = 1\\n        while(epoch <= epochs):\\n            y_hat = np.array([self.forward_pass(x) for x in x_train])\\n            self.back_propogate(x_train, y_train, y_hat)\\n            print('Epoch: ', epoch, 'Loss: ', self.loss(y_train, y_hat))\\n            epoch += 1\\n\\n    def loss(self, y_train, y_hat):\\n        return np.mean((y_train - y_hat)**2)\\n    \\n    def predict(self, test_x):\\n#         if test_x.shape == (2,): test_x = np.reshape(test_x, (1,2))\\n        y_hats = np.array([self.forward_pass(x) for x in test_x])\\n        y_outs = int(y_hats > 0.5)\\n        return y_outs\\n    \\n#     def load_model(self):\\n#         # something here\\n#     def get_weights(self):\\n#         # some code here\\n#     def load_weights(self):\\n#         # some code here\\n#     def save_model(self):\\n#         # some code here\\n\""
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignore this, this does not work \n",
    "# couldn't find the time to fix it, just submitting my effort\n",
    "# apologies if the above code is not convienent\n",
    "\"\"\"\n",
    "class NN(object):\n",
    "    def __init__(self, hidden_dim=2, learn_rate=0.01):\n",
    "        self.learn_rate = learn_rate\n",
    "        self.input_dim = 2\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = 1\n",
    "        # initialize all weights and biases to zeros\n",
    "#         self.A = np.zeros((self.hidden_dim, self.input_dim))  # input to hidden layer weights\n",
    "#         self.B = np.zeros(self.hidden_dim)                     # hidden layer to output weights \n",
    "#         self.a0 = np.zeros(hidden_dim) # input to hidden layer bias\n",
    "#         self.b0 = 0 # hidden layer to output bias\n",
    "        self.A = np.random.normal(0, 1, (self.hidden_dim, self.input_dim))\n",
    "        self.B = np.random.normal(0, 1, self.hidden_dim)\n",
    "        self.a0 = np.random.normal(0, 1, self.hidden_dim)\n",
    "        self.b0 = np.random.normal(0, 1, 1)\n",
    "        \n",
    "    def sigmoid(self, t):\n",
    "        return 1/(1  + np.exp(-t))\n",
    "    \n",
    "    def dsigmoid(self, t):\n",
    "        return self.sigmoid(t)*(1 - self.sigmoid(t))\n",
    "    \n",
    "    def hidden_layer(self, x):\n",
    "        self.z = self.sigmoid(np.dot(self.A, x) + self.a0) \n",
    "        return self.z\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        self.y_hat = self.sigmoid(np.dot(self.B, self.hidden_layer(x)) + self.b0)\n",
    "        return self.y_hat\n",
    "    \n",
    "    def back_propogate(self, X, Y, Y_hat):\n",
    "        SSE_a, SSE_b   = np.zeros_like(self.A), np.zeros_like(self.B)\n",
    "        z_bias, y_bias = np.zeros_like(self.a0), np.zeros_like(self.b0)\n",
    "        for i, x in enumerate(X):\n",
    "            z = self.hidden_layer(x)\n",
    "            y_error = Y[i] - Y_hat[i]\n",
    "            y_delta = -2* y_error * self.dsigmoid(np.dot(self.B, z) + self.b0)\n",
    "            s = self.dsigmoid(np.dot(self.A,x) + self.a0) * self.B * y_delta\n",
    "            SSE_b += y_delta*z\n",
    "            SSE_a += np.tensordot(x,s, axes=0)\n",
    "            y_bias += y_delta\n",
    "            z_bias += s\n",
    "        # update the weights and biases\n",
    "        self.A -= self.learn_rate * SSE_a\n",
    "        self.B -= self.learn_rate * SSE_b\n",
    "        self.a0 -= self.learn_rate * s\n",
    "        self.b0 -= self.learn_rate * y_delta\n",
    "        \n",
    "    def train(self, x_train, y_train, epochs, shuffle=True): \n",
    "        if shuffle:\n",
    "            indices = np.arange(N)\n",
    "            np.random.shuffle(indices)\n",
    "            x_train, y_train = x_train[indices], y_train[indices]\n",
    "\n",
    "        epoch = 1\n",
    "        while(epoch <= epochs):\n",
    "            y_hat = np.array([self.forward_pass(x) for x in x_train])\n",
    "            self.back_propogate(x_train, y_train, y_hat)\n",
    "            print('Epoch: ', epoch, 'Loss: ', self.loss(y_train, y_hat))\n",
    "            epoch += 1\n",
    "\n",
    "    def loss(self, y_train, y_hat):\n",
    "        return np.mean((y_train - y_hat)**2)\n",
    "    \n",
    "    def predict(self, test_x):\n",
    "#         if test_x.shape == (2,): test_x = np.reshape(test_x, (1,2))\n",
    "        y_hats = np.array([self.forward_pass(x) for x in test_x])\n",
    "        y_outs = int(y_hats > 0.5)\n",
    "        return y_outs\n",
    "    \n",
    "#     def load_model(self):\n",
    "#         # something here\n",
    "#     def get_weights(self):\n",
    "#         # some code here\n",
    "#     def load_weights(self):\n",
    "#         # some code here\n",
    "#     def save_model(self):\n",
    "#         # some code here\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
