{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the back-propagation algorithm to learn the weights of a perceptron with 2 input nodes, 2 hidden nodes and 1 output node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **Python3** in used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters - input all parameter values here\n",
    "input_dim = 2\n",
    "hidden_dim = 2 # dimensions of hidden layers\n",
    "std = 0.01  # train data noise standard deviation\n",
    "w_std = 0.5\n",
    "learn_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "x_inputs = np.array([np.zeros(2), np.ones(2), np.array([1,0]), np.array([0,1])])\n",
    "def generate_trainset(N):\n",
    "    X = np.repeat(x_inputs, N//4, axis=0)\n",
    "    y_xor = np.logical_xor(X.T[0], X.T[1]).astype(np.float)\n",
    "    # add noise to data\n",
    "    X += np.random.normal(0, std, X.shape)\n",
    "    y_xor += np.random.normal(0, std, N)\n",
    "    # shuffle the training data\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    x_train, y_train = X[indices], y_xor[indices]\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid( t):\n",
    "    return 1/(1  + np.exp(-t))\n",
    "\n",
    "def dsigmoid( t):\n",
    "    return sigmoid(t)*(1 - sigmoid(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Experiment with N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "x_train, y_train = generate_trainset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/300  Loss:  [0.24970636]\n",
      "Epoch:  2/300  Loss:  [0.2496569]\n",
      "Epoch:  3/300  Loss:  [0.24964112]\n",
      "Epoch:  4/300  Loss:  [0.24963403]\n",
      "Epoch:  5/300  Loss:  [0.24962876]\n",
      "Epoch:  6/300  Loss:  [0.24962351]\n",
      "Epoch:  7/300  Loss:  [0.2496179]\n",
      "Epoch:  8/300  Loss:  [0.24961187]\n",
      "Epoch:  9/300  Loss:  [0.24960545]\n",
      "Epoch:  10/300  Loss:  [0.24959863]\n",
      "Epoch:  11/300  Loss:  [0.24959138]\n",
      "Epoch:  12/300  Loss:  [0.2495837]\n",
      "Epoch:  13/300  Loss:  [0.24957552]\n",
      "Epoch:  14/300  Loss:  [0.24956681]\n",
      "Epoch:  15/300  Loss:  [0.24955751]\n",
      "Epoch:  16/300  Loss:  [0.24954754]\n",
      "Epoch:  17/300  Loss:  [0.24953684]\n",
      "Epoch:  18/300  Loss:  [0.24952533]\n",
      "Epoch:  19/300  Loss:  [0.24951291]\n",
      "Epoch:  20/300  Loss:  [0.2494995]\n",
      "Epoch:  21/300  Loss:  [0.24948497]\n",
      "Epoch:  22/300  Loss:  [0.24946922]\n",
      "Epoch:  23/300  Loss:  [0.24945209]\n",
      "Epoch:  24/300  Loss:  [0.24943344]\n",
      "Epoch:  25/300  Loss:  [0.2494131]\n",
      "Epoch:  26/300  Loss:  [0.24939087]\n",
      "Epoch:  27/300  Loss:  [0.24936654]\n",
      "Epoch:  28/300  Loss:  [0.24933985]\n",
      "Epoch:  29/300  Loss:  [0.24931054]\n",
      "Epoch:  30/300  Loss:  [0.24927828]\n",
      "Epoch:  31/300  Loss:  [0.24924272]\n",
      "Epoch:  32/300  Loss:  [0.24920344]\n",
      "Epoch:  33/300  Loss:  [0.24915999]\n",
      "Epoch:  34/300  Loss:  [0.24911182]\n",
      "Epoch:  35/300  Loss:  [0.24905834]\n",
      "Epoch:  36/300  Loss:  [0.24899882]\n",
      "Epoch:  37/300  Loss:  [0.24893248]\n",
      "Epoch:  38/300  Loss:  [0.24885839]\n",
      "Epoch:  39/300  Loss:  [0.24877546]\n",
      "Epoch:  40/300  Loss:  [0.24868249]\n",
      "Epoch:  41/300  Loss:  [0.24857803]\n",
      "Epoch:  42/300  Loss:  [0.24846046]\n",
      "Epoch:  43/300  Loss:  [0.24832786]\n",
      "Epoch:  44/300  Loss:  [0.24817806]\n",
      "Epoch:  45/300  Loss:  [0.24800852]\n",
      "Epoch:  46/300  Loss:  [0.24781633]\n",
      "Epoch:  47/300  Loss:  [0.24759816]\n",
      "Epoch:  48/300  Loss:  [0.24735018]\n",
      "Epoch:  49/300  Loss:  [0.24706802]\n",
      "Epoch:  50/300  Loss:  [0.24674675]\n",
      "Epoch:  51/300  Loss:  [0.24638081]\n",
      "Epoch:  52/300  Loss:  [0.245964]\n",
      "Epoch:  53/300  Loss:  [0.24548949]\n",
      "Epoch:  54/300  Loss:  [0.24494989]\n",
      "Epoch:  55/300  Loss:  [0.24433728]\n",
      "Epoch:  56/300  Loss:  [0.24364341]\n",
      "Epoch:  57/300  Loss:  [0.24285987]\n",
      "Epoch:  58/300  Loss:  [0.24197844]\n",
      "Epoch:  59/300  Loss:  [0.24099136]\n",
      "Epoch:  60/300  Loss:  [0.2398918]\n",
      "Epoch:  61/300  Loss:  [0.23867429]\n",
      "Epoch:  62/300  Loss:  [0.2373351]\n",
      "Epoch:  63/300  Loss:  [0.23587265]\n",
      "Epoch:  64/300  Loss:  [0.23428776]\n",
      "Epoch:  65/300  Loss:  [0.23258381]\n",
      "Epoch:  66/300  Loss:  [0.23076666]\n",
      "Epoch:  67/300  Loss:  [0.22884451]\n",
      "Epoch:  68/300  Loss:  [0.22682752]\n",
      "Epoch:  69/300  Loss:  [0.22472737]\n",
      "Epoch:  70/300  Loss:  [0.22255681]\n",
      "Epoch:  71/300  Loss:  [0.22032907]\n",
      "Epoch:  72/300  Loss:  [0.21805749]\n",
      "Epoch:  73/300  Loss:  [0.21575502]\n",
      "Epoch:  74/300  Loss:  [0.213434]\n",
      "Epoch:  75/300  Loss:  [0.21110584]\n",
      "Epoch:  76/300  Loss:  [0.20878095]\n",
      "Epoch:  77/300  Loss:  [0.2064686]\n",
      "Epoch:  78/300  Loss:  [0.20417688]\n",
      "Epoch:  79/300  Loss:  [0.20191277]\n",
      "Epoch:  80/300  Loss:  [0.19968211]\n",
      "Epoch:  81/300  Loss:  [0.19748972]\n",
      "Epoch:  82/300  Loss:  [0.19533944]\n",
      "Epoch:  83/300  Loss:  [0.19323427]\n",
      "Epoch:  84/300  Loss:  [0.1911764]\n",
      "Epoch:  85/300  Loss:  [0.18916738]\n",
      "Epoch:  86/300  Loss:  [0.18720815]\n",
      "Epoch:  87/300  Loss:  [0.18529919]\n",
      "Epoch:  88/300  Loss:  [0.18344054]\n",
      "Epoch:  89/300  Loss:  [0.18163193]\n",
      "Epoch:  90/300  Loss:  [0.1798728]\n",
      "Epoch:  91/300  Loss:  [0.17816238]\n",
      "Epoch:  92/300  Loss:  [0.17649973]\n",
      "Epoch:  93/300  Loss:  [0.17488378]\n",
      "Epoch:  94/300  Loss:  [0.17331337]\n",
      "Epoch:  95/300  Loss:  [0.17178726]\n",
      "Epoch:  96/300  Loss:  [0.17030417]\n",
      "Epoch:  97/300  Loss:  [0.16886279]\n",
      "Epoch:  98/300  Loss:  [0.16746181]\n",
      "Epoch:  99/300  Loss:  [0.1660999]\n",
      "Epoch:  100/300  Loss:  [0.16477577]\n",
      "Epoch:  101/300  Loss:  [0.16348812]\n",
      "Epoch:  102/300  Loss:  [0.16223568]\n",
      "Epoch:  103/300  Loss:  [0.16101723]\n",
      "Epoch:  104/300  Loss:  [0.15983156]\n",
      "Epoch:  105/300  Loss:  [0.15867751]\n",
      "Epoch:  106/300  Loss:  [0.15755393]\n",
      "Epoch:  107/300  Loss:  [0.15645975]\n",
      "Epoch:  108/300  Loss:  [0.15539389]\n",
      "Epoch:  109/300  Loss:  [0.15435535]\n",
      "Epoch:  110/300  Loss:  [0.15334314]\n",
      "Epoch:  111/300  Loss:  [0.15235633]\n",
      "Epoch:  112/300  Loss:  [0.151394]\n",
      "Epoch:  113/300  Loss:  [0.15045528]\n",
      "Epoch:  114/300  Loss:  [0.14953935]\n",
      "Epoch:  115/300  Loss:  [0.14864539]\n",
      "Epoch:  116/300  Loss:  [0.14777264]\n",
      "Epoch:  117/300  Loss:  [0.14692036]\n",
      "Epoch:  118/300  Loss:  [0.14608784]\n",
      "Epoch:  119/300  Loss:  [0.1452744]\n",
      "Epoch:  120/300  Loss:  [0.14447939]\n",
      "Epoch:  121/300  Loss:  [0.14370219]\n",
      "Epoch:  122/300  Loss:  [0.1429422]\n",
      "Epoch:  123/300  Loss:  [0.14219885]\n",
      "Epoch:  124/300  Loss:  [0.14147158]\n",
      "Epoch:  125/300  Loss:  [0.14075987]\n",
      "Epoch:  126/300  Loss:  [0.14006321]\n",
      "Epoch:  127/300  Loss:  [0.13938111]\n",
      "Epoch:  128/300  Loss:  [0.13871312]\n",
      "Epoch:  129/300  Loss:  [0.13805879]\n",
      "Epoch:  130/300  Loss:  [0.13741768]\n",
      "Epoch:  131/300  Loss:  [0.13678938]\n",
      "Epoch:  132/300  Loss:  [0.1361735]\n",
      "Epoch:  133/300  Loss:  [0.13556966]\n",
      "Epoch:  134/300  Loss:  [0.1349775]\n",
      "Epoch:  135/300  Loss:  [0.13439666]\n",
      "Epoch:  136/300  Loss:  [0.1338268]\n",
      "Epoch:  137/300  Loss:  [0.13326761]\n",
      "Epoch:  138/300  Loss:  [0.13271878]\n",
      "Epoch:  139/300  Loss:  [0.13217999]\n",
      "Epoch:  140/300  Loss:  [0.13165097]\n",
      "Epoch:  141/300  Loss:  [0.13113143]\n",
      "Epoch:  142/300  Loss:  [0.13062112]\n",
      "Epoch:  143/300  Loss:  [0.13011977]\n",
      "Epoch:  144/300  Loss:  [0.12962713]\n",
      "Epoch:  145/300  Loss:  [0.12914297]\n",
      "Epoch:  146/300  Loss:  [0.12866705]\n",
      "Epoch:  147/300  Loss:  [0.12819916]\n",
      "Epoch:  148/300  Loss:  [0.12773908]\n",
      "Epoch:  149/300  Loss:  [0.1272866]\n",
      "Epoch:  150/300  Loss:  [0.12684153]\n",
      "Epoch:  151/300  Loss:  [0.12640366]\n",
      "Epoch:  152/300  Loss:  [0.12597283]\n",
      "Epoch:  153/300  Loss:  [0.12554884]\n",
      "Epoch:  154/300  Loss:  [0.12513152]\n",
      "Epoch:  155/300  Loss:  [0.12472072]\n",
      "Epoch:  156/300  Loss:  [0.12431625]\n",
      "Epoch:  157/300  Loss:  [0.12391798]\n",
      "Epoch:  158/300  Loss:  [0.12352574]\n",
      "Epoch:  159/300  Loss:  [0.1231394]\n",
      "Epoch:  160/300  Loss:  [0.1227588]\n",
      "Epoch:  161/300  Loss:  [0.12238382]\n",
      "Epoch:  162/300  Loss:  [0.12201432]\n",
      "Epoch:  163/300  Loss:  [0.12165016]\n",
      "Epoch:  164/300  Loss:  [0.12129124]\n",
      "Epoch:  165/300  Loss:  [0.12093742]\n",
      "Epoch:  166/300  Loss:  [0.1205886]\n",
      "Epoch:  167/300  Loss:  [0.12024465]\n",
      "Epoch:  168/300  Loss:  [0.11990547]\n",
      "Epoch:  169/300  Loss:  [0.11957095]\n",
      "Epoch:  170/300  Loss:  [0.11924098]\n",
      "Epoch:  171/300  Loss:  [0.11891548]\n",
      "Epoch:  172/300  Loss:  [0.11859434]\n",
      "Epoch:  173/300  Loss:  [0.11827746]\n",
      "Epoch:  174/300  Loss:  [0.11796476]\n",
      "Epoch:  175/300  Loss:  [0.11765614]\n",
      "Epoch:  176/300  Loss:  [0.11735153]\n",
      "Epoch:  177/300  Loss:  [0.11705083]\n",
      "Epoch:  178/300  Loss:  [0.11675397]\n",
      "Epoch:  179/300  Loss:  [0.11646086]\n",
      "Epoch:  180/300  Loss:  [0.11617144]\n",
      "Epoch:  181/300  Loss:  [0.11588561]\n",
      "Epoch:  182/300  Loss:  [0.11560332]\n",
      "Epoch:  183/300  Loss:  [0.11532449]\n",
      "Epoch:  184/300  Loss:  [0.11504905]\n",
      "Epoch:  185/300  Loss:  [0.11477694]\n",
      "Epoch:  186/300  Loss:  [0.11450809]\n",
      "Epoch:  187/300  Loss:  [0.11424243]\n",
      "Epoch:  188/300  Loss:  [0.11397991]\n",
      "Epoch:  189/300  Loss:  [0.11372047]\n",
      "Epoch:  190/300  Loss:  [0.11346404]\n",
      "Epoch:  191/300  Loss:  [0.11321057]\n",
      "Epoch:  192/300  Loss:  [0.11296]\n",
      "Epoch:  193/300  Loss:  [0.11271229]\n",
      "Epoch:  194/300  Loss:  [0.11246737]\n",
      "Epoch:  195/300  Loss:  [0.1122252]\n",
      "Epoch:  196/300  Loss:  [0.11198572]\n",
      "Epoch:  197/300  Loss:  [0.11174889]\n",
      "Epoch:  198/300  Loss:  [0.11151466]\n",
      "Epoch:  199/300  Loss:  [0.11128298]\n",
      "Epoch:  200/300  Loss:  [0.11105381]\n",
      "Epoch:  201/300  Loss:  [0.11082711]\n",
      "Epoch:  202/300  Loss:  [0.11060282]\n",
      "Epoch:  203/300  Loss:  [0.11038091]\n",
      "Epoch:  204/300  Loss:  [0.11016134]\n",
      "Epoch:  205/300  Loss:  [0.10994407]\n",
      "Epoch:  206/300  Loss:  [0.10972905]\n",
      "Epoch:  207/300  Loss:  [0.10951625]\n",
      "Epoch:  208/300  Loss:  [0.10930564]\n",
      "Epoch:  209/300  Loss:  [0.10909717]\n",
      "Epoch:  210/300  Loss:  [0.10889081]\n",
      "Epoch:  211/300  Loss:  [0.10868652]\n",
      "Epoch:  212/300  Loss:  [0.10848427]\n",
      "Epoch:  213/300  Loss:  [0.10828404]\n",
      "Epoch:  214/300  Loss:  [0.10808577]\n",
      "Epoch:  215/300  Loss:  [0.10788945]\n",
      "Epoch:  216/300  Loss:  [0.10769505]\n",
      "Epoch:  217/300  Loss:  [0.10750252]\n",
      "Epoch:  218/300  Loss:  [0.10731184]\n",
      "Epoch:  219/300  Loss:  [0.10712299]\n",
      "Epoch:  220/300  Loss:  [0.10693593]\n",
      "Epoch:  221/300  Loss:  [0.10675064]\n",
      "Epoch:  222/300  Loss:  [0.10656709]\n",
      "Epoch:  223/300  Loss:  [0.10638524]\n",
      "Epoch:  224/300  Loss:  [0.10620509]\n",
      "Epoch:  225/300  Loss:  [0.10602659]\n",
      "Epoch:  226/300  Loss:  [0.10584973]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  227/300  Loss:  [0.10567449]\n",
      "Epoch:  228/300  Loss:  [0.10550082]\n",
      "Epoch:  229/300  Loss:  [0.10532872]\n",
      "Epoch:  230/300  Loss:  [0.10515816]\n",
      "Epoch:  231/300  Loss:  [0.10498912]\n",
      "Epoch:  232/300  Loss:  [0.10482158]\n",
      "Epoch:  233/300  Loss:  [0.10465551]\n",
      "Epoch:  234/300  Loss:  [0.10449089]\n",
      "Epoch:  235/300  Loss:  [0.1043277]\n",
      "Epoch:  236/300  Loss:  [0.10416592]\n",
      "Epoch:  237/300  Loss:  [0.10400553]\n",
      "Epoch:  238/300  Loss:  [0.10384652]\n",
      "Epoch:  239/300  Loss:  [0.10368886]\n",
      "Epoch:  240/300  Loss:  [0.10353253]\n",
      "Epoch:  241/300  Loss:  [0.10337752]\n",
      "Epoch:  242/300  Loss:  [0.1032238]\n",
      "Epoch:  243/300  Loss:  [0.10307136]\n",
      "Epoch:  244/300  Loss:  [0.10292018]\n",
      "Epoch:  245/300  Loss:  [0.10277025]\n",
      "Epoch:  246/300  Loss:  [0.10262155]\n",
      "Epoch:  247/300  Loss:  [0.10247406]\n",
      "Epoch:  248/300  Loss:  [0.10232776]\n",
      "Epoch:  249/300  Loss:  [0.10218264]\n",
      "Epoch:  250/300  Loss:  [0.10203868]\n",
      "Epoch:  251/300  Loss:  [0.10189587]\n",
      "Epoch:  252/300  Loss:  [0.1017542]\n",
      "Epoch:  253/300  Loss:  [0.10161364]\n",
      "Epoch:  254/300  Loss:  [0.10147419]\n",
      "Epoch:  255/300  Loss:  [0.10133582]\n",
      "Epoch:  256/300  Loss:  [0.10119854]\n",
      "Epoch:  257/300  Loss:  [0.10106231]\n",
      "Epoch:  258/300  Loss:  [0.10092713]\n",
      "Epoch:  259/300  Loss:  [0.10079299]\n",
      "Epoch:  260/300  Loss:  [0.10065987]\n",
      "Epoch:  261/300  Loss:  [0.10052775]\n",
      "Epoch:  262/300  Loss:  [0.10039664]\n",
      "Epoch:  263/300  Loss:  [0.10026651]\n",
      "Epoch:  264/300  Loss:  [0.10013735]\n",
      "Epoch:  265/300  Loss:  [0.10000915]\n",
      "Epoch:  266/300  Loss:  [0.0998819]\n",
      "Epoch:  267/300  Loss:  [0.09975558]\n",
      "Epoch:  268/300  Loss:  [0.0996302]\n",
      "Epoch:  269/300  Loss:  [0.09950572]\n",
      "Epoch:  270/300  Loss:  [0.09938215]\n",
      "Epoch:  271/300  Loss:  [0.09925947]\n",
      "Epoch:  272/300  Loss:  [0.09913768]\n",
      "Epoch:  273/300  Loss:  [0.09901675]\n",
      "Epoch:  274/300  Loss:  [0.09889669]\n",
      "Epoch:  275/300  Loss:  [0.09877748]\n",
      "Epoch:  276/300  Loss:  [0.0986591]\n",
      "Epoch:  277/300  Loss:  [0.09854156]\n",
      "Epoch:  278/300  Loss:  [0.09842485]\n",
      "Epoch:  279/300  Loss:  [0.09830894]\n",
      "Epoch:  280/300  Loss:  [0.09819384]\n",
      "Epoch:  281/300  Loss:  [0.09807953]\n",
      "Epoch:  282/300  Loss:  [0.09796601]\n",
      "Epoch:  283/300  Loss:  [0.09785326]\n",
      "Epoch:  284/300  Loss:  [0.09774128]\n",
      "Epoch:  285/300  Loss:  [0.09763006]\n",
      "Epoch:  286/300  Loss:  [0.09751959]\n",
      "Epoch:  287/300  Loss:  [0.09740986]\n",
      "Epoch:  288/300  Loss:  [0.09730086]\n",
      "Epoch:  289/300  Loss:  [0.09719259]\n",
      "Epoch:  290/300  Loss:  [0.09708504]\n",
      "Epoch:  291/300  Loss:  [0.0969782]\n",
      "Epoch:  292/300  Loss:  [0.09687206]\n",
      "Epoch:  293/300  Loss:  [0.09676661]\n",
      "Epoch:  294/300  Loss:  [0.09666185]\n",
      "Epoch:  295/300  Loss:  [0.09655777]\n",
      "Epoch:  296/300  Loss:  [0.09645437]\n",
      "Epoch:  297/300  Loss:  [0.09635163]\n",
      "Epoch:  298/300  Loss:  [0.09624954]\n",
      "Epoch:  299/300  Loss:  [0.09614811]\n",
      "Epoch:  300/300  Loss:  [0.09604732]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 300 # number of itrations\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09480666]\n",
      " [0.5129879 ]\n",
      " [0.76515515]\n",
      " [0.76614306]]\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Experiment with N = 100\n",
    "poor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "x_train, y_train = generate_trainset(N)\n",
    "learn_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/250  Loss:  [0.11016344]\n",
      "Epoch:  2/250  Loss:  [0.09819333]\n",
      "Epoch:  3/250  Loss:  [0.08815396]\n",
      "Epoch:  4/250  Loss:  [0.0796583]\n",
      "Epoch:  5/250  Loss:  [0.07240785]\n",
      "Epoch:  6/250  Loss:  [0.06617087]\n",
      "Epoch:  7/250  Loss:  [0.06076607]\n",
      "Epoch:  8/250  Loss:  [0.05605038]\n",
      "Epoch:  9/250  Loss:  [0.05190999]\n",
      "Epoch:  10/250  Loss:  [0.04825349]\n",
      "Epoch:  11/250  Loss:  [0.04500694]\n",
      "Epoch:  12/250  Loss:  [0.04211002]\n",
      "Epoch:  13/250  Loss:  [0.03951312]\n",
      "Epoch:  14/250  Loss:  [0.03717519]\n",
      "Epoch:  15/250  Loss:  [0.03506202]\n",
      "Epoch:  16/250  Loss:  [0.0331449]\n",
      "Epoch:  17/250  Loss:  [0.0313996]\n",
      "Epoch:  18/250  Loss:  [0.02980557]\n",
      "Epoch:  19/250  Loss:  [0.02834527]\n",
      "Epoch:  20/250  Loss:  [0.02700366]\n",
      "Epoch:  21/250  Loss:  [0.02576778]\n",
      "Epoch:  22/250  Loss:  [0.02462641]\n",
      "Epoch:  23/250  Loss:  [0.02356981]\n",
      "Epoch:  24/250  Loss:  [0.02258948]\n",
      "Epoch:  25/250  Loss:  [0.02167797]\n",
      "Epoch:  26/250  Loss:  [0.02082874]\n",
      "Epoch:  27/250  Loss:  [0.020036]\n",
      "Epoch:  28/250  Loss:  [0.01929464]\n",
      "Epoch:  29/250  Loss:  [0.01860015]\n",
      "Epoch:  30/250  Loss:  [0.01794846]\n",
      "Epoch:  31/250  Loss:  [0.017336]\n",
      "Epoch:  32/250  Loss:  [0.01675951]\n",
      "Epoch:  33/250  Loss:  [0.01621613]\n",
      "Epoch:  34/250  Loss:  [0.01570323]\n",
      "Epoch:  35/250  Loss:  [0.01521847]\n",
      "Epoch:  36/250  Loss:  [0.01475973]\n",
      "Epoch:  37/250  Loss:  [0.0143251]\n",
      "Epoch:  38/250  Loss:  [0.01391281]\n",
      "Epoch:  39/250  Loss:  [0.0135213]\n",
      "Epoch:  40/250  Loss:  [0.01314912]\n",
      "Epoch:  41/250  Loss:  [0.01279494]\n",
      "Epoch:  42/250  Loss:  [0.01245756]\n",
      "Epoch:  43/250  Loss:  [0.01213589]\n",
      "Epoch:  44/250  Loss:  [0.0118289]\n",
      "Epoch:  45/250  Loss:  [0.01153567]\n",
      "Epoch:  46/250  Loss:  [0.01125534]\n",
      "Epoch:  47/250  Loss:  [0.01098713]\n",
      "Epoch:  48/250  Loss:  [0.01073029]\n",
      "Epoch:  49/250  Loss:  [0.01048417]\n",
      "Epoch:  50/250  Loss:  [0.01024814]\n",
      "Epoch:  51/250  Loss:  [0.01002162]\n",
      "Epoch:  52/250  Loss:  [0.00980408]\n",
      "Epoch:  53/250  Loss:  [0.00959503]\n",
      "Epoch:  54/250  Loss:  [0.00939398]\n",
      "Epoch:  55/250  Loss:  [0.00920053]\n",
      "Epoch:  56/250  Loss:  [0.00901427]\n",
      "Epoch:  57/250  Loss:  [0.00883482]\n",
      "Epoch:  58/250  Loss:  [0.00866183]\n",
      "Epoch:  59/250  Loss:  [0.00849498]\n",
      "Epoch:  60/250  Loss:  [0.00833397]\n",
      "Epoch:  61/250  Loss:  [0.0081785]\n",
      "Epoch:  62/250  Loss:  [0.0080283]\n",
      "Epoch:  63/250  Loss:  [0.00788313]\n",
      "Epoch:  64/250  Loss:  [0.00774275]\n",
      "Epoch:  65/250  Loss:  [0.00760694]\n",
      "Epoch:  66/250  Loss:  [0.00747549]\n",
      "Epoch:  67/250  Loss:  [0.00734819]\n",
      "Epoch:  68/250  Loss:  [0.00722487]\n",
      "Epoch:  69/250  Loss:  [0.00710535]\n",
      "Epoch:  70/250  Loss:  [0.00698946]\n",
      "Epoch:  71/250  Loss:  [0.00687705]\n",
      "Epoch:  72/250  Loss:  [0.00676797]\n",
      "Epoch:  73/250  Loss:  [0.00666208]\n",
      "Epoch:  74/250  Loss:  [0.00655925]\n",
      "Epoch:  75/250  Loss:  [0.00645936]\n",
      "Epoch:  76/250  Loss:  [0.00636228]\n",
      "Epoch:  77/250  Loss:  [0.0062679]\n",
      "Epoch:  78/250  Loss:  [0.00617612]\n",
      "Epoch:  79/250  Loss:  [0.00608683]\n",
      "Epoch:  80/250  Loss:  [0.00599994]\n",
      "Epoch:  81/250  Loss:  [0.00591536]\n",
      "Epoch:  82/250  Loss:  [0.005833]\n",
      "Epoch:  83/250  Loss:  [0.00575278]\n",
      "Epoch:  84/250  Loss:  [0.00567461]\n",
      "Epoch:  85/250  Loss:  [0.00559843]\n",
      "Epoch:  86/250  Loss:  [0.00552416]\n",
      "Epoch:  87/250  Loss:  [0.00545173]\n",
      "Epoch:  88/250  Loss:  [0.00538109]\n",
      "Epoch:  89/250  Loss:  [0.00531216]\n",
      "Epoch:  90/250  Loss:  [0.00524488]\n",
      "Epoch:  91/250  Loss:  [0.00517921]\n",
      "Epoch:  92/250  Loss:  [0.00511508]\n",
      "Epoch:  93/250  Loss:  [0.00505244]\n",
      "Epoch:  94/250  Loss:  [0.00499125]\n",
      "Epoch:  95/250  Loss:  [0.00493146]\n",
      "Epoch:  96/250  Loss:  [0.00487302]\n",
      "Epoch:  97/250  Loss:  [0.00481588]\n",
      "Epoch:  98/250  Loss:  [0.00476001]\n",
      "Epoch:  99/250  Loss:  [0.00470537]\n",
      "Epoch:  100/250  Loss:  [0.00465192]\n",
      "Epoch:  101/250  Loss:  [0.00459961]\n",
      "Epoch:  102/250  Loss:  [0.00454842]\n",
      "Epoch:  103/250  Loss:  [0.00449831]\n",
      "Epoch:  104/250  Loss:  [0.00444925]\n",
      "Epoch:  105/250  Loss:  [0.0044012]\n",
      "Epoch:  106/250  Loss:  [0.00435415]\n",
      "Epoch:  107/250  Loss:  [0.00430805]\n",
      "Epoch:  108/250  Loss:  [0.00426288]\n",
      "Epoch:  109/250  Loss:  [0.00421861]\n",
      "Epoch:  110/250  Loss:  [0.00417522]\n",
      "Epoch:  111/250  Loss:  [0.00413268]\n",
      "Epoch:  112/250  Loss:  [0.00409097]\n",
      "Epoch:  113/250  Loss:  [0.00405007]\n",
      "Epoch:  114/250  Loss:  [0.00400994]\n",
      "Epoch:  115/250  Loss:  [0.00397058]\n",
      "Epoch:  116/250  Loss:  [0.00393196]\n",
      "Epoch:  117/250  Loss:  [0.00389406]\n",
      "Epoch:  118/250  Loss:  [0.00385686]\n",
      "Epoch:  119/250  Loss:  [0.00382034]\n",
      "Epoch:  120/250  Loss:  [0.00378449]\n",
      "Epoch:  121/250  Loss:  [0.00374928]\n",
      "Epoch:  122/250  Loss:  [0.00371471]\n",
      "Epoch:  123/250  Loss:  [0.00368075]\n",
      "Epoch:  124/250  Loss:  [0.00364738]\n",
      "Epoch:  125/250  Loss:  [0.0036146]\n",
      "Epoch:  126/250  Loss:  [0.00358239]\n",
      "Epoch:  127/250  Loss:  [0.00355074]\n",
      "Epoch:  128/250  Loss:  [0.00351962]\n",
      "Epoch:  129/250  Loss:  [0.00348903]\n",
      "Epoch:  130/250  Loss:  [0.00345895]\n",
      "Epoch:  131/250  Loss:  [0.00342938]\n",
      "Epoch:  132/250  Loss:  [0.0034003]\n",
      "Epoch:  133/250  Loss:  [0.00337169]\n",
      "Epoch:  134/250  Loss:  [0.00334356]\n",
      "Epoch:  135/250  Loss:  [0.00331587]\n",
      "Epoch:  136/250  Loss:  [0.00328864]\n",
      "Epoch:  137/250  Loss:  [0.00326183]\n",
      "Epoch:  138/250  Loss:  [0.00323546]\n",
      "Epoch:  139/250  Loss:  [0.00320949]\n",
      "Epoch:  140/250  Loss:  [0.00318394]\n",
      "Epoch:  141/250  Loss:  [0.00315878]\n",
      "Epoch:  142/250  Loss:  [0.003134]\n",
      "Epoch:  143/250  Loss:  [0.00310961]\n",
      "Epoch:  144/250  Loss:  [0.00308559]\n",
      "Epoch:  145/250  Loss:  [0.00306193]\n",
      "Epoch:  146/250  Loss:  [0.00303862]\n",
      "Epoch:  147/250  Loss:  [0.00301566]\n",
      "Epoch:  148/250  Loss:  [0.00299304]\n",
      "Epoch:  149/250  Loss:  [0.00297075]\n",
      "Epoch:  150/250  Loss:  [0.00294879]\n",
      "Epoch:  151/250  Loss:  [0.00292715]\n",
      "Epoch:  152/250  Loss:  [0.00290581]\n",
      "Epoch:  153/250  Loss:  [0.00288479]\n",
      "Epoch:  154/250  Loss:  [0.00286406]\n",
      "Epoch:  155/250  Loss:  [0.00284362]\n",
      "Epoch:  156/250  Loss:  [0.00282347]\n",
      "Epoch:  157/250  Loss:  [0.0028036]\n",
      "Epoch:  158/250  Loss:  [0.00278401]\n",
      "Epoch:  159/250  Loss:  [0.00276469]\n",
      "Epoch:  160/250  Loss:  [0.00274563]\n",
      "Epoch:  161/250  Loss:  [0.00272683]\n",
      "Epoch:  162/250  Loss:  [0.00270828]\n",
      "Epoch:  163/250  Loss:  [0.00268998]\n",
      "Epoch:  164/250  Loss:  [0.00267193]\n",
      "Epoch:  165/250  Loss:  [0.00265411]\n",
      "Epoch:  166/250  Loss:  [0.00263654]\n",
      "Epoch:  167/250  Loss:  [0.00261919]\n",
      "Epoch:  168/250  Loss:  [0.00260206]\n",
      "Epoch:  169/250  Loss:  [0.00258516]\n",
      "Epoch:  170/250  Loss:  [0.00256848]\n",
      "Epoch:  171/250  Loss:  [0.00255201]\n",
      "Epoch:  172/250  Loss:  [0.00253574]\n",
      "Epoch:  173/250  Loss:  [0.00251969]\n",
      "Epoch:  174/250  Loss:  [0.00250384]\n",
      "Epoch:  175/250  Loss:  [0.00248818]\n",
      "Epoch:  176/250  Loss:  [0.00247272]\n",
      "Epoch:  177/250  Loss:  [0.00245745]\n",
      "Epoch:  178/250  Loss:  [0.00244237]\n",
      "Epoch:  179/250  Loss:  [0.00242747]\n",
      "Epoch:  180/250  Loss:  [0.00241275]\n",
      "Epoch:  181/250  Loss:  [0.00239821]\n",
      "Epoch:  182/250  Loss:  [0.00238385]\n",
      "Epoch:  183/250  Loss:  [0.00236965]\n",
      "Epoch:  184/250  Loss:  [0.00235563]\n",
      "Epoch:  185/250  Loss:  [0.00234177]\n",
      "Epoch:  186/250  Loss:  [0.00232807]\n",
      "Epoch:  187/250  Loss:  [0.00231454]\n",
      "Epoch:  188/250  Loss:  [0.00230116]\n",
      "Epoch:  189/250  Loss:  [0.00228793]\n",
      "Epoch:  190/250  Loss:  [0.00227486]\n",
      "Epoch:  191/250  Loss:  [0.00226194]\n",
      "Epoch:  192/250  Loss:  [0.00224917]\n",
      "Epoch:  193/250  Loss:  [0.00223653]\n",
      "Epoch:  194/250  Loss:  [0.00222405]\n",
      "Epoch:  195/250  Loss:  [0.0022117]\n",
      "Epoch:  196/250  Loss:  [0.00219949]\n",
      "Epoch:  197/250  Loss:  [0.00218741]\n",
      "Epoch:  198/250  Loss:  [0.00217547]\n",
      "Epoch:  199/250  Loss:  [0.00216366]\n",
      "Epoch:  200/250  Loss:  [0.00215198]\n",
      "Epoch:  201/250  Loss:  [0.00214042]\n",
      "Epoch:  202/250  Loss:  [0.00212899]\n",
      "Epoch:  203/250  Loss:  [0.00211768]\n",
      "Epoch:  204/250  Loss:  [0.0021065]\n",
      "Epoch:  205/250  Loss:  [0.00209543]\n",
      "Epoch:  206/250  Loss:  [0.00208448]\n",
      "Epoch:  207/250  Loss:  [0.00207365]\n",
      "Epoch:  208/250  Loss:  [0.00206293]\n",
      "Epoch:  209/250  Loss:  [0.00205232]\n",
      "Epoch:  210/250  Loss:  [0.00204182]\n",
      "Epoch:  211/250  Loss:  [0.00203143]\n",
      "Epoch:  212/250  Loss:  [0.00202115]\n",
      "Epoch:  213/250  Loss:  [0.00201097]\n",
      "Epoch:  214/250  Loss:  [0.00200089]\n",
      "Epoch:  215/250  Loss:  [0.00199092]\n",
      "Epoch:  216/250  Loss:  [0.00198105]\n",
      "Epoch:  217/250  Loss:  [0.00197128]\n",
      "Epoch:  218/250  Loss:  [0.00196161]\n",
      "Epoch:  219/250  Loss:  [0.00195203]\n",
      "Epoch:  220/250  Loss:  [0.00194255]\n",
      "Epoch:  221/250  Loss:  [0.00193316]\n",
      "Epoch:  222/250  Loss:  [0.00192386]\n",
      "Epoch:  223/250  Loss:  [0.00191465]\n",
      "Epoch:  224/250  Loss:  [0.00190554]\n",
      "Epoch:  225/250  Loss:  [0.00189651]\n",
      "Epoch:  226/250  Loss:  [0.00188757]\n",
      "Epoch:  227/250  Loss:  [0.00187871]\n",
      "Epoch:  228/250  Loss:  [0.00186994]\n",
      "Epoch:  229/250  Loss:  [0.00186126]\n",
      "Epoch:  230/250  Loss:  [0.00185265]\n",
      "Epoch:  231/250  Loss:  [0.00184413]\n",
      "Epoch:  232/250  Loss:  [0.00183568]\n",
      "Epoch:  233/250  Loss:  [0.00182732]\n",
      "Epoch:  234/250  Loss:  [0.00181903]\n",
      "Epoch:  235/250  Loss:  [0.00181082]\n",
      "Epoch:  236/250  Loss:  [0.00180269]\n",
      "Epoch:  237/250  Loss:  [0.00179463]\n",
      "Epoch:  238/250  Loss:  [0.00178664]\n",
      "Epoch:  239/250  Loss:  [0.00177873]\n",
      "Epoch:  240/250  Loss:  [0.00177089]\n",
      "Epoch:  241/250  Loss:  [0.00176312]\n",
      "Epoch:  242/250  Loss:  [0.00175542]\n",
      "Epoch:  243/250  Loss:  [0.00174779]\n",
      "Epoch:  244/250  Loss:  [0.00174023]\n",
      "Epoch:  245/250  Loss:  [0.00173273]\n",
      "Epoch:  246/250  Loss:  [0.0017253]\n",
      "Epoch:  247/250  Loss:  [0.00171794]\n",
      "Epoch:  248/250  Loss:  [0.00171064]\n",
      "Epoch:  249/250  Loss:  [0.0017034]\n",
      "Epoch:  250/250  Loss:  [0.00169623]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 250 # number of itrations\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_xor[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0446088 ]\n",
      " [0.03235253]\n",
      " [0.0445691 ]\n",
      " [0.0323346 ]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
