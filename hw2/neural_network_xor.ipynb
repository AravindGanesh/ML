{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the back-propagation algorithm to learn the weights of a perceptron with 2 input nodes, 2 hidden nodes and 1 output node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **Python3** in used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters - input all parameter values here\n",
    "# note: Unless a cell is found with changed values, the training cells the parameters here \n",
    "input_dim = 2\n",
    "hidden_dim = 2 # dimensions of hidden layers\n",
    "std = 0.005  # train data noise standard deviation\n",
    "w_std = 0.5\n",
    "learn_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "x_inputs = np.array([np.zeros(2), np.ones(2), np.array([1,0]), np.array([0,1])])\n",
    "def generate_trainset(N):\n",
    "    X = np.repeat(x_inputs, N//4, axis=0)\n",
    "    y_xor = np.logical_xor(X.T[0], X.T[1]).astype(np.float)\n",
    "    # add noise to data\n",
    "    X += np.random.normal(0, std, X.shape)\n",
    "    y_xor += np.random.normal(0, std, N)\n",
    "    # shuffle the training data\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    x_train, y_train = X[indices], y_xor[indices]\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid( t):\n",
    "    return 1/(1  + np.exp(-t))\n",
    "\n",
    "def dsigmoid( t):\n",
    "    return sigmoid(t)*(1 - sigmoid(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Experiment with N = 1000\n",
    "This gives good performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "x_train, y_train = generate_trainset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/500  Loss:  [0.25778441]\n",
      "Epoch:  2/500  Loss:  [0.25423825]\n",
      "Epoch:  3/500  Loss:  [0.25228139]\n",
      "Epoch:  4/500  Loss:  [0.25120912]\n",
      "Epoch:  5/500  Loss:  [0.25062088]\n",
      "Epoch:  6/500  Loss:  [0.25029642]\n",
      "Epoch:  7/500  Loss:  [0.25011601]\n",
      "Epoch:  8/500  Loss:  [0.25001467]\n",
      "Epoch:  9/500  Loss:  [0.24995702]\n",
      "Epoch:  10/500  Loss:  [0.24992369]\n",
      "Epoch:  11/500  Loss:  [0.249904]\n",
      "Epoch:  12/500  Loss:  [0.24989204]\n",
      "Epoch:  13/500  Loss:  [0.2498845]\n",
      "Epoch:  14/500  Loss:  [0.24987954]\n",
      "Epoch:  15/500  Loss:  [0.24987609]\n",
      "Epoch:  16/500  Loss:  [0.24987355]\n",
      "Epoch:  17/500  Loss:  [0.24987156]\n",
      "Epoch:  18/500  Loss:  [0.24986992]\n",
      "Epoch:  19/500  Loss:  [0.2498685]\n",
      "Epoch:  20/500  Loss:  [0.24986722]\n",
      "Epoch:  21/500  Loss:  [0.24986603]\n",
      "Epoch:  22/500  Loss:  [0.24986491]\n",
      "Epoch:  23/500  Loss:  [0.24986383]\n",
      "Epoch:  24/500  Loss:  [0.24986278]\n",
      "Epoch:  25/500  Loss:  [0.24986174]\n",
      "Epoch:  26/500  Loss:  [0.24986072]\n",
      "Epoch:  27/500  Loss:  [0.2498597]\n",
      "Epoch:  28/500  Loss:  [0.24985867]\n",
      "Epoch:  29/500  Loss:  [0.24985765]\n",
      "Epoch:  30/500  Loss:  [0.24985662]\n",
      "Epoch:  31/500  Loss:  [0.24985558]\n",
      "Epoch:  32/500  Loss:  [0.24985454]\n",
      "Epoch:  33/500  Loss:  [0.24985348]\n",
      "Epoch:  34/500  Loss:  [0.2498524]\n",
      "Epoch:  35/500  Loss:  [0.24985132]\n",
      "Epoch:  36/500  Loss:  [0.24985021]\n",
      "Epoch:  37/500  Loss:  [0.24984909]\n",
      "Epoch:  38/500  Loss:  [0.24984795]\n",
      "Epoch:  39/500  Loss:  [0.24984679]\n",
      "Epoch:  40/500  Loss:  [0.24984561]\n",
      "Epoch:  41/500  Loss:  [0.2498444]\n",
      "Epoch:  42/500  Loss:  [0.24984317]\n",
      "Epoch:  43/500  Loss:  [0.24984192]\n",
      "Epoch:  44/500  Loss:  [0.24984064]\n",
      "Epoch:  45/500  Loss:  [0.24983933]\n",
      "Epoch:  46/500  Loss:  [0.24983799]\n",
      "Epoch:  47/500  Loss:  [0.24983662]\n",
      "Epoch:  48/500  Loss:  [0.24983522]\n",
      "Epoch:  49/500  Loss:  [0.24983379]\n",
      "Epoch:  50/500  Loss:  [0.24983232]\n",
      "Epoch:  51/500  Loss:  [0.24983081]\n",
      "Epoch:  52/500  Loss:  [0.24982927]\n",
      "Epoch:  53/500  Loss:  [0.24982769]\n",
      "Epoch:  54/500  Loss:  [0.24982607]\n",
      "Epoch:  55/500  Loss:  [0.24982441]\n",
      "Epoch:  56/500  Loss:  [0.2498227]\n",
      "Epoch:  57/500  Loss:  [0.24982095]\n",
      "Epoch:  58/500  Loss:  [0.24981915]\n",
      "Epoch:  59/500  Loss:  [0.24981731]\n",
      "Epoch:  60/500  Loss:  [0.24981541]\n",
      "Epoch:  61/500  Loss:  [0.24981346]\n",
      "Epoch:  62/500  Loss:  [0.24981146]\n",
      "Epoch:  63/500  Loss:  [0.24980941]\n",
      "Epoch:  64/500  Loss:  [0.2498073]\n",
      "Epoch:  65/500  Loss:  [0.24980513]\n",
      "Epoch:  66/500  Loss:  [0.2498029]\n",
      "Epoch:  67/500  Loss:  [0.24980061]\n",
      "Epoch:  68/500  Loss:  [0.24979825]\n",
      "Epoch:  69/500  Loss:  [0.24979583]\n",
      "Epoch:  70/500  Loss:  [0.24979334]\n",
      "Epoch:  71/500  Loss:  [0.24979077]\n",
      "Epoch:  72/500  Loss:  [0.24978814]\n",
      "Epoch:  73/500  Loss:  [0.24978543]\n",
      "Epoch:  74/500  Loss:  [0.24978265]\n",
      "Epoch:  75/500  Loss:  [0.24977979]\n",
      "Epoch:  76/500  Loss:  [0.24977684]\n",
      "Epoch:  77/500  Loss:  [0.24977381]\n",
      "Epoch:  78/500  Loss:  [0.24977069]\n",
      "Epoch:  79/500  Loss:  [0.24976749]\n",
      "Epoch:  80/500  Loss:  [0.24976419]\n",
      "Epoch:  81/500  Loss:  [0.24976079]\n",
      "Epoch:  82/500  Loss:  [0.2497573]\n",
      "Epoch:  83/500  Loss:  [0.24975371]\n",
      "Epoch:  84/500  Loss:  [0.24975001]\n",
      "Epoch:  85/500  Loss:  [0.24974621]\n",
      "Epoch:  86/500  Loss:  [0.24974229]\n",
      "Epoch:  87/500  Loss:  [0.24973826]\n",
      "Epoch:  88/500  Loss:  [0.24973411]\n",
      "Epoch:  89/500  Loss:  [0.24972984]\n",
      "Epoch:  90/500  Loss:  [0.24972544]\n",
      "Epoch:  91/500  Loss:  [0.24972091]\n",
      "Epoch:  92/500  Loss:  [0.24971624]\n",
      "Epoch:  93/500  Loss:  [0.24971143]\n",
      "Epoch:  94/500  Loss:  [0.24970648]\n",
      "Epoch:  95/500  Loss:  [0.24970138]\n",
      "Epoch:  96/500  Loss:  [0.24969613]\n",
      "Epoch:  97/500  Loss:  [0.24969071]\n",
      "Epoch:  98/500  Loss:  [0.24968512]\n",
      "Epoch:  99/500  Loss:  [0.24967936]\n",
      "Epoch:  100/500  Loss:  [0.24967342]\n",
      "Epoch:  101/500  Loss:  [0.24966729]\n",
      "Epoch:  102/500  Loss:  [0.24966097]\n",
      "Epoch:  103/500  Loss:  [0.24965445]\n",
      "Epoch:  104/500  Loss:  [0.24964771]\n",
      "Epoch:  105/500  Loss:  [0.24964076]\n",
      "Epoch:  106/500  Loss:  [0.24963357]\n",
      "Epoch:  107/500  Loss:  [0.24962615]\n",
      "Epoch:  108/500  Loss:  [0.24961848]\n",
      "Epoch:  109/500  Loss:  [0.24961055]\n",
      "Epoch:  110/500  Loss:  [0.24960234]\n",
      "Epoch:  111/500  Loss:  [0.24959386]\n",
      "Epoch:  112/500  Loss:  [0.24958508]\n",
      "Epoch:  113/500  Loss:  [0.24957598]\n",
      "Epoch:  114/500  Loss:  [0.24956656]\n",
      "Epoch:  115/500  Loss:  [0.24955681]\n",
      "Epoch:  116/500  Loss:  [0.24954669]\n",
      "Epoch:  117/500  Loss:  [0.2495362]\n",
      "Epoch:  118/500  Loss:  [0.24952532]\n",
      "Epoch:  119/500  Loss:  [0.24951402]\n",
      "Epoch:  120/500  Loss:  [0.24950228]\n",
      "Epoch:  121/500  Loss:  [0.24949009]\n",
      "Epoch:  122/500  Loss:  [0.24947742]\n",
      "Epoch:  123/500  Loss:  [0.24946423]\n",
      "Epoch:  124/500  Loss:  [0.24945052]\n",
      "Epoch:  125/500  Loss:  [0.24943623]\n",
      "Epoch:  126/500  Loss:  [0.24942135]\n",
      "Epoch:  127/500  Loss:  [0.24940584]\n",
      "Epoch:  128/500  Loss:  [0.24938965]\n",
      "Epoch:  129/500  Loss:  [0.24937276]\n",
      "Epoch:  130/500  Loss:  [0.24935512]\n",
      "Epoch:  131/500  Loss:  [0.24933669]\n",
      "Epoch:  132/500  Loss:  [0.24931742]\n",
      "Epoch:  133/500  Loss:  [0.24929725]\n",
      "Epoch:  134/500  Loss:  [0.24927613]\n",
      "Epoch:  135/500  Loss:  [0.249254]\n",
      "Epoch:  136/500  Loss:  [0.24923079]\n",
      "Epoch:  137/500  Loss:  [0.24920645]\n",
      "Epoch:  138/500  Loss:  [0.24918088]\n",
      "Epoch:  139/500  Loss:  [0.24915401]\n",
      "Epoch:  140/500  Loss:  [0.24912576]\n",
      "Epoch:  141/500  Loss:  [0.24909602]\n",
      "Epoch:  142/500  Loss:  [0.24906471]\n",
      "Epoch:  143/500  Loss:  [0.2490317]\n",
      "Epoch:  144/500  Loss:  [0.24899687]\n",
      "Epoch:  145/500  Loss:  [0.24896011]\n",
      "Epoch:  146/500  Loss:  [0.24892127]\n",
      "Epoch:  147/500  Loss:  [0.24888019]\n",
      "Epoch:  148/500  Loss:  [0.24883672]\n",
      "Epoch:  149/500  Loss:  [0.24879068]\n",
      "Epoch:  150/500  Loss:  [0.24874187]\n",
      "Epoch:  151/500  Loss:  [0.24869009]\n",
      "Epoch:  152/500  Loss:  [0.2486351]\n",
      "Epoch:  153/500  Loss:  [0.24857666]\n",
      "Epoch:  154/500  Loss:  [0.2485145]\n",
      "Epoch:  155/500  Loss:  [0.24844834]\n",
      "Epoch:  156/500  Loss:  [0.24837784]\n",
      "Epoch:  157/500  Loss:  [0.24830266]\n",
      "Epoch:  158/500  Loss:  [0.24822244]\n",
      "Epoch:  159/500  Loss:  [0.24813675]\n",
      "Epoch:  160/500  Loss:  [0.24804516]\n",
      "Epoch:  161/500  Loss:  [0.24794717]\n",
      "Epoch:  162/500  Loss:  [0.24784226]\n",
      "Epoch:  163/500  Loss:  [0.24772986]\n",
      "Epoch:  164/500  Loss:  [0.24760935]\n",
      "Epoch:  165/500  Loss:  [0.24748004]\n",
      "Epoch:  166/500  Loss:  [0.2473412]\n",
      "Epoch:  167/500  Loss:  [0.24719205]\n",
      "Epoch:  168/500  Loss:  [0.24703171]\n",
      "Epoch:  169/500  Loss:  [0.24685926]\n",
      "Epoch:  170/500  Loss:  [0.2466737]\n",
      "Epoch:  171/500  Loss:  [0.24647394]\n",
      "Epoch:  172/500  Loss:  [0.24625883]\n",
      "Epoch:  173/500  Loss:  [0.24602712]\n",
      "Epoch:  174/500  Loss:  [0.24577749]\n",
      "Epoch:  175/500  Loss:  [0.24550852]\n",
      "Epoch:  176/500  Loss:  [0.24521873]\n",
      "Epoch:  177/500  Loss:  [0.24490653]\n",
      "Epoch:  178/500  Loss:  [0.24457028]\n",
      "Epoch:  179/500  Loss:  [0.24420824]\n",
      "Epoch:  180/500  Loss:  [0.24381864]\n",
      "Epoch:  181/500  Loss:  [0.24339964]\n",
      "Epoch:  182/500  Loss:  [0.24294937]\n",
      "Epoch:  183/500  Loss:  [0.24246596]\n",
      "Epoch:  184/500  Loss:  [0.24194752]\n",
      "Epoch:  185/500  Loss:  [0.24139221]\n",
      "Epoch:  186/500  Loss:  [0.24079825]\n",
      "Epoch:  187/500  Loss:  [0.24016395]\n",
      "Epoch:  188/500  Loss:  [0.23948774]\n",
      "Epoch:  189/500  Loss:  [0.23876824]\n",
      "Epoch:  190/500  Loss:  [0.23800422]\n",
      "Epoch:  191/500  Loss:  [0.23719474]\n",
      "Epoch:  192/500  Loss:  [0.23633907]\n",
      "Epoch:  193/500  Loss:  [0.23543681]\n",
      "Epoch:  194/500  Loss:  [0.23448787]\n",
      "Epoch:  195/500  Loss:  [0.23349249]\n",
      "Epoch:  196/500  Loss:  [0.23245128]\n",
      "Epoch:  197/500  Loss:  [0.23136519]\n",
      "Epoch:  198/500  Loss:  [0.23023552]\n",
      "Epoch:  199/500  Loss:  [0.2290639]\n",
      "Epoch:  200/500  Loss:  [0.22785228]\n",
      "Epoch:  201/500  Loss:  [0.22660291]\n",
      "Epoch:  202/500  Loss:  [0.22531827]\n",
      "Epoch:  203/500  Loss:  [0.22400107]\n",
      "Epoch:  204/500  Loss:  [0.22265419]\n",
      "Epoch:  205/500  Loss:  [0.22128062]\n",
      "Epoch:  206/500  Loss:  [0.21988345]\n",
      "Epoch:  207/500  Loss:  [0.21846581]\n",
      "Epoch:  208/500  Loss:  [0.21703082]\n",
      "Epoch:  209/500  Loss:  [0.21558156]\n",
      "Epoch:  210/500  Loss:  [0.21412103]\n",
      "Epoch:  211/500  Loss:  [0.21265217]\n",
      "Epoch:  212/500  Loss:  [0.21117775]\n",
      "Epoch:  213/500  Loss:  [0.20970042]\n",
      "Epoch:  214/500  Loss:  [0.20822267]\n",
      "Epoch:  215/500  Loss:  [0.20674684]\n",
      "Epoch:  216/500  Loss:  [0.20527507]\n",
      "Epoch:  217/500  Loss:  [0.20380937]\n",
      "Epoch:  218/500  Loss:  [0.20235154]\n",
      "Epoch:  219/500  Loss:  [0.20090323]\n",
      "Epoch:  220/500  Loss:  [0.19946593]\n",
      "Epoch:  221/500  Loss:  [0.19804097]\n",
      "Epoch:  222/500  Loss:  [0.19662953]\n",
      "Epoch:  223/500  Loss:  [0.19523267]\n",
      "Epoch:  224/500  Loss:  [0.19385128]\n",
      "Epoch:  225/500  Loss:  [0.19248616]\n",
      "Epoch:  226/500  Loss:  [0.19113799]\n",
      "Epoch:  227/500  Loss:  [0.18980733]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  228/500  Loss:  [0.18849467]\n",
      "Epoch:  229/500  Loss:  [0.18720038]\n",
      "Epoch:  230/500  Loss:  [0.18592477]\n",
      "Epoch:  231/500  Loss:  [0.18466807]\n",
      "Epoch:  232/500  Loss:  [0.18343044]\n",
      "Epoch:  233/500  Loss:  [0.18221197]\n",
      "Epoch:  234/500  Loss:  [0.18101271]\n",
      "Epoch:  235/500  Loss:  [0.17983266]\n",
      "Epoch:  236/500  Loss:  [0.17867176]\n",
      "Epoch:  237/500  Loss:  [0.17752993]\n",
      "Epoch:  238/500  Loss:  [0.17640704]\n",
      "Epoch:  239/500  Loss:  [0.17530293]\n",
      "Epoch:  240/500  Loss:  [0.17421742]\n",
      "Epoch:  241/500  Loss:  [0.17315031]\n",
      "Epoch:  242/500  Loss:  [0.17210135]\n",
      "Epoch:  243/500  Loss:  [0.17107032]\n",
      "Epoch:  244/500  Loss:  [0.17005695]\n",
      "Epoch:  245/500  Loss:  [0.16906096]\n",
      "Epoch:  246/500  Loss:  [0.16808208]\n",
      "Epoch:  247/500  Loss:  [0.16712002]\n",
      "Epoch:  248/500  Loss:  [0.16617447]\n",
      "Epoch:  249/500  Loss:  [0.16524513]\n",
      "Epoch:  250/500  Loss:  [0.16433171]\n",
      "Epoch:  251/500  Loss:  [0.16343389]\n",
      "Epoch:  252/500  Loss:  [0.16255137]\n",
      "Epoch:  253/500  Loss:  [0.16168384]\n",
      "Epoch:  254/500  Loss:  [0.16083098]\n",
      "Epoch:  255/500  Loss:  [0.15999251]\n",
      "Epoch:  256/500  Loss:  [0.1591681]\n",
      "Epoch:  257/500  Loss:  [0.15835747]\n",
      "Epoch:  258/500  Loss:  [0.15756032]\n",
      "Epoch:  259/500  Loss:  [0.15677635]\n",
      "Epoch:  260/500  Loss:  [0.15600528]\n",
      "Epoch:  261/500  Loss:  [0.15524682]\n",
      "Epoch:  262/500  Loss:  [0.15450069]\n",
      "Epoch:  263/500  Loss:  [0.15376662]\n",
      "Epoch:  264/500  Loss:  [0.15304434]\n",
      "Epoch:  265/500  Loss:  [0.15233358]\n",
      "Epoch:  266/500  Loss:  [0.15163409]\n",
      "Epoch:  267/500  Loss:  [0.15094562]\n",
      "Epoch:  268/500  Loss:  [0.1502679]\n",
      "Epoch:  269/500  Loss:  [0.14960071]\n",
      "Epoch:  270/500  Loss:  [0.1489438]\n",
      "Epoch:  271/500  Loss:  [0.14829695]\n",
      "Epoch:  272/500  Loss:  [0.14765992]\n",
      "Epoch:  273/500  Loss:  [0.14703249]\n",
      "Epoch:  274/500  Loss:  [0.14641446]\n",
      "Epoch:  275/500  Loss:  [0.1458056]\n",
      "Epoch:  276/500  Loss:  [0.14520571]\n",
      "Epoch:  277/500  Loss:  [0.14461459]\n",
      "Epoch:  278/500  Loss:  [0.14403205]\n",
      "Epoch:  279/500  Loss:  [0.1434579]\n",
      "Epoch:  280/500  Loss:  [0.14289194]\n",
      "Epoch:  281/500  Loss:  [0.142334]\n",
      "Epoch:  282/500  Loss:  [0.1417839]\n",
      "Epoch:  283/500  Loss:  [0.14124147]\n",
      "Epoch:  284/500  Loss:  [0.14070654]\n",
      "Epoch:  285/500  Loss:  [0.14017894]\n",
      "Epoch:  286/500  Loss:  [0.13965852]\n",
      "Epoch:  287/500  Loss:  [0.13914512]\n",
      "Epoch:  288/500  Loss:  [0.1386386]\n",
      "Epoch:  289/500  Loss:  [0.13813879]\n",
      "Epoch:  290/500  Loss:  [0.13764557]\n",
      "Epoch:  291/500  Loss:  [0.13715878]\n",
      "Epoch:  292/500  Loss:  [0.13667829]\n",
      "Epoch:  293/500  Loss:  [0.13620398]\n",
      "Epoch:  294/500  Loss:  [0.13573571]\n",
      "Epoch:  295/500  Loss:  [0.13527335]\n",
      "Epoch:  296/500  Loss:  [0.13481678]\n",
      "Epoch:  297/500  Loss:  [0.13436588]\n",
      "Epoch:  298/500  Loss:  [0.13392054]\n",
      "Epoch:  299/500  Loss:  [0.13348064]\n",
      "Epoch:  300/500  Loss:  [0.13304607]\n",
      "Epoch:  301/500  Loss:  [0.13261672]\n",
      "Epoch:  302/500  Loss:  [0.13219249]\n",
      "Epoch:  303/500  Loss:  [0.13177327]\n",
      "Epoch:  304/500  Loss:  [0.13135897]\n",
      "Epoch:  305/500  Loss:  [0.13094949]\n",
      "Epoch:  306/500  Loss:  [0.13054473]\n",
      "Epoch:  307/500  Loss:  [0.1301446]\n",
      "Epoch:  308/500  Loss:  [0.12974901]\n",
      "Epoch:  309/500  Loss:  [0.12935787]\n",
      "Epoch:  310/500  Loss:  [0.12897109]\n",
      "Epoch:  311/500  Loss:  [0.1285886]\n",
      "Epoch:  312/500  Loss:  [0.12821031]\n",
      "Epoch:  313/500  Loss:  [0.12783614]\n",
      "Epoch:  314/500  Loss:  [0.12746601]\n",
      "Epoch:  315/500  Loss:  [0.12709985]\n",
      "Epoch:  316/500  Loss:  [0.12673758]\n",
      "Epoch:  317/500  Loss:  [0.12637913]\n",
      "Epoch:  318/500  Loss:  [0.12602443]\n",
      "Epoch:  319/500  Loss:  [0.12567342]\n",
      "Epoch:  320/500  Loss:  [0.12532602]\n",
      "Epoch:  321/500  Loss:  [0.12498217]\n",
      "Epoch:  322/500  Loss:  [0.1246418]\n",
      "Epoch:  323/500  Loss:  [0.12430486]\n",
      "Epoch:  324/500  Loss:  [0.12397128]\n",
      "Epoch:  325/500  Loss:  [0.123641]\n",
      "Epoch:  326/500  Loss:  [0.12331396]\n",
      "Epoch:  327/500  Loss:  [0.12299011]\n",
      "Epoch:  328/500  Loss:  [0.1226694]\n",
      "Epoch:  329/500  Loss:  [0.12235176]\n",
      "Epoch:  330/500  Loss:  [0.12203714]\n",
      "Epoch:  331/500  Loss:  [0.1217255]\n",
      "Epoch:  332/500  Loss:  [0.12141678]\n",
      "Epoch:  333/500  Loss:  [0.12111093]\n",
      "Epoch:  334/500  Loss:  [0.12080791]\n",
      "Epoch:  335/500  Loss:  [0.12050767]\n",
      "Epoch:  336/500  Loss:  [0.12021015]\n",
      "Epoch:  337/500  Loss:  [0.11991532]\n",
      "Epoch:  338/500  Loss:  [0.11962313]\n",
      "Epoch:  339/500  Loss:  [0.11933354]\n",
      "Epoch:  340/500  Loss:  [0.11904651]\n",
      "Epoch:  341/500  Loss:  [0.11876199]\n",
      "Epoch:  342/500  Loss:  [0.11847995]\n",
      "Epoch:  343/500  Loss:  [0.11820034]\n",
      "Epoch:  344/500  Loss:  [0.11792312]\n",
      "Epoch:  345/500  Loss:  [0.11764826]\n",
      "Epoch:  346/500  Loss:  [0.11737573]\n",
      "Epoch:  347/500  Loss:  [0.11710548]\n",
      "Epoch:  348/500  Loss:  [0.11683748]\n",
      "Epoch:  349/500  Loss:  [0.11657169]\n",
      "Epoch:  350/500  Loss:  [0.11630808]\n",
      "Epoch:  351/500  Loss:  [0.11604662]\n",
      "Epoch:  352/500  Loss:  [0.11578727]\n",
      "Epoch:  353/500  Loss:  [0.11553001]\n",
      "Epoch:  354/500  Loss:  [0.11527479]\n",
      "Epoch:  355/500  Loss:  [0.1150216]\n",
      "Epoch:  356/500  Loss:  [0.1147704]\n",
      "Epoch:  357/500  Loss:  [0.11452115]\n",
      "Epoch:  358/500  Loss:  [0.11427384]\n",
      "Epoch:  359/500  Loss:  [0.11402843]\n",
      "Epoch:  360/500  Loss:  [0.1137849]\n",
      "Epoch:  361/500  Loss:  [0.11354322]\n",
      "Epoch:  362/500  Loss:  [0.11330336]\n",
      "Epoch:  363/500  Loss:  [0.11306529]\n",
      "Epoch:  364/500  Loss:  [0.112829]\n",
      "Epoch:  365/500  Loss:  [0.11259445]\n",
      "Epoch:  366/500  Loss:  [0.11236162]\n",
      "Epoch:  367/500  Loss:  [0.11213048]\n",
      "Epoch:  368/500  Loss:  [0.11190102]\n",
      "Epoch:  369/500  Loss:  [0.11167321]\n",
      "Epoch:  370/500  Loss:  [0.11144703]\n",
      "Epoch:  371/500  Loss:  [0.11122245]\n",
      "Epoch:  372/500  Loss:  [0.11099945]\n",
      "Epoch:  373/500  Loss:  [0.11077802]\n",
      "Epoch:  374/500  Loss:  [0.11055812]\n",
      "Epoch:  375/500  Loss:  [0.11033975]\n",
      "Epoch:  376/500  Loss:  [0.11012287]\n",
      "Epoch:  377/500  Loss:  [0.10990747]\n",
      "Epoch:  378/500  Loss:  [0.10969354]\n",
      "Epoch:  379/500  Loss:  [0.10948104]\n",
      "Epoch:  380/500  Loss:  [0.10926996]\n",
      "Epoch:  381/500  Loss:  [0.10906029]\n",
      "Epoch:  382/500  Loss:  [0.108852]\n",
      "Epoch:  383/500  Loss:  [0.10864508]\n",
      "Epoch:  384/500  Loss:  [0.10843951]\n",
      "Epoch:  385/500  Loss:  [0.10823527]\n",
      "Epoch:  386/500  Loss:  [0.10803235]\n",
      "Epoch:  387/500  Loss:  [0.10783072]\n",
      "Epoch:  388/500  Loss:  [0.10763038]\n",
      "Epoch:  389/500  Loss:  [0.1074313]\n",
      "Epoch:  390/500  Loss:  [0.10723348]\n",
      "Epoch:  391/500  Loss:  [0.10703688]\n",
      "Epoch:  392/500  Loss:  [0.10684151]\n",
      "Epoch:  393/500  Loss:  [0.10664735]\n",
      "Epoch:  394/500  Loss:  [0.10645437]\n",
      "Epoch:  395/500  Loss:  [0.10626258]\n",
      "Epoch:  396/500  Loss:  [0.10607194]\n",
      "Epoch:  397/500  Loss:  [0.10588245]\n",
      "Epoch:  398/500  Loss:  [0.10569409]\n",
      "Epoch:  399/500  Loss:  [0.10550686]\n",
      "Epoch:  400/500  Loss:  [0.10532073]\n",
      "Epoch:  401/500  Loss:  [0.1051357]\n",
      "Epoch:  402/500  Loss:  [0.10495175]\n",
      "Epoch:  403/500  Loss:  [0.10476887]\n",
      "Epoch:  404/500  Loss:  [0.10458704]\n",
      "Epoch:  405/500  Loss:  [0.10440626]\n",
      "Epoch:  406/500  Loss:  [0.10422652]\n",
      "Epoch:  407/500  Loss:  [0.10404779]\n",
      "Epoch:  408/500  Loss:  [0.10387007]\n",
      "Epoch:  409/500  Loss:  [0.10369335]\n",
      "Epoch:  410/500  Loss:  [0.10351762]\n",
      "Epoch:  411/500  Loss:  [0.10334286]\n",
      "Epoch:  412/500  Loss:  [0.10316907]\n",
      "Epoch:  413/500  Loss:  [0.10299623]\n",
      "Epoch:  414/500  Loss:  [0.10282433]\n",
      "Epoch:  415/500  Loss:  [0.10265337]\n",
      "Epoch:  416/500  Loss:  [0.10248333]\n",
      "Epoch:  417/500  Loss:  [0.1023142]\n",
      "Epoch:  418/500  Loss:  [0.10214598]\n",
      "Epoch:  419/500  Loss:  [0.10197864]\n",
      "Epoch:  420/500  Loss:  [0.1018122]\n",
      "Epoch:  421/500  Loss:  [0.10164662]\n",
      "Epoch:  422/500  Loss:  [0.10148191]\n",
      "Epoch:  423/500  Loss:  [0.10131806]\n",
      "Epoch:  424/500  Loss:  [0.10115506]\n",
      "Epoch:  425/500  Loss:  [0.10099289]\n",
      "Epoch:  426/500  Loss:  [0.10083155]\n",
      "Epoch:  427/500  Loss:  [0.10067103]\n",
      "Epoch:  428/500  Loss:  [0.10051133]\n",
      "Epoch:  429/500  Loss:  [0.10035243]\n",
      "Epoch:  430/500  Loss:  [0.10019432]\n",
      "Epoch:  431/500  Loss:  [0.100037]\n",
      "Epoch:  432/500  Loss:  [0.09988046]\n",
      "Epoch:  433/500  Loss:  [0.09972469]\n",
      "Epoch:  434/500  Loss:  [0.09956969]\n",
      "Epoch:  435/500  Loss:  [0.09941544]\n",
      "Epoch:  436/500  Loss:  [0.09926195]\n",
      "Epoch:  437/500  Loss:  [0.09910919]\n",
      "Epoch:  438/500  Loss:  [0.09895717]\n",
      "Epoch:  439/500  Loss:  [0.09880587]\n",
      "Epoch:  440/500  Loss:  [0.09865529]\n",
      "Epoch:  441/500  Loss:  [0.09850543]\n",
      "Epoch:  442/500  Loss:  [0.09835627]\n",
      "Epoch:  443/500  Loss:  [0.09820782]\n",
      "Epoch:  444/500  Loss:  [0.09806005]\n",
      "Epoch:  445/500  Loss:  [0.09791297]\n",
      "Epoch:  446/500  Loss:  [0.09776657]\n",
      "Epoch:  447/500  Loss:  [0.09762084]\n",
      "Epoch:  448/500  Loss:  [0.09747578]\n",
      "Epoch:  449/500  Loss:  [0.09733138]\n",
      "Epoch:  450/500  Loss:  [0.09718764]\n",
      "Epoch:  451/500  Loss:  [0.09704454]\n",
      "Epoch:  452/500  Loss:  [0.09690208]\n",
      "Epoch:  453/500  Loss:  [0.09676026]\n",
      "Epoch:  454/500  Loss:  [0.09661907]\n",
      "Epoch:  455/500  Loss:  [0.09647851]\n",
      "Epoch:  456/500  Loss:  [0.09633856]\n",
      "Epoch:  457/500  Loss:  [0.09619923]\n",
      "Epoch:  458/500  Loss:  [0.09606051]\n",
      "Epoch:  459/500  Loss:  [0.09592238]\n",
      "Epoch:  460/500  Loss:  [0.09578486]\n",
      "Epoch:  461/500  Loss:  [0.09564793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  462/500  Loss:  [0.09551158]\n",
      "Epoch:  463/500  Loss:  [0.09537582]\n",
      "Epoch:  464/500  Loss:  [0.09524063]\n",
      "Epoch:  465/500  Loss:  [0.09510601]\n",
      "Epoch:  466/500  Loss:  [0.09497196]\n",
      "Epoch:  467/500  Loss:  [0.09483847]\n",
      "Epoch:  468/500  Loss:  [0.09470554]\n",
      "Epoch:  469/500  Loss:  [0.09457315]\n",
      "Epoch:  470/500  Loss:  [0.09444132]\n",
      "Epoch:  471/500  Loss:  [0.09431003]\n",
      "Epoch:  472/500  Loss:  [0.09417927]\n",
      "Epoch:  473/500  Loss:  [0.09404905]\n",
      "Epoch:  474/500  Loss:  [0.09391936]\n",
      "Epoch:  475/500  Loss:  [0.09379019]\n",
      "Epoch:  476/500  Loss:  [0.09366155]\n",
      "Epoch:  477/500  Loss:  [0.09353342]\n",
      "Epoch:  478/500  Loss:  [0.0934058]\n",
      "Epoch:  479/500  Loss:  [0.09327868]\n",
      "Epoch:  480/500  Loss:  [0.09315207]\n",
      "Epoch:  481/500  Loss:  [0.09302596]\n",
      "Epoch:  482/500  Loss:  [0.09290035]\n",
      "Epoch:  483/500  Loss:  [0.09277522]\n",
      "Epoch:  484/500  Loss:  [0.09265059]\n",
      "Epoch:  485/500  Loss:  [0.09252643]\n",
      "Epoch:  486/500  Loss:  [0.09240276]\n",
      "Epoch:  487/500  Loss:  [0.09227956]\n",
      "Epoch:  488/500  Loss:  [0.09215683]\n",
      "Epoch:  489/500  Loss:  [0.09203457]\n",
      "Epoch:  490/500  Loss:  [0.09191278]\n",
      "Epoch:  491/500  Loss:  [0.09179145]\n",
      "Epoch:  492/500  Loss:  [0.09167057]\n",
      "Epoch:  493/500  Loss:  [0.09155015]\n",
      "Epoch:  494/500  Loss:  [0.09143017]\n",
      "Epoch:  495/500  Loss:  [0.09131065]\n",
      "Epoch:  496/500  Loss:  [0.09119156]\n",
      "Epoch:  497/500  Loss:  [0.09107292]\n",
      "Epoch:  498/500  Loss:  [0.09095471]\n",
      "Epoch:  499/500  Loss:  [0.09083694]\n",
      "Epoch:  500/500  Loss:  [0.0907196]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))# [1,1], [1,0], [0,0], [0,1]\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 500 # number of itrations\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.13525868]\n",
      " [0.48221731]\n",
      " [0.76386501]\n",
      " [0.76426623]]\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Experiment with N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "x_train, y_train = generate_trainset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/1000  Loss:  [0.24921357]\n",
      "Epoch:  2/1000  Loss:  [0.24921568]\n",
      "Epoch:  3/1000  Loss:  [0.24921784]\n",
      "Epoch:  4/1000  Loss:  [0.24922006]\n",
      "Epoch:  5/1000  Loss:  [0.24922232]\n",
      "Epoch:  6/1000  Loss:  [0.24922462]\n",
      "Epoch:  7/1000  Loss:  [0.24922695]\n",
      "Epoch:  8/1000  Loss:  [0.24922931]\n",
      "Epoch:  9/1000  Loss:  [0.2492317]\n",
      "Epoch:  10/1000  Loss:  [0.24923411]\n",
      "Epoch:  11/1000  Loss:  [0.24923654]\n",
      "Epoch:  12/1000  Loss:  [0.24923898]\n",
      "Epoch:  13/1000  Loss:  [0.24924144]\n",
      "Epoch:  14/1000  Loss:  [0.2492439]\n",
      "Epoch:  15/1000  Loss:  [0.24924637]\n",
      "Epoch:  16/1000  Loss:  [0.24924884]\n",
      "Epoch:  17/1000  Loss:  [0.24925132]\n",
      "Epoch:  18/1000  Loss:  [0.24925379]\n",
      "Epoch:  19/1000  Loss:  [0.24925626]\n",
      "Epoch:  20/1000  Loss:  [0.24925872]\n",
      "Epoch:  21/1000  Loss:  [0.24926118]\n",
      "Epoch:  22/1000  Loss:  [0.24926363]\n",
      "Epoch:  23/1000  Loss:  [0.24926607]\n",
      "Epoch:  24/1000  Loss:  [0.2492685]\n",
      "Epoch:  25/1000  Loss:  [0.24927091]\n",
      "Epoch:  26/1000  Loss:  [0.24927332]\n",
      "Epoch:  27/1000  Loss:  [0.24927571]\n",
      "Epoch:  28/1000  Loss:  [0.24927808]\n",
      "Epoch:  29/1000  Loss:  [0.24928044]\n",
      "Epoch:  30/1000  Loss:  [0.24928278]\n",
      "Epoch:  31/1000  Loss:  [0.24928511]\n",
      "Epoch:  32/1000  Loss:  [0.24928742]\n",
      "Epoch:  33/1000  Loss:  [0.24928971]\n",
      "Epoch:  34/1000  Loss:  [0.24929198]\n",
      "Epoch:  35/1000  Loss:  [0.24929423]\n",
      "Epoch:  36/1000  Loss:  [0.24929647]\n",
      "Epoch:  37/1000  Loss:  [0.24929868]\n",
      "Epoch:  38/1000  Loss:  [0.24930088]\n",
      "Epoch:  39/1000  Loss:  [0.24930306]\n",
      "Epoch:  40/1000  Loss:  [0.24930521]\n",
      "Epoch:  41/1000  Loss:  [0.24930735]\n",
      "Epoch:  42/1000  Loss:  [0.24930947]\n",
      "Epoch:  43/1000  Loss:  [0.24931157]\n",
      "Epoch:  44/1000  Loss:  [0.24931365]\n",
      "Epoch:  45/1000  Loss:  [0.24931571]\n",
      "Epoch:  46/1000  Loss:  [0.24931775]\n",
      "Epoch:  47/1000  Loss:  [0.24931977]\n",
      "Epoch:  48/1000  Loss:  [0.24932177]\n",
      "Epoch:  49/1000  Loss:  [0.24932375]\n",
      "Epoch:  50/1000  Loss:  [0.24932572]\n",
      "Epoch:  51/1000  Loss:  [0.24932766]\n",
      "Epoch:  52/1000  Loss:  [0.24932959]\n",
      "Epoch:  53/1000  Loss:  [0.2493315]\n",
      "Epoch:  54/1000  Loss:  [0.24933339]\n",
      "Epoch:  55/1000  Loss:  [0.24933526]\n",
      "Epoch:  56/1000  Loss:  [0.24933712]\n",
      "Epoch:  57/1000  Loss:  [0.24933896]\n",
      "Epoch:  58/1000  Loss:  [0.24934078]\n",
      "Epoch:  59/1000  Loss:  [0.24934259]\n",
      "Epoch:  60/1000  Loss:  [0.24934437]\n",
      "Epoch:  61/1000  Loss:  [0.24934615]\n",
      "Epoch:  62/1000  Loss:  [0.24934791]\n",
      "Epoch:  63/1000  Loss:  [0.24934965]\n",
      "Epoch:  64/1000  Loss:  [0.24935137]\n",
      "Epoch:  65/1000  Loss:  [0.24935309]\n",
      "Epoch:  66/1000  Loss:  [0.24935478]\n",
      "Epoch:  67/1000  Loss:  [0.24935647]\n",
      "Epoch:  68/1000  Loss:  [0.24935814]\n",
      "Epoch:  69/1000  Loss:  [0.24935979]\n",
      "Epoch:  70/1000  Loss:  [0.24936144]\n",
      "Epoch:  71/1000  Loss:  [0.24936307]\n",
      "Epoch:  72/1000  Loss:  [0.24936468]\n",
      "Epoch:  73/1000  Loss:  [0.24936629]\n",
      "Epoch:  74/1000  Loss:  [0.24936788]\n",
      "Epoch:  75/1000  Loss:  [0.24936947]\n",
      "Epoch:  76/1000  Loss:  [0.24937104]\n",
      "Epoch:  77/1000  Loss:  [0.2493726]\n",
      "Epoch:  78/1000  Loss:  [0.24937414]\n",
      "Epoch:  79/1000  Loss:  [0.24937568]\n",
      "Epoch:  80/1000  Loss:  [0.24937721]\n",
      "Epoch:  81/1000  Loss:  [0.24937873]\n",
      "Epoch:  82/1000  Loss:  [0.24938024]\n",
      "Epoch:  83/1000  Loss:  [0.24938174]\n",
      "Epoch:  84/1000  Loss:  [0.24938323]\n",
      "Epoch:  85/1000  Loss:  [0.24938471]\n",
      "Epoch:  86/1000  Loss:  [0.24938618]\n",
      "Epoch:  87/1000  Loss:  [0.24938765]\n",
      "Epoch:  88/1000  Loss:  [0.2493891]\n",
      "Epoch:  89/1000  Loss:  [0.24939055]\n",
      "Epoch:  90/1000  Loss:  [0.249392]\n",
      "Epoch:  91/1000  Loss:  [0.24939343]\n",
      "Epoch:  92/1000  Loss:  [0.24939486]\n",
      "Epoch:  93/1000  Loss:  [0.24939628]\n",
      "Epoch:  94/1000  Loss:  [0.24939769]\n",
      "Epoch:  95/1000  Loss:  [0.2493991]\n",
      "Epoch:  96/1000  Loss:  [0.24940051]\n",
      "Epoch:  97/1000  Loss:  [0.2494019]\n",
      "Epoch:  98/1000  Loss:  [0.24940329]\n",
      "Epoch:  99/1000  Loss:  [0.24940468]\n",
      "Epoch:  100/1000  Loss:  [0.24940606]\n",
      "Epoch:  101/1000  Loss:  [0.24940744]\n",
      "Epoch:  102/1000  Loss:  [0.24940881]\n",
      "Epoch:  103/1000  Loss:  [0.24941018]\n",
      "Epoch:  104/1000  Loss:  [0.24941154]\n",
      "Epoch:  105/1000  Loss:  [0.2494129]\n",
      "Epoch:  106/1000  Loss:  [0.24941425]\n",
      "Epoch:  107/1000  Loss:  [0.24941561]\n",
      "Epoch:  108/1000  Loss:  [0.24941695]\n",
      "Epoch:  109/1000  Loss:  [0.2494183]\n",
      "Epoch:  110/1000  Loss:  [0.24941964]\n",
      "Epoch:  111/1000  Loss:  [0.24942098]\n",
      "Epoch:  112/1000  Loss:  [0.24942231]\n",
      "Epoch:  113/1000  Loss:  [0.24942365]\n",
      "Epoch:  114/1000  Loss:  [0.24942498]\n",
      "Epoch:  115/1000  Loss:  [0.2494263]\n",
      "Epoch:  116/1000  Loss:  [0.24942763]\n",
      "Epoch:  117/1000  Loss:  [0.24942895]\n",
      "Epoch:  118/1000  Loss:  [0.24943028]\n",
      "Epoch:  119/1000  Loss:  [0.2494316]\n",
      "Epoch:  120/1000  Loss:  [0.24943291]\n",
      "Epoch:  121/1000  Loss:  [0.24943423]\n",
      "Epoch:  122/1000  Loss:  [0.24943555]\n",
      "Epoch:  123/1000  Loss:  [0.24943686]\n",
      "Epoch:  124/1000  Loss:  [0.24943818]\n",
      "Epoch:  125/1000  Loss:  [0.24943949]\n",
      "Epoch:  126/1000  Loss:  [0.2494408]\n",
      "Epoch:  127/1000  Loss:  [0.24944211]\n",
      "Epoch:  128/1000  Loss:  [0.24944342]\n",
      "Epoch:  129/1000  Loss:  [0.24944473]\n",
      "Epoch:  130/1000  Loss:  [0.24944604]\n",
      "Epoch:  131/1000  Loss:  [0.24944735]\n",
      "Epoch:  132/1000  Loss:  [0.24944866]\n",
      "Epoch:  133/1000  Loss:  [0.24944996]\n",
      "Epoch:  134/1000  Loss:  [0.24945127]\n",
      "Epoch:  135/1000  Loss:  [0.24945258]\n",
      "Epoch:  136/1000  Loss:  [0.24945389]\n",
      "Epoch:  137/1000  Loss:  [0.2494552]\n",
      "Epoch:  138/1000  Loss:  [0.24945651]\n",
      "Epoch:  139/1000  Loss:  [0.24945782]\n",
      "Epoch:  140/1000  Loss:  [0.24945913]\n",
      "Epoch:  141/1000  Loss:  [0.24946044]\n",
      "Epoch:  142/1000  Loss:  [0.24946175]\n",
      "Epoch:  143/1000  Loss:  [0.24946306]\n",
      "Epoch:  144/1000  Loss:  [0.24946437]\n",
      "Epoch:  145/1000  Loss:  [0.24946569]\n",
      "Epoch:  146/1000  Loss:  [0.249467]\n",
      "Epoch:  147/1000  Loss:  [0.24946831]\n",
      "Epoch:  148/1000  Loss:  [0.24946963]\n",
      "Epoch:  149/1000  Loss:  [0.24947095]\n",
      "Epoch:  150/1000  Loss:  [0.24947227]\n",
      "Epoch:  151/1000  Loss:  [0.24947359]\n",
      "Epoch:  152/1000  Loss:  [0.24947491]\n",
      "Epoch:  153/1000  Loss:  [0.24947623]\n",
      "Epoch:  154/1000  Loss:  [0.24947755]\n",
      "Epoch:  155/1000  Loss:  [0.24947888]\n",
      "Epoch:  156/1000  Loss:  [0.24948021]\n",
      "Epoch:  157/1000  Loss:  [0.24948153]\n",
      "Epoch:  158/1000  Loss:  [0.24948286]\n",
      "Epoch:  159/1000  Loss:  [0.2494842]\n",
      "Epoch:  160/1000  Loss:  [0.24948553]\n",
      "Epoch:  161/1000  Loss:  [0.24948686]\n",
      "Epoch:  162/1000  Loss:  [0.2494882]\n",
      "Epoch:  163/1000  Loss:  [0.24948954]\n",
      "Epoch:  164/1000  Loss:  [0.24949088]\n",
      "Epoch:  165/1000  Loss:  [0.24949222]\n",
      "Epoch:  166/1000  Loss:  [0.24949356]\n",
      "Epoch:  167/1000  Loss:  [0.24949491]\n",
      "Epoch:  168/1000  Loss:  [0.24949626]\n",
      "Epoch:  169/1000  Loss:  [0.24949761]\n",
      "Epoch:  170/1000  Loss:  [0.24949896]\n",
      "Epoch:  171/1000  Loss:  [0.24950031]\n",
      "Epoch:  172/1000  Loss:  [0.24950167]\n",
      "Epoch:  173/1000  Loss:  [0.24950302]\n",
      "Epoch:  174/1000  Loss:  [0.24950438]\n",
      "Epoch:  175/1000  Loss:  [0.24950575]\n",
      "Epoch:  176/1000  Loss:  [0.24950711]\n",
      "Epoch:  177/1000  Loss:  [0.24950848]\n",
      "Epoch:  178/1000  Loss:  [0.24950984]\n",
      "Epoch:  179/1000  Loss:  [0.24951121]\n",
      "Epoch:  180/1000  Loss:  [0.24951259]\n",
      "Epoch:  181/1000  Loss:  [0.24951396]\n",
      "Epoch:  182/1000  Loss:  [0.24951534]\n",
      "Epoch:  183/1000  Loss:  [0.24951672]\n",
      "Epoch:  184/1000  Loss:  [0.2495181]\n",
      "Epoch:  185/1000  Loss:  [0.24951948]\n",
      "Epoch:  186/1000  Loss:  [0.24952087]\n",
      "Epoch:  187/1000  Loss:  [0.24952226]\n",
      "Epoch:  188/1000  Loss:  [0.24952365]\n",
      "Epoch:  189/1000  Loss:  [0.24952504]\n",
      "Epoch:  190/1000  Loss:  [0.24952644]\n",
      "Epoch:  191/1000  Loss:  [0.24952784]\n",
      "Epoch:  192/1000  Loss:  [0.24952924]\n",
      "Epoch:  193/1000  Loss:  [0.24953064]\n",
      "Epoch:  194/1000  Loss:  [0.24953204]\n",
      "Epoch:  195/1000  Loss:  [0.24953345]\n",
      "Epoch:  196/1000  Loss:  [0.24953486]\n",
      "Epoch:  197/1000  Loss:  [0.24953627]\n",
      "Epoch:  198/1000  Loss:  [0.24953769]\n",
      "Epoch:  199/1000  Loss:  [0.2495391]\n",
      "Epoch:  200/1000  Loss:  [0.24954052]\n",
      "Epoch:  201/1000  Loss:  [0.24954194]\n",
      "Epoch:  202/1000  Loss:  [0.24954337]\n",
      "Epoch:  203/1000  Loss:  [0.24954479]\n",
      "Epoch:  204/1000  Loss:  [0.24954622]\n",
      "Epoch:  205/1000  Loss:  [0.24954765]\n",
      "Epoch:  206/1000  Loss:  [0.24954909]\n",
      "Epoch:  207/1000  Loss:  [0.24955052]\n",
      "Epoch:  208/1000  Loss:  [0.24955196]\n",
      "Epoch:  209/1000  Loss:  [0.2495534]\n",
      "Epoch:  210/1000  Loss:  [0.24955484]\n",
      "Epoch:  211/1000  Loss:  [0.24955629]\n",
      "Epoch:  212/1000  Loss:  [0.24955774]\n",
      "Epoch:  213/1000  Loss:  [0.24955919]\n",
      "Epoch:  214/1000  Loss:  [0.24956064]\n",
      "Epoch:  215/1000  Loss:  [0.24956209]\n",
      "Epoch:  216/1000  Loss:  [0.24956355]\n",
      "Epoch:  217/1000  Loss:  [0.24956501]\n",
      "Epoch:  218/1000  Loss:  [0.24956647]\n",
      "Epoch:  219/1000  Loss:  [0.24956794]\n",
      "Epoch:  220/1000  Loss:  [0.2495694]\n",
      "Epoch:  221/1000  Loss:  [0.24957087]\n",
      "Epoch:  222/1000  Loss:  [0.24957234]\n",
      "Epoch:  223/1000  Loss:  [0.24957381]\n",
      "Epoch:  224/1000  Loss:  [0.24957529]\n",
      "Epoch:  225/1000  Loss:  [0.24957677]\n",
      "Epoch:  226/1000  Loss:  [0.24957825]\n",
      "Epoch:  227/1000  Loss:  [0.24957973]\n",
      "Epoch:  228/1000  Loss:  [0.24958121]\n",
      "Epoch:  229/1000  Loss:  [0.2495827]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  230/1000  Loss:  [0.24958419]\n",
      "Epoch:  231/1000  Loss:  [0.24958568]\n",
      "Epoch:  232/1000  Loss:  [0.24958717]\n",
      "Epoch:  233/1000  Loss:  [0.24958867]\n",
      "Epoch:  234/1000  Loss:  [0.24959017]\n",
      "Epoch:  235/1000  Loss:  [0.24959167]\n",
      "Epoch:  236/1000  Loss:  [0.24959317]\n",
      "Epoch:  237/1000  Loss:  [0.24959467]\n",
      "Epoch:  238/1000  Loss:  [0.24959618]\n",
      "Epoch:  239/1000  Loss:  [0.24959769]\n",
      "Epoch:  240/1000  Loss:  [0.2495992]\n",
      "Epoch:  241/1000  Loss:  [0.24960071]\n",
      "Epoch:  242/1000  Loss:  [0.24960222]\n",
      "Epoch:  243/1000  Loss:  [0.24960374]\n",
      "Epoch:  244/1000  Loss:  [0.24960526]\n",
      "Epoch:  245/1000  Loss:  [0.24960678]\n",
      "Epoch:  246/1000  Loss:  [0.2496083]\n",
      "Epoch:  247/1000  Loss:  [0.24960983]\n",
      "Epoch:  248/1000  Loss:  [0.24961135]\n",
      "Epoch:  249/1000  Loss:  [0.24961288]\n",
      "Epoch:  250/1000  Loss:  [0.24961441]\n",
      "Epoch:  251/1000  Loss:  [0.24961594]\n",
      "Epoch:  252/1000  Loss:  [0.24961748]\n",
      "Epoch:  253/1000  Loss:  [0.24961901]\n",
      "Epoch:  254/1000  Loss:  [0.24962055]\n",
      "Epoch:  255/1000  Loss:  [0.24962209]\n",
      "Epoch:  256/1000  Loss:  [0.24962363]\n",
      "Epoch:  257/1000  Loss:  [0.24962518]\n",
      "Epoch:  258/1000  Loss:  [0.24962672]\n",
      "Epoch:  259/1000  Loss:  [0.24962827]\n",
      "Epoch:  260/1000  Loss:  [0.24962982]\n",
      "Epoch:  261/1000  Loss:  [0.24963137]\n",
      "Epoch:  262/1000  Loss:  [0.24963292]\n",
      "Epoch:  263/1000  Loss:  [0.24963448]\n",
      "Epoch:  264/1000  Loss:  [0.24963603]\n",
      "Epoch:  265/1000  Loss:  [0.24963759]\n",
      "Epoch:  266/1000  Loss:  [0.24963915]\n",
      "Epoch:  267/1000  Loss:  [0.24964071]\n",
      "Epoch:  268/1000  Loss:  [0.24964227]\n",
      "Epoch:  269/1000  Loss:  [0.24964384]\n",
      "Epoch:  270/1000  Loss:  [0.2496454]\n",
      "Epoch:  271/1000  Loss:  [0.24964697]\n",
      "Epoch:  272/1000  Loss:  [0.24964854]\n",
      "Epoch:  273/1000  Loss:  [0.24965011]\n",
      "Epoch:  274/1000  Loss:  [0.24965168]\n",
      "Epoch:  275/1000  Loss:  [0.24965326]\n",
      "Epoch:  276/1000  Loss:  [0.24965483]\n",
      "Epoch:  277/1000  Loss:  [0.24965641]\n",
      "Epoch:  278/1000  Loss:  [0.24965799]\n",
      "Epoch:  279/1000  Loss:  [0.24965957]\n",
      "Epoch:  280/1000  Loss:  [0.24966115]\n",
      "Epoch:  281/1000  Loss:  [0.24966273]\n",
      "Epoch:  282/1000  Loss:  [0.24966431]\n",
      "Epoch:  283/1000  Loss:  [0.2496659]\n",
      "Epoch:  284/1000  Loss:  [0.24966749]\n",
      "Epoch:  285/1000  Loss:  [0.24966907]\n",
      "Epoch:  286/1000  Loss:  [0.24967066]\n",
      "Epoch:  287/1000  Loss:  [0.24967225]\n",
      "Epoch:  288/1000  Loss:  [0.24967384]\n",
      "Epoch:  289/1000  Loss:  [0.24967544]\n",
      "Epoch:  290/1000  Loss:  [0.24967703]\n",
      "Epoch:  291/1000  Loss:  [0.24967863]\n",
      "Epoch:  292/1000  Loss:  [0.24968022]\n",
      "Epoch:  293/1000  Loss:  [0.24968182]\n",
      "Epoch:  294/1000  Loss:  [0.24968342]\n",
      "Epoch:  295/1000  Loss:  [0.24968502]\n",
      "Epoch:  296/1000  Loss:  [0.24968662]\n",
      "Epoch:  297/1000  Loss:  [0.24968822]\n",
      "Epoch:  298/1000  Loss:  [0.24968982]\n",
      "Epoch:  299/1000  Loss:  [0.24969143]\n",
      "Epoch:  300/1000  Loss:  [0.24969303]\n",
      "Epoch:  301/1000  Loss:  [0.24969464]\n",
      "Epoch:  302/1000  Loss:  [0.24969625]\n",
      "Epoch:  303/1000  Loss:  [0.24969785]\n",
      "Epoch:  304/1000  Loss:  [0.24969946]\n",
      "Epoch:  305/1000  Loss:  [0.24970107]\n",
      "Epoch:  306/1000  Loss:  [0.24970268]\n",
      "Epoch:  307/1000  Loss:  [0.24970429]\n",
      "Epoch:  308/1000  Loss:  [0.24970591]\n",
      "Epoch:  309/1000  Loss:  [0.24970752]\n",
      "Epoch:  310/1000  Loss:  [0.24970913]\n",
      "Epoch:  311/1000  Loss:  [0.24971075]\n",
      "Epoch:  312/1000  Loss:  [0.24971236]\n",
      "Epoch:  313/1000  Loss:  [0.24971398]\n",
      "Epoch:  314/1000  Loss:  [0.2497156]\n",
      "Epoch:  315/1000  Loss:  [0.24971721]\n",
      "Epoch:  316/1000  Loss:  [0.24971883]\n",
      "Epoch:  317/1000  Loss:  [0.24972045]\n",
      "Epoch:  318/1000  Loss:  [0.24972207]\n",
      "Epoch:  319/1000  Loss:  [0.24972369]\n",
      "Epoch:  320/1000  Loss:  [0.24972531]\n",
      "Epoch:  321/1000  Loss:  [0.24972693]\n",
      "Epoch:  322/1000  Loss:  [0.24972855]\n",
      "Epoch:  323/1000  Loss:  [0.24973018]\n",
      "Epoch:  324/1000  Loss:  [0.2497318]\n",
      "Epoch:  325/1000  Loss:  [0.24973342]\n",
      "Epoch:  326/1000  Loss:  [0.24973505]\n",
      "Epoch:  327/1000  Loss:  [0.24973667]\n",
      "Epoch:  328/1000  Loss:  [0.2497383]\n",
      "Epoch:  329/1000  Loss:  [0.24973992]\n",
      "Epoch:  330/1000  Loss:  [0.24974155]\n",
      "Epoch:  331/1000  Loss:  [0.24974317]\n",
      "Epoch:  332/1000  Loss:  [0.2497448]\n",
      "Epoch:  333/1000  Loss:  [0.24974642]\n",
      "Epoch:  334/1000  Loss:  [0.24974805]\n",
      "Epoch:  335/1000  Loss:  [0.24974968]\n",
      "Epoch:  336/1000  Loss:  [0.2497513]\n",
      "Epoch:  337/1000  Loss:  [0.24975293]\n",
      "Epoch:  338/1000  Loss:  [0.24975456]\n",
      "Epoch:  339/1000  Loss:  [0.24975619]\n",
      "Epoch:  340/1000  Loss:  [0.24975781]\n",
      "Epoch:  341/1000  Loss:  [0.24975944]\n",
      "Epoch:  342/1000  Loss:  [0.24976107]\n",
      "Epoch:  343/1000  Loss:  [0.2497627]\n",
      "Epoch:  344/1000  Loss:  [0.24976433]\n",
      "Epoch:  345/1000  Loss:  [0.24976595]\n",
      "Epoch:  346/1000  Loss:  [0.24976758]\n",
      "Epoch:  347/1000  Loss:  [0.24976921]\n",
      "Epoch:  348/1000  Loss:  [0.24977084]\n",
      "Epoch:  349/1000  Loss:  [0.24977247]\n",
      "Epoch:  350/1000  Loss:  [0.24977409]\n",
      "Epoch:  351/1000  Loss:  [0.24977572]\n",
      "Epoch:  352/1000  Loss:  [0.24977735]\n",
      "Epoch:  353/1000  Loss:  [0.24977898]\n",
      "Epoch:  354/1000  Loss:  [0.2497806]\n",
      "Epoch:  355/1000  Loss:  [0.24978223]\n",
      "Epoch:  356/1000  Loss:  [0.24978386]\n",
      "Epoch:  357/1000  Loss:  [0.24978548]\n",
      "Epoch:  358/1000  Loss:  [0.24978711]\n",
      "Epoch:  359/1000  Loss:  [0.24978874]\n",
      "Epoch:  360/1000  Loss:  [0.24979036]\n",
      "Epoch:  361/1000  Loss:  [0.24979199]\n",
      "Epoch:  362/1000  Loss:  [0.24979361]\n",
      "Epoch:  363/1000  Loss:  [0.24979524]\n",
      "Epoch:  364/1000  Loss:  [0.24979686]\n",
      "Epoch:  365/1000  Loss:  [0.24979849]\n",
      "Epoch:  366/1000  Loss:  [0.24980011]\n",
      "Epoch:  367/1000  Loss:  [0.24980173]\n",
      "Epoch:  368/1000  Loss:  [0.24980335]\n",
      "Epoch:  369/1000  Loss:  [0.24980498]\n",
      "Epoch:  370/1000  Loss:  [0.2498066]\n",
      "Epoch:  371/1000  Loss:  [0.24980822]\n",
      "Epoch:  372/1000  Loss:  [0.24980984]\n",
      "Epoch:  373/1000  Loss:  [0.24981146]\n",
      "Epoch:  374/1000  Loss:  [0.24981308]\n",
      "Epoch:  375/1000  Loss:  [0.2498147]\n",
      "Epoch:  376/1000  Loss:  [0.24981631]\n",
      "Epoch:  377/1000  Loss:  [0.24981793]\n",
      "Epoch:  378/1000  Loss:  [0.24981955]\n",
      "Epoch:  379/1000  Loss:  [0.24982116]\n",
      "Epoch:  380/1000  Loss:  [0.24982278]\n",
      "Epoch:  381/1000  Loss:  [0.24982439]\n",
      "Epoch:  382/1000  Loss:  [0.249826]\n",
      "Epoch:  383/1000  Loss:  [0.24982762]\n",
      "Epoch:  384/1000  Loss:  [0.24982923]\n",
      "Epoch:  385/1000  Loss:  [0.24983084]\n",
      "Epoch:  386/1000  Loss:  [0.24983245]\n",
      "Epoch:  387/1000  Loss:  [0.24983406]\n",
      "Epoch:  388/1000  Loss:  [0.24983567]\n",
      "Epoch:  389/1000  Loss:  [0.24983727]\n",
      "Epoch:  390/1000  Loss:  [0.24983888]\n",
      "Epoch:  391/1000  Loss:  [0.24984048]\n",
      "Epoch:  392/1000  Loss:  [0.24984209]\n",
      "Epoch:  393/1000  Loss:  [0.24984369]\n",
      "Epoch:  394/1000  Loss:  [0.24984529]\n",
      "Epoch:  395/1000  Loss:  [0.24984689]\n",
      "Epoch:  396/1000  Loss:  [0.24984849]\n",
      "Epoch:  397/1000  Loss:  [0.24985009]\n",
      "Epoch:  398/1000  Loss:  [0.24985169]\n",
      "Epoch:  399/1000  Loss:  [0.24985328]\n",
      "Epoch:  400/1000  Loss:  [0.24985488]\n",
      "Epoch:  401/1000  Loss:  [0.24985647]\n",
      "Epoch:  402/1000  Loss:  [0.24985806]\n",
      "Epoch:  403/1000  Loss:  [0.24985966]\n",
      "Epoch:  404/1000  Loss:  [0.24986124]\n",
      "Epoch:  405/1000  Loss:  [0.24986283]\n",
      "Epoch:  406/1000  Loss:  [0.24986442]\n",
      "Epoch:  407/1000  Loss:  [0.24986601]\n",
      "Epoch:  408/1000  Loss:  [0.24986759]\n",
      "Epoch:  409/1000  Loss:  [0.24986917]\n",
      "Epoch:  410/1000  Loss:  [0.24987075]\n",
      "Epoch:  411/1000  Loss:  [0.24987233]\n",
      "Epoch:  412/1000  Loss:  [0.24987391]\n",
      "Epoch:  413/1000  Loss:  [0.24987549]\n",
      "Epoch:  414/1000  Loss:  [0.24987706]\n",
      "Epoch:  415/1000  Loss:  [0.24987864]\n",
      "Epoch:  416/1000  Loss:  [0.24988021]\n",
      "Epoch:  417/1000  Loss:  [0.24988178]\n",
      "Epoch:  418/1000  Loss:  [0.24988335]\n",
      "Epoch:  419/1000  Loss:  [0.24988492]\n",
      "Epoch:  420/1000  Loss:  [0.24988648]\n",
      "Epoch:  421/1000  Loss:  [0.24988805]\n",
      "Epoch:  422/1000  Loss:  [0.24988961]\n",
      "Epoch:  423/1000  Loss:  [0.24989117]\n",
      "Epoch:  424/1000  Loss:  [0.24989273]\n",
      "Epoch:  425/1000  Loss:  [0.24989429]\n",
      "Epoch:  426/1000  Loss:  [0.24989584]\n",
      "Epoch:  427/1000  Loss:  [0.24989739]\n",
      "Epoch:  428/1000  Loss:  [0.24989895]\n",
      "Epoch:  429/1000  Loss:  [0.2499005]\n",
      "Epoch:  430/1000  Loss:  [0.24990204]\n",
      "Epoch:  431/1000  Loss:  [0.24990359]\n",
      "Epoch:  432/1000  Loss:  [0.24990513]\n",
      "Epoch:  433/1000  Loss:  [0.24990668]\n",
      "Epoch:  434/1000  Loss:  [0.24990822]\n",
      "Epoch:  435/1000  Loss:  [0.24990975]\n",
      "Epoch:  436/1000  Loss:  [0.24991129]\n",
      "Epoch:  437/1000  Loss:  [0.24991282]\n",
      "Epoch:  438/1000  Loss:  [0.24991436]\n",
      "Epoch:  439/1000  Loss:  [0.24991589]\n",
      "Epoch:  440/1000  Loss:  [0.24991741]\n",
      "Epoch:  441/1000  Loss:  [0.24991894]\n",
      "Epoch:  442/1000  Loss:  [0.24992046]\n",
      "Epoch:  443/1000  Loss:  [0.24992198]\n",
      "Epoch:  444/1000  Loss:  [0.2499235]\n",
      "Epoch:  445/1000  Loss:  [0.24992502]\n",
      "Epoch:  446/1000  Loss:  [0.24992653]\n",
      "Epoch:  447/1000  Loss:  [0.24992805]\n",
      "Epoch:  448/1000  Loss:  [0.24992956]\n",
      "Epoch:  449/1000  Loss:  [0.24993106]\n",
      "Epoch:  450/1000  Loss:  [0.24993257]\n",
      "Epoch:  451/1000  Loss:  [0.24993407]\n",
      "Epoch:  452/1000  Loss:  [0.24993557]\n",
      "Epoch:  453/1000  Loss:  [0.24993707]\n",
      "Epoch:  454/1000  Loss:  [0.24993857]\n",
      "Epoch:  455/1000  Loss:  [0.24994006]\n",
      "Epoch:  456/1000  Loss:  [0.24994155]\n",
      "Epoch:  457/1000  Loss:  [0.24994304]\n",
      "Epoch:  458/1000  Loss:  [0.24994453]\n",
      "Epoch:  459/1000  Loss:  [0.24994601]\n",
      "Epoch:  460/1000  Loss:  [0.24994749]\n",
      "Epoch:  461/1000  Loss:  [0.24994897]\n",
      "Epoch:  462/1000  Loss:  [0.24995045]\n",
      "Epoch:  463/1000  Loss:  [0.24995192]\n",
      "Epoch:  464/1000  Loss:  [0.24995339]\n",
      "Epoch:  465/1000  Loss:  [0.24995486]\n",
      "Epoch:  466/1000  Loss:  [0.24995633]\n",
      "Epoch:  467/1000  Loss:  [0.24995779]\n",
      "Epoch:  468/1000  Loss:  [0.24995925]\n",
      "Epoch:  469/1000  Loss:  [0.24996071]\n",
      "Epoch:  470/1000  Loss:  [0.24996216]\n",
      "Epoch:  471/1000  Loss:  [0.24996361]\n",
      "Epoch:  472/1000  Loss:  [0.24996506]\n",
      "Epoch:  473/1000  Loss:  [0.24996651]\n",
      "Epoch:  474/1000  Loss:  [0.24996795]\n",
      "Epoch:  475/1000  Loss:  [0.24996939]\n",
      "Epoch:  476/1000  Loss:  [0.24997083]\n",
      "Epoch:  477/1000  Loss:  [0.24997226]\n",
      "Epoch:  478/1000  Loss:  [0.2499737]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  479/1000  Loss:  [0.24997513]\n",
      "Epoch:  480/1000  Loss:  [0.24997655]\n",
      "Epoch:  481/1000  Loss:  [0.24997798]\n",
      "Epoch:  482/1000  Loss:  [0.2499794]\n",
      "Epoch:  483/1000  Loss:  [0.24998081]\n",
      "Epoch:  484/1000  Loss:  [0.24998223]\n",
      "Epoch:  485/1000  Loss:  [0.24998364]\n",
      "Epoch:  486/1000  Loss:  [0.24998505]\n",
      "Epoch:  487/1000  Loss:  [0.24998645]\n",
      "Epoch:  488/1000  Loss:  [0.24998785]\n",
      "Epoch:  489/1000  Loss:  [0.24998925]\n",
      "Epoch:  490/1000  Loss:  [0.24999065]\n",
      "Epoch:  491/1000  Loss:  [0.24999204]\n",
      "Epoch:  492/1000  Loss:  [0.24999343]\n",
      "Epoch:  493/1000  Loss:  [0.24999482]\n",
      "Epoch:  494/1000  Loss:  [0.2499962]\n",
      "Epoch:  495/1000  Loss:  [0.24999758]\n",
      "Epoch:  496/1000  Loss:  [0.24999895]\n",
      "Epoch:  497/1000  Loss:  [0.25000033]\n",
      "Epoch:  498/1000  Loss:  [0.2500017]\n",
      "Epoch:  499/1000  Loss:  [0.25000306]\n",
      "Epoch:  500/1000  Loss:  [0.25000443]\n",
      "Epoch:  501/1000  Loss:  [0.25000579]\n",
      "Epoch:  502/1000  Loss:  [0.25000714]\n",
      "Epoch:  503/1000  Loss:  [0.2500085]\n",
      "Epoch:  504/1000  Loss:  [0.25000985]\n",
      "Epoch:  505/1000  Loss:  [0.25001119]\n",
      "Epoch:  506/1000  Loss:  [0.25001254]\n",
      "Epoch:  507/1000  Loss:  [0.25001388]\n",
      "Epoch:  508/1000  Loss:  [0.25001521]\n",
      "Epoch:  509/1000  Loss:  [0.25001655]\n",
      "Epoch:  510/1000  Loss:  [0.25001787]\n",
      "Epoch:  511/1000  Loss:  [0.2500192]\n",
      "Epoch:  512/1000  Loss:  [0.25002052]\n",
      "Epoch:  513/1000  Loss:  [0.25002184]\n",
      "Epoch:  514/1000  Loss:  [0.25002316]\n",
      "Epoch:  515/1000  Loss:  [0.25002447]\n",
      "Epoch:  516/1000  Loss:  [0.25002577]\n",
      "Epoch:  517/1000  Loss:  [0.25002708]\n",
      "Epoch:  518/1000  Loss:  [0.25002838]\n",
      "Epoch:  519/1000  Loss:  [0.25002968]\n",
      "Epoch:  520/1000  Loss:  [0.25003097]\n",
      "Epoch:  521/1000  Loss:  [0.25003226]\n",
      "Epoch:  522/1000  Loss:  [0.25003354]\n",
      "Epoch:  523/1000  Loss:  [0.25003483]\n",
      "Epoch:  524/1000  Loss:  [0.2500361]\n",
      "Epoch:  525/1000  Loss:  [0.25003738]\n",
      "Epoch:  526/1000  Loss:  [0.25003865]\n",
      "Epoch:  527/1000  Loss:  [0.25003992]\n",
      "Epoch:  528/1000  Loss:  [0.25004118]\n",
      "Epoch:  529/1000  Loss:  [0.25004244]\n",
      "Epoch:  530/1000  Loss:  [0.25004369]\n",
      "Epoch:  531/1000  Loss:  [0.25004494]\n",
      "Epoch:  532/1000  Loss:  [0.25004619]\n",
      "Epoch:  533/1000  Loss:  [0.25004744]\n",
      "Epoch:  534/1000  Loss:  [0.25004868]\n",
      "Epoch:  535/1000  Loss:  [0.25004991]\n",
      "Epoch:  536/1000  Loss:  [0.25005114]\n",
      "Epoch:  537/1000  Loss:  [0.25005237]\n",
      "Epoch:  538/1000  Loss:  [0.2500536]\n",
      "Epoch:  539/1000  Loss:  [0.25005482]\n",
      "Epoch:  540/1000  Loss:  [0.25005603]\n",
      "Epoch:  541/1000  Loss:  [0.25005724]\n",
      "Epoch:  542/1000  Loss:  [0.25005845]\n",
      "Epoch:  543/1000  Loss:  [0.25005966]\n",
      "Epoch:  544/1000  Loss:  [0.25006086]\n",
      "Epoch:  545/1000  Loss:  [0.25006205]\n",
      "Epoch:  546/1000  Loss:  [0.25006324]\n",
      "Epoch:  547/1000  Loss:  [0.25006443]\n",
      "Epoch:  548/1000  Loss:  [0.25006561]\n",
      "Epoch:  549/1000  Loss:  [0.25006679]\n",
      "Epoch:  550/1000  Loss:  [0.25006797]\n",
      "Epoch:  551/1000  Loss:  [0.25006914]\n",
      "Epoch:  552/1000  Loss:  [0.25007031]\n",
      "Epoch:  553/1000  Loss:  [0.25007147]\n",
      "Epoch:  554/1000  Loss:  [0.25007263]\n",
      "Epoch:  555/1000  Loss:  [0.25007378]\n",
      "Epoch:  556/1000  Loss:  [0.25007493]\n",
      "Epoch:  557/1000  Loss:  [0.25007608]\n",
      "Epoch:  558/1000  Loss:  [0.25007722]\n",
      "Epoch:  559/1000  Loss:  [0.25007836]\n",
      "Epoch:  560/1000  Loss:  [0.25007949]\n",
      "Epoch:  561/1000  Loss:  [0.25008062]\n",
      "Epoch:  562/1000  Loss:  [0.25008174]\n",
      "Epoch:  563/1000  Loss:  [0.25008286]\n",
      "Epoch:  564/1000  Loss:  [0.25008398]\n",
      "Epoch:  565/1000  Loss:  [0.25008509]\n",
      "Epoch:  566/1000  Loss:  [0.2500862]\n",
      "Epoch:  567/1000  Loss:  [0.2500873]\n",
      "Epoch:  568/1000  Loss:  [0.2500884]\n",
      "Epoch:  569/1000  Loss:  [0.25008949]\n",
      "Epoch:  570/1000  Loss:  [0.25009058]\n",
      "Epoch:  571/1000  Loss:  [0.25009167]\n",
      "Epoch:  572/1000  Loss:  [0.25009275]\n",
      "Epoch:  573/1000  Loss:  [0.25009382]\n",
      "Epoch:  574/1000  Loss:  [0.2500949]\n",
      "Epoch:  575/1000  Loss:  [0.25009596]\n",
      "Epoch:  576/1000  Loss:  [0.25009703]\n",
      "Epoch:  577/1000  Loss:  [0.25009808]\n",
      "Epoch:  578/1000  Loss:  [0.25009914]\n",
      "Epoch:  579/1000  Loss:  [0.25010019]\n",
      "Epoch:  580/1000  Loss:  [0.25010123]\n",
      "Epoch:  581/1000  Loss:  [0.25010227]\n",
      "Epoch:  582/1000  Loss:  [0.25010331]\n",
      "Epoch:  583/1000  Loss:  [0.25010434]\n",
      "Epoch:  584/1000  Loss:  [0.25010537]\n",
      "Epoch:  585/1000  Loss:  [0.25010639]\n",
      "Epoch:  586/1000  Loss:  [0.25010741]\n",
      "Epoch:  587/1000  Loss:  [0.25010842]\n",
      "Epoch:  588/1000  Loss:  [0.25010943]\n",
      "Epoch:  589/1000  Loss:  [0.25011043]\n",
      "Epoch:  590/1000  Loss:  [0.25011143]\n",
      "Epoch:  591/1000  Loss:  [0.25011243]\n",
      "Epoch:  592/1000  Loss:  [0.25011342]\n",
      "Epoch:  593/1000  Loss:  [0.2501144]\n",
      "Epoch:  594/1000  Loss:  [0.25011538]\n",
      "Epoch:  595/1000  Loss:  [0.25011636]\n",
      "Epoch:  596/1000  Loss:  [0.25011733]\n",
      "Epoch:  597/1000  Loss:  [0.2501183]\n",
      "Epoch:  598/1000  Loss:  [0.25011926]\n",
      "Epoch:  599/1000  Loss:  [0.25012022]\n",
      "Epoch:  600/1000  Loss:  [0.25012117]\n",
      "Epoch:  601/1000  Loss:  [0.25012212]\n",
      "Epoch:  602/1000  Loss:  [0.25012306]\n",
      "Epoch:  603/1000  Loss:  [0.250124]\n",
      "Epoch:  604/1000  Loss:  [0.25012493]\n",
      "Epoch:  605/1000  Loss:  [0.25012586]\n",
      "Epoch:  606/1000  Loss:  [0.25012679]\n",
      "Epoch:  607/1000  Loss:  [0.25012771]\n",
      "Epoch:  608/1000  Loss:  [0.25012862]\n",
      "Epoch:  609/1000  Loss:  [0.25012953]\n",
      "Epoch:  610/1000  Loss:  [0.25013044]\n",
      "Epoch:  611/1000  Loss:  [0.25013134]\n",
      "Epoch:  612/1000  Loss:  [0.25013223]\n",
      "Epoch:  613/1000  Loss:  [0.25013312]\n",
      "Epoch:  614/1000  Loss:  [0.25013401]\n",
      "Epoch:  615/1000  Loss:  [0.25013489]\n",
      "Epoch:  616/1000  Loss:  [0.25013577]\n",
      "Epoch:  617/1000  Loss:  [0.25013664]\n",
      "Epoch:  618/1000  Loss:  [0.25013751]\n",
      "Epoch:  619/1000  Loss:  [0.25013837]\n",
      "Epoch:  620/1000  Loss:  [0.25013923]\n",
      "Epoch:  621/1000  Loss:  [0.25014008]\n",
      "Epoch:  622/1000  Loss:  [0.25014093]\n",
      "Epoch:  623/1000  Loss:  [0.25014177]\n",
      "Epoch:  624/1000  Loss:  [0.25014261]\n",
      "Epoch:  625/1000  Loss:  [0.25014344]\n",
      "Epoch:  626/1000  Loss:  [0.25014427]\n",
      "Epoch:  627/1000  Loss:  [0.25014509]\n",
      "Epoch:  628/1000  Loss:  [0.25014591]\n",
      "Epoch:  629/1000  Loss:  [0.25014672]\n",
      "Epoch:  630/1000  Loss:  [0.25014753]\n",
      "Epoch:  631/1000  Loss:  [0.25014834]\n",
      "Epoch:  632/1000  Loss:  [0.25014914]\n",
      "Epoch:  633/1000  Loss:  [0.25014993]\n",
      "Epoch:  634/1000  Loss:  [0.25015072]\n",
      "Epoch:  635/1000  Loss:  [0.2501515]\n",
      "Epoch:  636/1000  Loss:  [0.25015228]\n",
      "Epoch:  637/1000  Loss:  [0.25015306]\n",
      "Epoch:  638/1000  Loss:  [0.25015383]\n",
      "Epoch:  639/1000  Loss:  [0.25015459]\n",
      "Epoch:  640/1000  Loss:  [0.25015535]\n",
      "Epoch:  641/1000  Loss:  [0.25015611]\n",
      "Epoch:  642/1000  Loss:  [0.25015686]\n",
      "Epoch:  643/1000  Loss:  [0.2501576]\n",
      "Epoch:  644/1000  Loss:  [0.25015834]\n",
      "Epoch:  645/1000  Loss:  [0.25015908]\n",
      "Epoch:  646/1000  Loss:  [0.25015981]\n",
      "Epoch:  647/1000  Loss:  [0.25016054]\n",
      "Epoch:  648/1000  Loss:  [0.25016126]\n",
      "Epoch:  649/1000  Loss:  [0.25016197]\n",
      "Epoch:  650/1000  Loss:  [0.25016268]\n",
      "Epoch:  651/1000  Loss:  [0.25016339]\n",
      "Epoch:  652/1000  Loss:  [0.25016409]\n",
      "Epoch:  653/1000  Loss:  [0.25016479]\n",
      "Epoch:  654/1000  Loss:  [0.25016548]\n",
      "Epoch:  655/1000  Loss:  [0.25016617]\n",
      "Epoch:  656/1000  Loss:  [0.25016685]\n",
      "Epoch:  657/1000  Loss:  [0.25016752]\n",
      "Epoch:  658/1000  Loss:  [0.2501682]\n",
      "Epoch:  659/1000  Loss:  [0.25016886]\n",
      "Epoch:  660/1000  Loss:  [0.25016953]\n",
      "Epoch:  661/1000  Loss:  [0.25017018]\n",
      "Epoch:  662/1000  Loss:  [0.25017083]\n",
      "Epoch:  663/1000  Loss:  [0.25017148]\n",
      "Epoch:  664/1000  Loss:  [0.25017212]\n",
      "Epoch:  665/1000  Loss:  [0.25017276]\n",
      "Epoch:  666/1000  Loss:  [0.25017339]\n",
      "Epoch:  667/1000  Loss:  [0.25017402]\n",
      "Epoch:  668/1000  Loss:  [0.25017464]\n",
      "Epoch:  669/1000  Loss:  [0.25017526]\n",
      "Epoch:  670/1000  Loss:  [0.25017588]\n",
      "Epoch:  671/1000  Loss:  [0.25017648]\n",
      "Epoch:  672/1000  Loss:  [0.25017709]\n",
      "Epoch:  673/1000  Loss:  [0.25017769]\n",
      "Epoch:  674/1000  Loss:  [0.25017828]\n",
      "Epoch:  675/1000  Loss:  [0.25017887]\n",
      "Epoch:  676/1000  Loss:  [0.25017945]\n",
      "Epoch:  677/1000  Loss:  [0.25018003]\n",
      "Epoch:  678/1000  Loss:  [0.2501806]\n",
      "Epoch:  679/1000  Loss:  [0.25018117]\n",
      "Epoch:  680/1000  Loss:  [0.25018174]\n",
      "Epoch:  681/1000  Loss:  [0.2501823]\n",
      "Epoch:  682/1000  Loss:  [0.25018285]\n",
      "Epoch:  683/1000  Loss:  [0.2501834]\n",
      "Epoch:  684/1000  Loss:  [0.25018395]\n",
      "Epoch:  685/1000  Loss:  [0.25018449]\n",
      "Epoch:  686/1000  Loss:  [0.25018502]\n",
      "Epoch:  687/1000  Loss:  [0.25018555]\n",
      "Epoch:  688/1000  Loss:  [0.25018608]\n",
      "Epoch:  689/1000  Loss:  [0.2501866]\n",
      "Epoch:  690/1000  Loss:  [0.25018711]\n",
      "Epoch:  691/1000  Loss:  [0.25018763]\n",
      "Epoch:  692/1000  Loss:  [0.25018813]\n",
      "Epoch:  693/1000  Loss:  [0.25018863]\n",
      "Epoch:  694/1000  Loss:  [0.25018913]\n",
      "Epoch:  695/1000  Loss:  [0.25018962]\n",
      "Epoch:  696/1000  Loss:  [0.25019011]\n",
      "Epoch:  697/1000  Loss:  [0.25019059]\n",
      "Epoch:  698/1000  Loss:  [0.25019107]\n",
      "Epoch:  699/1000  Loss:  [0.25019154]\n",
      "Epoch:  700/1000  Loss:  [0.25019201]\n",
      "Epoch:  701/1000  Loss:  [0.25019247]\n",
      "Epoch:  702/1000  Loss:  [0.25019293]\n",
      "Epoch:  703/1000  Loss:  [0.25019339]\n",
      "Epoch:  704/1000  Loss:  [0.25019384]\n",
      "Epoch:  705/1000  Loss:  [0.25019428]\n",
      "Epoch:  706/1000  Loss:  [0.25019472]\n",
      "Epoch:  707/1000  Loss:  [0.25019516]\n",
      "Epoch:  708/1000  Loss:  [0.25019559]\n",
      "Epoch:  709/1000  Loss:  [0.25019601]\n",
      "Epoch:  710/1000  Loss:  [0.25019643]\n",
      "Epoch:  711/1000  Loss:  [0.25019685]\n",
      "Epoch:  712/1000  Loss:  [0.25019726]\n",
      "Epoch:  713/1000  Loss:  [0.25019767]\n",
      "Epoch:  714/1000  Loss:  [0.25019807]\n",
      "Epoch:  715/1000  Loss:  [0.25019847]\n",
      "Epoch:  716/1000  Loss:  [0.25019886]\n",
      "Epoch:  717/1000  Loss:  [0.25019925]\n",
      "Epoch:  718/1000  Loss:  [0.25019963]\n",
      "Epoch:  719/1000  Loss:  [0.25020001]\n",
      "Epoch:  720/1000  Loss:  [0.25020039]\n",
      "Epoch:  721/1000  Loss:  [0.25020076]\n",
      "Epoch:  722/1000  Loss:  [0.25020112]\n",
      "Epoch:  723/1000  Loss:  [0.25020148]\n",
      "Epoch:  724/1000  Loss:  [0.25020184]\n",
      "Epoch:  725/1000  Loss:  [0.25020219]\n",
      "Epoch:  726/1000  Loss:  [0.25020254]\n",
      "Epoch:  727/1000  Loss:  [0.25020288]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  728/1000  Loss:  [0.25020322]\n",
      "Epoch:  729/1000  Loss:  [0.25020355]\n",
      "Epoch:  730/1000  Loss:  [0.25020388]\n",
      "Epoch:  731/1000  Loss:  [0.2502042]\n",
      "Epoch:  732/1000  Loss:  [0.25020452]\n",
      "Epoch:  733/1000  Loss:  [0.25020484]\n",
      "Epoch:  734/1000  Loss:  [0.25020515]\n",
      "Epoch:  735/1000  Loss:  [0.25020546]\n",
      "Epoch:  736/1000  Loss:  [0.25020576]\n",
      "Epoch:  737/1000  Loss:  [0.25020606]\n",
      "Epoch:  738/1000  Loss:  [0.25020635]\n",
      "Epoch:  739/1000  Loss:  [0.25020664]\n",
      "Epoch:  740/1000  Loss:  [0.25020692]\n",
      "Epoch:  741/1000  Loss:  [0.2502072]\n",
      "Epoch:  742/1000  Loss:  [0.25020748]\n",
      "Epoch:  743/1000  Loss:  [0.25020775]\n",
      "Epoch:  744/1000  Loss:  [0.25020801]\n",
      "Epoch:  745/1000  Loss:  [0.25020828]\n",
      "Epoch:  746/1000  Loss:  [0.25020853]\n",
      "Epoch:  747/1000  Loss:  [0.25020879]\n",
      "Epoch:  748/1000  Loss:  [0.25020904]\n",
      "Epoch:  749/1000  Loss:  [0.25020928]\n",
      "Epoch:  750/1000  Loss:  [0.25020952]\n",
      "Epoch:  751/1000  Loss:  [0.25020976]\n",
      "Epoch:  752/1000  Loss:  [0.25020999]\n",
      "Epoch:  753/1000  Loss:  [0.25021022]\n",
      "Epoch:  754/1000  Loss:  [0.25021044]\n",
      "Epoch:  755/1000  Loss:  [0.25021066]\n",
      "Epoch:  756/1000  Loss:  [0.25021088]\n",
      "Epoch:  757/1000  Loss:  [0.25021109]\n",
      "Epoch:  758/1000  Loss:  [0.25021129]\n",
      "Epoch:  759/1000  Loss:  [0.2502115]\n",
      "Epoch:  760/1000  Loss:  [0.25021169]\n",
      "Epoch:  761/1000  Loss:  [0.25021189]\n",
      "Epoch:  762/1000  Loss:  [0.25021208]\n",
      "Epoch:  763/1000  Loss:  [0.25021226]\n",
      "Epoch:  764/1000  Loss:  [0.25021244]\n",
      "Epoch:  765/1000  Loss:  [0.25021262]\n",
      "Epoch:  766/1000  Loss:  [0.2502128]\n",
      "Epoch:  767/1000  Loss:  [0.25021296]\n",
      "Epoch:  768/1000  Loss:  [0.25021313]\n",
      "Epoch:  769/1000  Loss:  [0.25021329]\n",
      "Epoch:  770/1000  Loss:  [0.25021345]\n",
      "Epoch:  771/1000  Loss:  [0.2502136]\n",
      "Epoch:  772/1000  Loss:  [0.25021375]\n",
      "Epoch:  773/1000  Loss:  [0.25021389]\n",
      "Epoch:  774/1000  Loss:  [0.25021403]\n",
      "Epoch:  775/1000  Loss:  [0.25021417]\n",
      "Epoch:  776/1000  Loss:  [0.2502143]\n",
      "Epoch:  777/1000  Loss:  [0.25021443]\n",
      "Epoch:  778/1000  Loss:  [0.25021455]\n",
      "Epoch:  779/1000  Loss:  [0.25021467]\n",
      "Epoch:  780/1000  Loss:  [0.25021479]\n",
      "Epoch:  781/1000  Loss:  [0.2502149]\n",
      "Epoch:  782/1000  Loss:  [0.25021501]\n",
      "Epoch:  783/1000  Loss:  [0.25021511]\n",
      "Epoch:  784/1000  Loss:  [0.25021522]\n",
      "Epoch:  785/1000  Loss:  [0.25021531]\n",
      "Epoch:  786/1000  Loss:  [0.2502154]\n",
      "Epoch:  787/1000  Loss:  [0.25021549]\n",
      "Epoch:  788/1000  Loss:  [0.25021558]\n",
      "Epoch:  789/1000  Loss:  [0.25021566]\n",
      "Epoch:  790/1000  Loss:  [0.25021574]\n",
      "Epoch:  791/1000  Loss:  [0.25021581]\n",
      "Epoch:  792/1000  Loss:  [0.25021588]\n",
      "Epoch:  793/1000  Loss:  [0.25021594]\n",
      "Epoch:  794/1000  Loss:  [0.25021601]\n",
      "Epoch:  795/1000  Loss:  [0.25021606]\n",
      "Epoch:  796/1000  Loss:  [0.25021612]\n",
      "Epoch:  797/1000  Loss:  [0.25021617]\n",
      "Epoch:  798/1000  Loss:  [0.25021622]\n",
      "Epoch:  799/1000  Loss:  [0.25021626]\n",
      "Epoch:  800/1000  Loss:  [0.2502163]\n",
      "Epoch:  801/1000  Loss:  [0.25021633]\n",
      "Epoch:  802/1000  Loss:  [0.25021636]\n",
      "Epoch:  803/1000  Loss:  [0.25021639]\n",
      "Epoch:  804/1000  Loss:  [0.25021642]\n",
      "Epoch:  805/1000  Loss:  [0.25021644]\n",
      "Epoch:  806/1000  Loss:  [0.25021645]\n",
      "Epoch:  807/1000  Loss:  [0.25021647]\n",
      "Epoch:  808/1000  Loss:  [0.25021647]\n",
      "Epoch:  809/1000  Loss:  [0.25021648]\n",
      "Epoch:  810/1000  Loss:  [0.25021648]\n",
      "Epoch:  811/1000  Loss:  [0.25021648]\n",
      "Epoch:  812/1000  Loss:  [0.25021647]\n",
      "Epoch:  813/1000  Loss:  [0.25021647]\n",
      "Epoch:  814/1000  Loss:  [0.25021645]\n",
      "Epoch:  815/1000  Loss:  [0.25021644]\n",
      "Epoch:  816/1000  Loss:  [0.25021642]\n",
      "Epoch:  817/1000  Loss:  [0.25021639]\n",
      "Epoch:  818/1000  Loss:  [0.25021637]\n",
      "Epoch:  819/1000  Loss:  [0.25021634]\n",
      "Epoch:  820/1000  Loss:  [0.2502163]\n",
      "Epoch:  821/1000  Loss:  [0.25021626]\n",
      "Epoch:  822/1000  Loss:  [0.25021622]\n",
      "Epoch:  823/1000  Loss:  [0.25021618]\n",
      "Epoch:  824/1000  Loss:  [0.25021613]\n",
      "Epoch:  825/1000  Loss:  [0.25021608]\n",
      "Epoch:  826/1000  Loss:  [0.25021602]\n",
      "Epoch:  827/1000  Loss:  [0.25021596]\n",
      "Epoch:  828/1000  Loss:  [0.2502159]\n",
      "Epoch:  829/1000  Loss:  [0.25021583]\n",
      "Epoch:  830/1000  Loss:  [0.25021576]\n",
      "Epoch:  831/1000  Loss:  [0.25021569]\n",
      "Epoch:  832/1000  Loss:  [0.25021561]\n",
      "Epoch:  833/1000  Loss:  [0.25021553]\n",
      "Epoch:  834/1000  Loss:  [0.25021545]\n",
      "Epoch:  835/1000  Loss:  [0.25021536]\n",
      "Epoch:  836/1000  Loss:  [0.25021527]\n",
      "Epoch:  837/1000  Loss:  [0.25021518]\n",
      "Epoch:  838/1000  Loss:  [0.25021508]\n",
      "Epoch:  839/1000  Loss:  [0.25021498]\n",
      "Epoch:  840/1000  Loss:  [0.25021488]\n",
      "Epoch:  841/1000  Loss:  [0.25021477]\n",
      "Epoch:  842/1000  Loss:  [0.25021466]\n",
      "Epoch:  843/1000  Loss:  [0.25021454]\n",
      "Epoch:  844/1000  Loss:  [0.25021442]\n",
      "Epoch:  845/1000  Loss:  [0.2502143]\n",
      "Epoch:  846/1000  Loss:  [0.25021418]\n",
      "Epoch:  847/1000  Loss:  [0.25021405]\n",
      "Epoch:  848/1000  Loss:  [0.25021392]\n",
      "Epoch:  849/1000  Loss:  [0.25021378]\n",
      "Epoch:  850/1000  Loss:  [0.25021364]\n",
      "Epoch:  851/1000  Loss:  [0.2502135]\n",
      "Epoch:  852/1000  Loss:  [0.25021336]\n",
      "Epoch:  853/1000  Loss:  [0.25021321]\n",
      "Epoch:  854/1000  Loss:  [0.25021305]\n",
      "Epoch:  855/1000  Loss:  [0.2502129]\n",
      "Epoch:  856/1000  Loss:  [0.25021274]\n",
      "Epoch:  857/1000  Loss:  [0.25021258]\n",
      "Epoch:  858/1000  Loss:  [0.25021241]\n",
      "Epoch:  859/1000  Loss:  [0.25021224]\n",
      "Epoch:  860/1000  Loss:  [0.25021207]\n",
      "Epoch:  861/1000  Loss:  [0.25021189]\n",
      "Epoch:  862/1000  Loss:  [0.25021171]\n",
      "Epoch:  863/1000  Loss:  [0.25021153]\n",
      "Epoch:  864/1000  Loss:  [0.25021135]\n",
      "Epoch:  865/1000  Loss:  [0.25021116]\n",
      "Epoch:  866/1000  Loss:  [0.25021096]\n",
      "Epoch:  867/1000  Loss:  [0.25021077]\n",
      "Epoch:  868/1000  Loss:  [0.25021057]\n",
      "Epoch:  869/1000  Loss:  [0.25021036]\n",
      "Epoch:  870/1000  Loss:  [0.25021016]\n",
      "Epoch:  871/1000  Loss:  [0.25020995]\n",
      "Epoch:  872/1000  Loss:  [0.25020973]\n",
      "Epoch:  873/1000  Loss:  [0.25020952]\n",
      "Epoch:  874/1000  Loss:  [0.2502093]\n",
      "Epoch:  875/1000  Loss:  [0.25020907]\n",
      "Epoch:  876/1000  Loss:  [0.25020884]\n",
      "Epoch:  877/1000  Loss:  [0.25020861]\n",
      "Epoch:  878/1000  Loss:  [0.25020838]\n",
      "Epoch:  879/1000  Loss:  [0.25020814]\n",
      "Epoch:  880/1000  Loss:  [0.2502079]\n",
      "Epoch:  881/1000  Loss:  [0.25020766]\n",
      "Epoch:  882/1000  Loss:  [0.25020741]\n",
      "Epoch:  883/1000  Loss:  [0.25020716]\n",
      "Epoch:  884/1000  Loss:  [0.2502069]\n",
      "Epoch:  885/1000  Loss:  [0.25020664]\n",
      "Epoch:  886/1000  Loss:  [0.25020638]\n",
      "Epoch:  887/1000  Loss:  [0.25020612]\n",
      "Epoch:  888/1000  Loss:  [0.25020585]\n",
      "Epoch:  889/1000  Loss:  [0.25020557]\n",
      "Epoch:  890/1000  Loss:  [0.2502053]\n",
      "Epoch:  891/1000  Loss:  [0.25020502]\n",
      "Epoch:  892/1000  Loss:  [0.25020474]\n",
      "Epoch:  893/1000  Loss:  [0.25020445]\n",
      "Epoch:  894/1000  Loss:  [0.25020416]\n",
      "Epoch:  895/1000  Loss:  [0.25020386]\n",
      "Epoch:  896/1000  Loss:  [0.25020357]\n",
      "Epoch:  897/1000  Loss:  [0.25020327]\n",
      "Epoch:  898/1000  Loss:  [0.25020296]\n",
      "Epoch:  899/1000  Loss:  [0.25020265]\n",
      "Epoch:  900/1000  Loss:  [0.25020234]\n",
      "Epoch:  901/1000  Loss:  [0.25020202]\n",
      "Epoch:  902/1000  Loss:  [0.2502017]\n",
      "Epoch:  903/1000  Loss:  [0.25020138]\n",
      "Epoch:  904/1000  Loss:  [0.25020105]\n",
      "Epoch:  905/1000  Loss:  [0.25020072]\n",
      "Epoch:  906/1000  Loss:  [0.25020039]\n",
      "Epoch:  907/1000  Loss:  [0.25020005]\n",
      "Epoch:  908/1000  Loss:  [0.25019971]\n",
      "Epoch:  909/1000  Loss:  [0.25019936]\n",
      "Epoch:  910/1000  Loss:  [0.25019901]\n",
      "Epoch:  911/1000  Loss:  [0.25019866]\n",
      "Epoch:  912/1000  Loss:  [0.2501983]\n",
      "Epoch:  913/1000  Loss:  [0.25019794]\n",
      "Epoch:  914/1000  Loss:  [0.25019757]\n",
      "Epoch:  915/1000  Loss:  [0.2501972]\n",
      "Epoch:  916/1000  Loss:  [0.25019683]\n",
      "Epoch:  917/1000  Loss:  [0.25019645]\n",
      "Epoch:  918/1000  Loss:  [0.25019607]\n",
      "Epoch:  919/1000  Loss:  [0.25019569]\n",
      "Epoch:  920/1000  Loss:  [0.2501953]\n",
      "Epoch:  921/1000  Loss:  [0.2501949]\n",
      "Epoch:  922/1000  Loss:  [0.25019451]\n",
      "Epoch:  923/1000  Loss:  [0.2501941]\n",
      "Epoch:  924/1000  Loss:  [0.2501937]\n",
      "Epoch:  925/1000  Loss:  [0.25019329]\n",
      "Epoch:  926/1000  Loss:  [0.25019287]\n",
      "Epoch:  927/1000  Loss:  [0.25019246]\n",
      "Epoch:  928/1000  Loss:  [0.25019203]\n",
      "Epoch:  929/1000  Loss:  [0.2501916]\n",
      "Epoch:  930/1000  Loss:  [0.25019117]\n",
      "Epoch:  931/1000  Loss:  [0.25019074]\n",
      "Epoch:  932/1000  Loss:  [0.2501903]\n",
      "Epoch:  933/1000  Loss:  [0.25018985]\n",
      "Epoch:  934/1000  Loss:  [0.2501894]\n",
      "Epoch:  935/1000  Loss:  [0.25018895]\n",
      "Epoch:  936/1000  Loss:  [0.25018849]\n",
      "Epoch:  937/1000  Loss:  [0.25018803]\n",
      "Epoch:  938/1000  Loss:  [0.25018756]\n",
      "Epoch:  939/1000  Loss:  [0.25018709]\n",
      "Epoch:  940/1000  Loss:  [0.25018661]\n",
      "Epoch:  941/1000  Loss:  [0.25018613]\n",
      "Epoch:  942/1000  Loss:  [0.25018565]\n",
      "Epoch:  943/1000  Loss:  [0.25018515]\n",
      "Epoch:  944/1000  Loss:  [0.25018466]\n",
      "Epoch:  945/1000  Loss:  [0.25018416]\n",
      "Epoch:  946/1000  Loss:  [0.25018365]\n",
      "Epoch:  947/1000  Loss:  [0.25018314]\n",
      "Epoch:  948/1000  Loss:  [0.25018263]\n",
      "Epoch:  949/1000  Loss:  [0.25018211]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  950/1000  Loss:  [0.25018158]\n",
      "Epoch:  951/1000  Loss:  [0.25018105]\n",
      "Epoch:  952/1000  Loss:  [0.25018051]\n",
      "Epoch:  953/1000  Loss:  [0.25017997]\n",
      "Epoch:  954/1000  Loss:  [0.25017943]\n",
      "Epoch:  955/1000  Loss:  [0.25017888]\n",
      "Epoch:  956/1000  Loss:  [0.25017832]\n",
      "Epoch:  957/1000  Loss:  [0.25017776]\n",
      "Epoch:  958/1000  Loss:  [0.25017719]\n",
      "Epoch:  959/1000  Loss:  [0.25017662]\n",
      "Epoch:  960/1000  Loss:  [0.25017604]\n",
      "Epoch:  961/1000  Loss:  [0.25017545]\n",
      "Epoch:  962/1000  Loss:  [0.25017486]\n",
      "Epoch:  963/1000  Loss:  [0.25017427]\n",
      "Epoch:  964/1000  Loss:  [0.25017367]\n",
      "Epoch:  965/1000  Loss:  [0.25017306]\n",
      "Epoch:  966/1000  Loss:  [0.25017245]\n",
      "Epoch:  967/1000  Loss:  [0.25017183]\n",
      "Epoch:  968/1000  Loss:  [0.2501712]\n",
      "Epoch:  969/1000  Loss:  [0.25017057]\n",
      "Epoch:  970/1000  Loss:  [0.25016994]\n",
      "Epoch:  971/1000  Loss:  [0.25016929]\n",
      "Epoch:  972/1000  Loss:  [0.25016865]\n",
      "Epoch:  973/1000  Loss:  [0.25016799]\n",
      "Epoch:  974/1000  Loss:  [0.25016733]\n",
      "Epoch:  975/1000  Loss:  [0.25016666]\n",
      "Epoch:  976/1000  Loss:  [0.25016599]\n",
      "Epoch:  977/1000  Loss:  [0.25016531]\n",
      "Epoch:  978/1000  Loss:  [0.25016462]\n",
      "Epoch:  979/1000  Loss:  [0.25016392]\n",
      "Epoch:  980/1000  Loss:  [0.25016322]\n",
      "Epoch:  981/1000  Loss:  [0.25016252]\n",
      "Epoch:  982/1000  Loss:  [0.2501618]\n",
      "Epoch:  983/1000  Loss:  [0.25016108]\n",
      "Epoch:  984/1000  Loss:  [0.25016035]\n",
      "Epoch:  985/1000  Loss:  [0.25015962]\n",
      "Epoch:  986/1000  Loss:  [0.25015887]\n",
      "Epoch:  987/1000  Loss:  [0.25015812]\n",
      "Epoch:  988/1000  Loss:  [0.25015737]\n",
      "Epoch:  989/1000  Loss:  [0.2501566]\n",
      "Epoch:  990/1000  Loss:  [0.25015583]\n",
      "Epoch:  991/1000  Loss:  [0.25015505]\n",
      "Epoch:  992/1000  Loss:  [0.25015427]\n",
      "Epoch:  993/1000  Loss:  [0.25015347]\n",
      "Epoch:  994/1000  Loss:  [0.25015267]\n",
      "Epoch:  995/1000  Loss:  [0.25015186]\n",
      "Epoch:  996/1000  Loss:  [0.25015104]\n",
      "Epoch:  997/1000  Loss:  [0.25015022]\n",
      "Epoch:  998/1000  Loss:  [0.25014939]\n",
      "Epoch:  999/1000  Loss:  [0.25014854]\n",
      "Epoch:  1000/1000  Loss:  [0.25014769]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 1000 # number of itrations\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4689421 ]\n",
      " [0.49667954]\n",
      " [0.51204509]\n",
      " [0.45334759]]\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### experiment with 6 hidden nodes and N = 1000\n",
    "This gives good performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "x_train, y_train = generate_trainset(N)\n",
    "hidden_dim = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/300  Loss:  [0.25274302]\n",
      "Epoch:  2/300  Loss:  [0.2499302]\n",
      "Epoch:  3/300  Loss:  [0.24960193]\n",
      "Epoch:  4/300  Loss:  [0.24949844]\n",
      "Epoch:  5/300  Loss:  [0.24940887]\n",
      "Epoch:  6/300  Loss:  [0.24931951]\n",
      "Epoch:  7/300  Loss:  [0.24922614]\n",
      "Epoch:  8/300  Loss:  [0.24912852]\n",
      "Epoch:  9/300  Loss:  [0.24902584]\n",
      "Epoch:  10/300  Loss:  [0.24891744]\n",
      "Epoch:  11/300  Loss:  [0.24880256]\n",
      "Epoch:  12/300  Loss:  [0.24868039]\n",
      "Epoch:  13/300  Loss:  [0.24855007]\n",
      "Epoch:  14/300  Loss:  [0.24841063]\n",
      "Epoch:  15/300  Loss:  [0.24826103]\n",
      "Epoch:  16/300  Loss:  [0.24810012]\n",
      "Epoch:  17/300  Loss:  [0.24792667]\n",
      "Epoch:  18/300  Loss:  [0.24773931]\n",
      "Epoch:  19/300  Loss:  [0.24753655]\n",
      "Epoch:  20/300  Loss:  [0.24731676]\n",
      "Epoch:  21/300  Loss:  [0.24707817]\n",
      "Epoch:  22/300  Loss:  [0.24681884]\n",
      "Epoch:  23/300  Loss:  [0.2465367]\n",
      "Epoch:  24/300  Loss:  [0.24622946]\n",
      "Epoch:  25/300  Loss:  [0.2458947]\n",
      "Epoch:  26/300  Loss:  [0.2455298]\n",
      "Epoch:  27/300  Loss:  [0.245132]\n",
      "Epoch:  28/300  Loss:  [0.24469837]\n",
      "Epoch:  29/300  Loss:  [0.24422583]\n",
      "Epoch:  30/300  Loss:  [0.24371121]\n",
      "Epoch:  31/300  Loss:  [0.24315128]\n",
      "Epoch:  32/300  Loss:  [0.24254275]\n",
      "Epoch:  33/300  Loss:  [0.24188238]\n",
      "Epoch:  34/300  Loss:  [0.24116705]\n",
      "Epoch:  35/300  Loss:  [0.24039378]\n",
      "Epoch:  36/300  Loss:  [0.23955989]\n",
      "Epoch:  37/300  Loss:  [0.23866307]\n",
      "Epoch:  38/300  Loss:  [0.23770144]\n",
      "Epoch:  39/300  Loss:  [0.23667369]\n",
      "Epoch:  40/300  Loss:  [0.23557915]\n",
      "Epoch:  41/300  Loss:  [0.23441788]\n",
      "Epoch:  42/300  Loss:  [0.23319067]\n",
      "Epoch:  43/300  Loss:  [0.23189915]\n",
      "Epoch:  44/300  Loss:  [0.23054572]\n",
      "Epoch:  45/300  Loss:  [0.22913359]\n",
      "Epoch:  46/300  Loss:  [0.22766665]\n",
      "Epoch:  47/300  Loss:  [0.22614945]\n",
      "Epoch:  48/300  Loss:  [0.22458709]\n",
      "Epoch:  49/300  Loss:  [0.22298502]\n",
      "Epoch:  50/300  Loss:  [0.22134903]\n",
      "Epoch:  51/300  Loss:  [0.21968498]\n",
      "Epoch:  52/300  Loss:  [0.21799877]\n",
      "Epoch:  53/300  Loss:  [0.21629616]\n",
      "Epoch:  54/300  Loss:  [0.21458266]\n",
      "Epoch:  55/300  Loss:  [0.21286347]\n",
      "Epoch:  56/300  Loss:  [0.21114339]\n",
      "Epoch:  57/300  Loss:  [0.20942674]\n",
      "Epoch:  58/300  Loss:  [0.20771739]\n",
      "Epoch:  59/300  Loss:  [0.2060187]\n",
      "Epoch:  60/300  Loss:  [0.20433354]\n",
      "Epoch:  61/300  Loss:  [0.20266432]\n",
      "Epoch:  62/300  Loss:  [0.20101297]\n",
      "Epoch:  63/300  Loss:  [0.19938103]\n",
      "Epoch:  64/300  Loss:  [0.19776966]\n",
      "Epoch:  65/300  Loss:  [0.19617966]\n",
      "Epoch:  66/300  Loss:  [0.19461157]\n",
      "Epoch:  67/300  Loss:  [0.19306563]\n",
      "Epoch:  68/300  Loss:  [0.19154187]\n",
      "Epoch:  69/300  Loss:  [0.19004013]\n",
      "Epoch:  70/300  Loss:  [0.18856008]\n",
      "Epoch:  71/300  Loss:  [0.18710124]\n",
      "Epoch:  72/300  Loss:  [0.18566301]\n",
      "Epoch:  73/300  Loss:  [0.1842447]\n",
      "Epoch:  74/300  Loss:  [0.1828455]\n",
      "Epoch:  75/300  Loss:  [0.18146452]\n",
      "Epoch:  76/300  Loss:  [0.18010082]\n",
      "Epoch:  77/300  Loss:  [0.17875334]\n",
      "Epoch:  78/300  Loss:  [0.17742099]\n",
      "Epoch:  79/300  Loss:  [0.17610258]\n",
      "Epoch:  80/300  Loss:  [0.17479688]\n",
      "Epoch:  81/300  Loss:  [0.17350256]\n",
      "Epoch:  82/300  Loss:  [0.17221825]\n",
      "Epoch:  83/300  Loss:  [0.17094248]\n",
      "Epoch:  84/300  Loss:  [0.16967373]\n",
      "Epoch:  85/300  Loss:  [0.16841042]\n",
      "Epoch:  86/300  Loss:  [0.16715088]\n",
      "Epoch:  87/300  Loss:  [0.16589342]\n",
      "Epoch:  88/300  Loss:  [0.16463627]\n",
      "Epoch:  89/300  Loss:  [0.16337765]\n",
      "Epoch:  90/300  Loss:  [0.16211573]\n",
      "Epoch:  91/300  Loss:  [0.16084871]\n",
      "Epoch:  92/300  Loss:  [0.15957483]\n",
      "Epoch:  93/300  Loss:  [0.15829237]\n",
      "Epoch:  94/300  Loss:  [0.15699974]\n",
      "Epoch:  95/300  Loss:  [0.1556955]\n",
      "Epoch:  96/300  Loss:  [0.15437844]\n",
      "Epoch:  97/300  Loss:  [0.15304759]\n",
      "Epoch:  98/300  Loss:  [0.15170233]\n",
      "Epoch:  99/300  Loss:  [0.15034239]\n",
      "Epoch:  100/300  Loss:  [0.14896796]\n",
      "Epoch:  101/300  Loss:  [0.14757965]\n",
      "Epoch:  102/300  Loss:  [0.14617858]\n",
      "Epoch:  103/300  Loss:  [0.14476629]\n",
      "Epoch:  104/300  Loss:  [0.14334478]\n",
      "Epoch:  105/300  Loss:  [0.1419164]\n",
      "Epoch:  106/300  Loss:  [0.14048378]\n",
      "Epoch:  107/300  Loss:  [0.13904975]\n",
      "Epoch:  108/300  Loss:  [0.13761721]\n",
      "Epoch:  109/300  Loss:  [0.13618903]\n",
      "Epoch:  110/300  Loss:  [0.13476791]\n",
      "Epoch:  111/300  Loss:  [0.13335636]\n",
      "Epoch:  112/300  Loss:  [0.13195657]\n",
      "Epoch:  113/300  Loss:  [0.13057041]\n",
      "Epoch:  114/300  Loss:  [0.12919935]\n",
      "Epoch:  115/300  Loss:  [0.12784452]\n",
      "Epoch:  116/300  Loss:  [0.1265067]\n",
      "Epoch:  117/300  Loss:  [0.12518634]\n",
      "Epoch:  118/300  Loss:  [0.12388362]\n",
      "Epoch:  119/300  Loss:  [0.12259846]\n",
      "Epoch:  120/300  Loss:  [0.1213306]\n",
      "Epoch:  121/300  Loss:  [0.12007959]\n",
      "Epoch:  122/300  Loss:  [0.11884488]\n",
      "Epoch:  123/300  Loss:  [0.11762583]\n",
      "Epoch:  124/300  Loss:  [0.11642172]\n",
      "Epoch:  125/300  Loss:  [0.11523178]\n",
      "Epoch:  126/300  Loss:  [0.11405524]\n",
      "Epoch:  127/300  Loss:  [0.11289128]\n",
      "Epoch:  128/300  Loss:  [0.1117391]\n",
      "Epoch:  129/300  Loss:  [0.11059791]\n",
      "Epoch:  130/300  Loss:  [0.1094669]\n",
      "Epoch:  131/300  Loss:  [0.1083453]\n",
      "Epoch:  132/300  Loss:  [0.10723233]\n",
      "Epoch:  133/300  Loss:  [0.10612724]\n",
      "Epoch:  134/300  Loss:  [0.10502929]\n",
      "Epoch:  135/300  Loss:  [0.10393775]\n",
      "Epoch:  136/300  Loss:  [0.10285189]\n",
      "Epoch:  137/300  Loss:  [0.10177103]\n",
      "Epoch:  138/300  Loss:  [0.10069445]\n",
      "Epoch:  139/300  Loss:  [0.09962149]\n",
      "Epoch:  140/300  Loss:  [0.09855146]\n",
      "Epoch:  141/300  Loss:  [0.09748371]\n",
      "Epoch:  142/300  Loss:  [0.09641758]\n",
      "Epoch:  143/300  Loss:  [0.09535242]\n",
      "Epoch:  144/300  Loss:  [0.09428762]\n",
      "Epoch:  145/300  Loss:  [0.09322255]\n",
      "Epoch:  146/300  Loss:  [0.09215662]\n",
      "Epoch:  147/300  Loss:  [0.09108924]\n",
      "Epoch:  148/300  Loss:  [0.09001986]\n",
      "Epoch:  149/300  Loss:  [0.08894794]\n",
      "Epoch:  150/300  Loss:  [0.08787298]\n",
      "Epoch:  151/300  Loss:  [0.08679452]\n",
      "Epoch:  152/300  Loss:  [0.0857121]\n",
      "Epoch:  153/300  Loss:  [0.08462537]\n",
      "Epoch:  154/300  Loss:  [0.08353397]\n",
      "Epoch:  155/300  Loss:  [0.08243762]\n",
      "Epoch:  156/300  Loss:  [0.08133611]\n",
      "Epoch:  157/300  Loss:  [0.08022929]\n",
      "Epoch:  158/300  Loss:  [0.07911708]\n",
      "Epoch:  159/300  Loss:  [0.07799948]\n",
      "Epoch:  160/300  Loss:  [0.07687656]\n",
      "Epoch:  161/300  Loss:  [0.07574851]\n",
      "Epoch:  162/300  Loss:  [0.07461559]\n",
      "Epoch:  163/300  Loss:  [0.07347815]\n",
      "Epoch:  164/300  Loss:  [0.07233664]\n",
      "Epoch:  165/300  Loss:  [0.0711916]\n",
      "Epoch:  166/300  Loss:  [0.07004368]\n",
      "Epoch:  167/300  Loss:  [0.0688936]\n",
      "Epoch:  168/300  Loss:  [0.06774218]\n",
      "Epoch:  169/300  Loss:  [0.06659031]\n",
      "Epoch:  170/300  Loss:  [0.06543895]\n",
      "Epoch:  171/300  Loss:  [0.06428914]\n",
      "Epoch:  172/300  Loss:  [0.06314197]\n",
      "Epoch:  173/300  Loss:  [0.06199856]\n",
      "Epoch:  174/300  Loss:  [0.06086007]\n",
      "Epoch:  175/300  Loss:  [0.05972768]\n",
      "Epoch:  176/300  Loss:  [0.05860258]\n",
      "Epoch:  177/300  Loss:  [0.05748595]\n",
      "Epoch:  178/300  Loss:  [0.05637895]\n",
      "Epoch:  179/300  Loss:  [0.05528272]\n",
      "Epoch:  180/300  Loss:  [0.05419836]\n",
      "Epoch:  181/300  Loss:  [0.0531269]\n",
      "Epoch:  182/300  Loss:  [0.05206933]\n",
      "Epoch:  183/300  Loss:  [0.05102657]\n",
      "Epoch:  184/300  Loss:  [0.04999947]\n",
      "Epoch:  185/300  Loss:  [0.0489888]\n",
      "Epoch:  186/300  Loss:  [0.04799525]\n",
      "Epoch:  187/300  Loss:  [0.04701943]\n",
      "Epoch:  188/300  Loss:  [0.04606185]\n",
      "Epoch:  189/300  Loss:  [0.04512298]\n",
      "Epoch:  190/300  Loss:  [0.04420315]\n",
      "Epoch:  191/300  Loss:  [0.04330266]\n",
      "Epoch:  192/300  Loss:  [0.04242172]\n",
      "Epoch:  193/300  Loss:  [0.04156045]\n",
      "Epoch:  194/300  Loss:  [0.04071893]\n",
      "Epoch:  195/300  Loss:  [0.03989716]\n",
      "Epoch:  196/300  Loss:  [0.03909508]\n",
      "Epoch:  197/300  Loss:  [0.03831259]\n",
      "Epoch:  198/300  Loss:  [0.03754954]\n",
      "Epoch:  199/300  Loss:  [0.03680572]\n",
      "Epoch:  200/300  Loss:  [0.03608089]\n",
      "Epoch:  201/300  Loss:  [0.03537479]\n",
      "Epoch:  202/300  Loss:  [0.03468711]\n",
      "Epoch:  203/300  Loss:  [0.03401753]\n",
      "Epoch:  204/300  Loss:  [0.0333657]\n",
      "Epoch:  205/300  Loss:  [0.03273125]\n",
      "Epoch:  206/300  Loss:  [0.03211382]\n",
      "Epoch:  207/300  Loss:  [0.031513]\n",
      "Epoch:  208/300  Loss:  [0.0309284]\n",
      "Epoch:  209/300  Loss:  [0.03035962]\n",
      "Epoch:  210/300  Loss:  [0.02980625]\n",
      "Epoch:  211/300  Loss:  [0.02926788]\n",
      "Epoch:  212/300  Loss:  [0.0287441]\n",
      "Epoch:  213/300  Loss:  [0.02823452]\n",
      "Epoch:  214/300  Loss:  [0.02773871]\n",
      "Epoch:  215/300  Loss:  [0.0272563]\n",
      "Epoch:  216/300  Loss:  [0.02678688]\n",
      "Epoch:  217/300  Loss:  [0.02633007]\n",
      "Epoch:  218/300  Loss:  [0.02588549]\n",
      "Epoch:  219/300  Loss:  [0.02545277]\n",
      "Epoch:  220/300  Loss:  [0.02503154]\n",
      "Epoch:  221/300  Loss:  [0.02462146]\n",
      "Epoch:  222/300  Loss:  [0.02422217]\n",
      "Epoch:  223/300  Loss:  [0.02383335]\n",
      "Epoch:  224/300  Loss:  [0.02345465]\n",
      "Epoch:  225/300  Loss:  [0.02308577]\n",
      "Epoch:  226/300  Loss:  [0.02272639]\n",
      "Epoch:  227/300  Loss:  [0.02237622]\n",
      "Epoch:  228/300  Loss:  [0.02203497]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  229/300  Loss:  [0.02170234]\n",
      "Epoch:  230/300  Loss:  [0.02137808]\n",
      "Epoch:  231/300  Loss:  [0.02106191]\n",
      "Epoch:  232/300  Loss:  [0.02075359]\n",
      "Epoch:  233/300  Loss:  [0.02045286]\n",
      "Epoch:  234/300  Loss:  [0.02015948]\n",
      "Epoch:  235/300  Loss:  [0.01987323]\n",
      "Epoch:  236/300  Loss:  [0.01959389]\n",
      "Epoch:  237/300  Loss:  [0.01932123]\n",
      "Epoch:  238/300  Loss:  [0.01905505]\n",
      "Epoch:  239/300  Loss:  [0.01879515]\n",
      "Epoch:  240/300  Loss:  [0.01854133]\n",
      "Epoch:  241/300  Loss:  [0.01829342]\n",
      "Epoch:  242/300  Loss:  [0.01805122]\n",
      "Epoch:  243/300  Loss:  [0.01781457]\n",
      "Epoch:  244/300  Loss:  [0.01758329]\n",
      "Epoch:  245/300  Loss:  [0.01735723]\n",
      "Epoch:  246/300  Loss:  [0.01713623]\n",
      "Epoch:  247/300  Loss:  [0.01692013]\n",
      "Epoch:  248/300  Loss:  [0.01670879]\n",
      "Epoch:  249/300  Loss:  [0.01650208]\n",
      "Epoch:  250/300  Loss:  [0.01629985]\n",
      "Epoch:  251/300  Loss:  [0.01610197]\n",
      "Epoch:  252/300  Loss:  [0.01590832]\n",
      "Epoch:  253/300  Loss:  [0.01571877]\n",
      "Epoch:  254/300  Loss:  [0.01553321]\n",
      "Epoch:  255/300  Loss:  [0.01535153]\n",
      "Epoch:  256/300  Loss:  [0.01517361]\n",
      "Epoch:  257/300  Loss:  [0.01499934]\n",
      "Epoch:  258/300  Loss:  [0.01482863]\n",
      "Epoch:  259/300  Loss:  [0.01466138]\n",
      "Epoch:  260/300  Loss:  [0.01449748]\n",
      "Epoch:  261/300  Loss:  [0.01433685]\n",
      "Epoch:  262/300  Loss:  [0.0141794]\n",
      "Epoch:  263/300  Loss:  [0.01402504]\n",
      "Epoch:  264/300  Loss:  [0.01387369]\n",
      "Epoch:  265/300  Loss:  [0.01372526]\n",
      "Epoch:  266/300  Loss:  [0.01357969]\n",
      "Epoch:  267/300  Loss:  [0.01343688]\n",
      "Epoch:  268/300  Loss:  [0.01329678]\n",
      "Epoch:  269/300  Loss:  [0.01315931]\n",
      "Epoch:  270/300  Loss:  [0.01302439]\n",
      "Epoch:  271/300  Loss:  [0.01289197]\n",
      "Epoch:  272/300  Loss:  [0.01276198]\n",
      "Epoch:  273/300  Loss:  [0.01263436]\n",
      "Epoch:  274/300  Loss:  [0.01250905]\n",
      "Epoch:  275/300  Loss:  [0.01238599]\n",
      "Epoch:  276/300  Loss:  [0.01226512]\n",
      "Epoch:  277/300  Loss:  [0.01214639]\n",
      "Epoch:  278/300  Loss:  [0.01202975]\n",
      "Epoch:  279/300  Loss:  [0.01191514]\n",
      "Epoch:  280/300  Loss:  [0.01180252]\n",
      "Epoch:  281/300  Loss:  [0.01169183]\n",
      "Epoch:  282/300  Loss:  [0.01158304]\n",
      "Epoch:  283/300  Loss:  [0.01147609]\n",
      "Epoch:  284/300  Loss:  [0.01137094]\n",
      "Epoch:  285/300  Loss:  [0.01126755]\n",
      "Epoch:  286/300  Loss:  [0.01116587]\n",
      "Epoch:  287/300  Loss:  [0.01106587]\n",
      "Epoch:  288/300  Loss:  [0.01096752]\n",
      "Epoch:  289/300  Loss:  [0.01087076]\n",
      "Epoch:  290/300  Loss:  [0.01077556]\n",
      "Epoch:  291/300  Loss:  [0.01068189]\n",
      "Epoch:  292/300  Loss:  [0.01058972]\n",
      "Epoch:  293/300  Loss:  [0.010499]\n",
      "Epoch:  294/300  Loss:  [0.01040971]\n",
      "Epoch:  295/300  Loss:  [0.01032182]\n",
      "Epoch:  296/300  Loss:  [0.01023529]\n",
      "Epoch:  297/300  Loss:  [0.01015009]\n",
      "Epoch:  298/300  Loss:  [0.0100662]\n",
      "Epoch:  299/300  Loss:  [0.00998359]\n",
      "Epoch:  300/300  Loss:  [0.00990222]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 300 # number of itrations\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09557819]\n",
      " [0.10251848]\n",
      " [0.9003078 ]\n",
      " [0.90251733]]\n",
      "[[0]\n",
      " [0]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass NN(object):\\n    def __init__(self, hidden_dim=2, learn_rate=0.01):\\n        self.learn_rate = learn_rate\\n        self.input_dim = 2\\n        self.hidden_dim = hidden_dim\\n        self.output_dim = 1\\n        # initialize all weights and biases to zeros\\n#         self.A = np.zeros((self.hidden_dim, self.input_dim))  # input to hidden layer weights\\n#         self.B = np.zeros(self.hidden_dim)                     # hidden layer to output weights \\n#         self.a0 = np.zeros(hidden_dim) # input to hidden layer bias\\n#         self.b0 = 0 # hidden layer to output bias\\n        self.A = np.random.normal(0, 1, (self.hidden_dim, self.input_dim))\\n        self.B = np.random.normal(0, 1, self.hidden_dim)\\n        self.a0 = np.random.normal(0, 1, self.hidden_dim)\\n        self.b0 = np.random.normal(0, 1, 1)\\n        \\n    def sigmoid(self, t):\\n        return 1/(1  + np.exp(-t))\\n    \\n    def dsigmoid(self, t):\\n        return self.sigmoid(t)*(1 - self.sigmoid(t))\\n    \\n    def hidden_layer(self, x):\\n        self.z = self.sigmoid(np.dot(self.A, x) + self.a0) \\n        return self.z\\n    \\n    def forward_pass(self, x):\\n        self.y_hat = self.sigmoid(np.dot(self.B, self.hidden_layer(x)) + self.b0)\\n        return self.y_hat\\n    \\n    def back_propogate(self, X, Y, Y_hat):\\n        SSE_a, SSE_b   = np.zeros_like(self.A), np.zeros_like(self.B)\\n        z_bias, y_bias = np.zeros_like(self.a0), np.zeros_like(self.b0)\\n        for i, x in enumerate(X):\\n            z = self.hidden_layer(x)\\n            y_error = Y[i] - Y_hat[i]\\n            y_delta = -2* y_error * self.dsigmoid(np.dot(self.B, z) + self.b0)\\n            s = self.dsigmoid(np.dot(self.A,x) + self.a0) * self.B * y_delta\\n            SSE_b += y_delta*z\\n            SSE_a += np.tensordot(x,s, axes=0)\\n            y_bias += y_delta\\n            z_bias += s\\n        # update the weights and biases\\n        self.A -= self.learn_rate * SSE_a\\n        self.B -= self.learn_rate * SSE_b\\n        self.a0 -= self.learn_rate * s\\n        self.b0 -= self.learn_rate * y_delta\\n        \\n    def train(self, x_train, y_train, epochs, shuffle=True): \\n        if shuffle:\\n            indices = np.arange(N)\\n            np.random.shuffle(indices)\\n            x_train, y_train = x_train[indices], y_train[indices]\\n\\n        epoch = 1\\n        while(epoch <= epochs):\\n            y_hat = np.array([self.forward_pass(x) for x in x_train])\\n            self.back_propogate(x_train, y_train, y_hat)\\n            print('Epoch: ', epoch, 'Loss: ', self.loss(y_train, y_hat))\\n            epoch += 1\\n\\n    def loss(self, y_train, y_hat):\\n        return np.mean((y_train - y_hat)**2)\\n    \\n    def predict(self, test_x):\\n#         if test_x.shape == (2,): test_x = np.reshape(test_x, (1,2))\\n        y_hats = np.array([self.forward_pass(x) for x in test_x])\\n        y_outs = int(y_hats > 0.5)\\n        return y_outs\\n    \\n#     def load_model(self):\\n#         # something here\\n#     def get_weights(self):\\n#         # some code here\\n#     def load_weights(self):\\n#         # some code here\\n#     def save_model(self):\\n#         # some code here\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ignore this, this does not work \n",
    "# couldn't find the time to fix it, just submitting my effort\n",
    "# apologies if the above code is not convienent\n",
    "\"\"\"\n",
    "class NN(object):\n",
    "    def __init__(self, hidden_dim=2, learn_rate=0.01):\n",
    "        self.learn_rate = learn_rate\n",
    "        self.input_dim = 2\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = 1\n",
    "        # initialize all weights and biases to zeros\n",
    "#         self.A = np.zeros((self.hidden_dim, self.input_dim))  # input to hidden layer weights\n",
    "#         self.B = np.zeros(self.hidden_dim)                     # hidden layer to output weights \n",
    "#         self.a0 = np.zeros(hidden_dim) # input to hidden layer bias\n",
    "#         self.b0 = 0 # hidden layer to output bias\n",
    "        self.A = np.random.normal(0, 1, (self.hidden_dim, self.input_dim))\n",
    "        self.B = np.random.normal(0, 1, self.hidden_dim)\n",
    "        self.a0 = np.random.normal(0, 1, self.hidden_dim)\n",
    "        self.b0 = np.random.normal(0, 1, 1)\n",
    "        \n",
    "    def sigmoid(self, t):\n",
    "        return 1/(1  + np.exp(-t))\n",
    "    \n",
    "    def dsigmoid(self, t):\n",
    "        return self.sigmoid(t)*(1 - self.sigmoid(t))\n",
    "    \n",
    "    def hidden_layer(self, x):\n",
    "        self.z = self.sigmoid(np.dot(self.A, x) + self.a0) \n",
    "        return self.z\n",
    "    \n",
    "    def forward_pass(self, x):\n",
    "        self.y_hat = self.sigmoid(np.dot(self.B, self.hidden_layer(x)) + self.b0)\n",
    "        return self.y_hat\n",
    "    \n",
    "    def back_propogate(self, X, Y, Y_hat):\n",
    "        SSE_a, SSE_b   = np.zeros_like(self.A), np.zeros_like(self.B)\n",
    "        z_bias, y_bias = np.zeros_like(self.a0), np.zeros_like(self.b0)\n",
    "        for i, x in enumerate(X):\n",
    "            z = self.hidden_layer(x)\n",
    "            y_error = Y[i] - Y_hat[i]\n",
    "            y_delta = -2* y_error * self.dsigmoid(np.dot(self.B, z) + self.b0)\n",
    "            s = self.dsigmoid(np.dot(self.A,x) + self.a0) * self.B * y_delta\n",
    "            SSE_b += y_delta*z\n",
    "            SSE_a += np.tensordot(x,s, axes=0)\n",
    "            y_bias += y_delta\n",
    "            z_bias += s\n",
    "        # update the weights and biases\n",
    "        self.A -= self.learn_rate * SSE_a\n",
    "        self.B -= self.learn_rate * SSE_b\n",
    "        self.a0 -= self.learn_rate * s\n",
    "        self.b0 -= self.learn_rate * y_delta\n",
    "        \n",
    "    def train(self, x_train, y_train, epochs, shuffle=True): \n",
    "        if shuffle:\n",
    "            indices = np.arange(N)\n",
    "            np.random.shuffle(indices)\n",
    "            x_train, y_train = x_train[indices], y_train[indices]\n",
    "\n",
    "        epoch = 1\n",
    "        while(epoch <= epochs):\n",
    "            y_hat = np.array([self.forward_pass(x) for x in x_train])\n",
    "            self.back_propogate(x_train, y_train, y_hat)\n",
    "            print('Epoch: ', epoch, 'Loss: ', self.loss(y_train, y_hat))\n",
    "            epoch += 1\n",
    "\n",
    "    def loss(self, y_train, y_hat):\n",
    "        return np.mean((y_train - y_hat)**2)\n",
    "    \n",
    "    def predict(self, test_x):\n",
    "#         if test_x.shape == (2,): test_x = np.reshape(test_x, (1,2))\n",
    "        y_hats = np.array([self.forward_pass(x) for x in test_x])\n",
    "        y_outs = int(y_hats > 0.5)\n",
    "        return y_outs\n",
    "    \n",
    "#     def load_model(self):\n",
    "#         # something here\n",
    "#     def get_weights(self):\n",
    "#         # some code here\n",
    "#     def load_weights(self):\n",
    "#         # some code here\n",
    "#     def save_model(self):\n",
    "#         # some code here\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
