{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the back-propagation algorithm to learn the weights of a perceptron with 2 input nodes, 2 hidden nodes and 1 output node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: **Python3** in used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters - input all parameter values here\n",
    "input_dim = 2\n",
    "hidden_dim = 2 # dimensions of hidden layers\n",
    "std = 0.01  # train data noise standard deviation\n",
    "w_std = 1\n",
    "learn_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "x_inputs = np.array([np.zeros(2), np.ones(2), np.array([1,0]), np.array([0,1])])\n",
    "def generate_trainset(N):\n",
    "    X = np.repeat(x_inputs, N//4, axis=0)\n",
    "    y_and = np.logical_and(X.T[0], X.T[1]).astype(np.float)\n",
    "    # add noise to data\n",
    "    X += np.random.normal(0, std, X.shape)\n",
    "    y_and += np.random.normal(0, std, N)\n",
    "    # shuffle the training data\n",
    "    indices = np.arange(N)\n",
    "    np.random.shuffle(indices)\n",
    "    x_train, y_train = X[indices], y_and[indices]\n",
    "    return x_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid( t):\n",
    "    return 1/(1  + np.exp(-t))\n",
    "\n",
    "def dsigmoid( t):\n",
    "    return sigmoid(t)*(1 - sigmoid(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Experiment with N = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000\n",
    "x_train, y_train = generate_trainset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/300  Loss:  [0.35316205]\n",
      "Epoch:  2/300  Loss:  [0.3065292]\n",
      "Epoch:  3/300  Loss:  [0.26516439]\n",
      "Epoch:  4/300  Loss:  [0.23015027]\n",
      "Epoch:  5/300  Loss:  [0.2026048]\n",
      "Epoch:  6/300  Loss:  [0.18259133]\n",
      "Epoch:  7/300  Loss:  [0.16827796]\n",
      "Epoch:  8/300  Loss:  [0.15717554]\n",
      "Epoch:  9/300  Loss:  [0.14771514]\n",
      "Epoch:  10/300  Loss:  [0.1392497]\n",
      "Epoch:  11/300  Loss:  [0.13152086]\n",
      "Epoch:  12/300  Loss:  [0.12439547]\n",
      "Epoch:  13/300  Loss:  [0.1177938]\n",
      "Epoch:  14/300  Loss:  [0.11166927]\n",
      "Epoch:  15/300  Loss:  [0.10599634]\n",
      "Epoch:  16/300  Loss:  [0.10076012]\n",
      "Epoch:  17/300  Loss:  [0.09594842]\n",
      "Epoch:  18/300  Loss:  [0.09154691]\n",
      "Epoch:  19/300  Loss:  [0.08753701]\n",
      "Epoch:  20/300  Loss:  [0.08389583]\n",
      "Epoch:  21/300  Loss:  [0.08059723]\n",
      "Epoch:  22/300  Loss:  [0.07761319]\n",
      "Epoch:  23/300  Loss:  [0.07491523]\n",
      "Epoch:  24/300  Loss:  [0.0724755]\n",
      "Epoch:  25/300  Loss:  [0.07026758]\n",
      "Epoch:  26/300  Loss:  [0.06826694]\n",
      "Epoch:  27/300  Loss:  [0.06645119]\n",
      "Epoch:  28/300  Loss:  [0.06480012]\n",
      "Epoch:  29/300  Loss:  [0.0632957]\n",
      "Epoch:  30/300  Loss:  [0.06192189]\n",
      "Epoch:  31/300  Loss:  [0.06066451]\n",
      "Epoch:  32/300  Loss:  [0.05951106]\n",
      "Epoch:  33/300  Loss:  [0.05845051]\n",
      "Epoch:  34/300  Loss:  [0.05747317]\n",
      "Epoch:  35/300  Loss:  [0.0565705]\n",
      "Epoch:  36/300  Loss:  [0.05573499]\n",
      "Epoch:  37/300  Loss:  [0.05495999]\n",
      "Epoch:  38/300  Loss:  [0.05423966]\n",
      "Epoch:  39/300  Loss:  [0.05356881]\n",
      "Epoch:  40/300  Loss:  [0.05294285]\n",
      "Epoch:  41/300  Loss:  [0.0523577]\n",
      "Epoch:  42/300  Loss:  [0.05180973]\n",
      "Epoch:  43/300  Loss:  [0.05129571]\n",
      "Epoch:  44/300  Loss:  [0.05081275]\n",
      "Epoch:  45/300  Loss:  [0.05035825]\n",
      "Epoch:  46/300  Loss:  [0.04992991]\n",
      "Epoch:  47/300  Loss:  [0.04952563]\n",
      "Epoch:  48/300  Loss:  [0.04914353]\n",
      "Epoch:  49/300  Loss:  [0.04878191]\n",
      "Epoch:  50/300  Loss:  [0.04843925]\n",
      "Epoch:  51/300  Loss:  [0.04811414]\n",
      "Epoch:  52/300  Loss:  [0.04780533]\n",
      "Epoch:  53/300  Loss:  [0.04751166]\n",
      "Epoch:  54/300  Loss:  [0.0472321]\n",
      "Epoch:  55/300  Loss:  [0.04696567]\n",
      "Epoch:  56/300  Loss:  [0.04671152]\n",
      "Epoch:  57/300  Loss:  [0.04646884]\n",
      "Epoch:  58/300  Loss:  [0.04623689]\n",
      "Epoch:  59/300  Loss:  [0.04601501]\n",
      "Epoch:  60/300  Loss:  [0.04580257]\n",
      "Epoch:  61/300  Loss:  [0.04559899]\n",
      "Epoch:  62/300  Loss:  [0.04540376]\n",
      "Epoch:  63/300  Loss:  [0.04521637]\n",
      "Epoch:  64/300  Loss:  [0.0450364]\n",
      "Epoch:  65/300  Loss:  [0.0448634]\n",
      "Epoch:  66/300  Loss:  [0.044697]\n",
      "Epoch:  67/300  Loss:  [0.04453684]\n",
      "Epoch:  68/300  Loss:  [0.04438257]\n",
      "Epoch:  69/300  Loss:  [0.0442339]\n",
      "Epoch:  70/300  Loss:  [0.04409052]\n",
      "Epoch:  71/300  Loss:  [0.04395217]\n",
      "Epoch:  72/300  Loss:  [0.04381859]\n",
      "Epoch:  73/300  Loss:  [0.04368955]\n",
      "Epoch:  74/300  Loss:  [0.04356482]\n",
      "Epoch:  75/300  Loss:  [0.04344419]\n",
      "Epoch:  76/300  Loss:  [0.04332748]\n",
      "Epoch:  77/300  Loss:  [0.04321449]\n",
      "Epoch:  78/300  Loss:  [0.04310506]\n",
      "Epoch:  79/300  Loss:  [0.04299902]\n",
      "Epoch:  80/300  Loss:  [0.04289623]\n",
      "Epoch:  81/300  Loss:  [0.04279653]\n",
      "Epoch:  82/300  Loss:  [0.0426998]\n",
      "Epoch:  83/300  Loss:  [0.0426059]\n",
      "Epoch:  84/300  Loss:  [0.04251471]\n",
      "Epoch:  85/300  Loss:  [0.04242613]\n",
      "Epoch:  86/300  Loss:  [0.04234004]\n",
      "Epoch:  87/300  Loss:  [0.04225634]\n",
      "Epoch:  88/300  Loss:  [0.04217494]\n",
      "Epoch:  89/300  Loss:  [0.04209574]\n",
      "Epoch:  90/300  Loss:  [0.04201866]\n",
      "Epoch:  91/300  Loss:  [0.04194361]\n",
      "Epoch:  92/300  Loss:  [0.04187052]\n",
      "Epoch:  93/300  Loss:  [0.04179931]\n",
      "Epoch:  94/300  Loss:  [0.04172991]\n",
      "Epoch:  95/300  Loss:  [0.04166226]\n",
      "Epoch:  96/300  Loss:  [0.04159629]\n",
      "Epoch:  97/300  Loss:  [0.04153193]\n",
      "Epoch:  98/300  Loss:  [0.04146914]\n",
      "Epoch:  99/300  Loss:  [0.04140786]\n",
      "Epoch:  100/300  Loss:  [0.04134803]\n",
      "Epoch:  101/300  Loss:  [0.0412896]\n",
      "Epoch:  102/300  Loss:  [0.04123253]\n",
      "Epoch:  103/300  Loss:  [0.04117676]\n",
      "Epoch:  104/300  Loss:  [0.04112227]\n",
      "Epoch:  105/300  Loss:  [0.04106899]\n",
      "Epoch:  106/300  Loss:  [0.0410169]\n",
      "Epoch:  107/300  Loss:  [0.04096596]\n",
      "Epoch:  108/300  Loss:  [0.04091612]\n",
      "Epoch:  109/300  Loss:  [0.04086736]\n",
      "Epoch:  110/300  Loss:  [0.04081963]\n",
      "Epoch:  111/300  Loss:  [0.04077291]\n",
      "Epoch:  112/300  Loss:  [0.04072717]\n",
      "Epoch:  113/300  Loss:  [0.04068237]\n",
      "Epoch:  114/300  Loss:  [0.04063849]\n",
      "Epoch:  115/300  Loss:  [0.0405955]\n",
      "Epoch:  116/300  Loss:  [0.04055338]\n",
      "Epoch:  117/300  Loss:  [0.04051209]\n",
      "Epoch:  118/300  Loss:  [0.04047161]\n",
      "Epoch:  119/300  Loss:  [0.04043192]\n",
      "Epoch:  120/300  Loss:  [0.040393]\n",
      "Epoch:  121/300  Loss:  [0.04035483]\n",
      "Epoch:  122/300  Loss:  [0.04031738]\n",
      "Epoch:  123/300  Loss:  [0.04028063]\n",
      "Epoch:  124/300  Loss:  [0.04024457]\n",
      "Epoch:  125/300  Loss:  [0.04020918]\n",
      "Epoch:  126/300  Loss:  [0.04017443]\n",
      "Epoch:  127/300  Loss:  [0.04014032]\n",
      "Epoch:  128/300  Loss:  [0.04010681]\n",
      "Epoch:  129/300  Loss:  [0.04007391]\n",
      "Epoch:  130/300  Loss:  [0.04004159]\n",
      "Epoch:  131/300  Loss:  [0.04000983]\n",
      "Epoch:  132/300  Loss:  [0.03997863]\n",
      "Epoch:  133/300  Loss:  [0.03994796]\n",
      "Epoch:  134/300  Loss:  [0.03991782]\n",
      "Epoch:  135/300  Loss:  [0.03988819]\n",
      "Epoch:  136/300  Loss:  [0.03985905]\n",
      "Epoch:  137/300  Loss:  [0.03983041]\n",
      "Epoch:  138/300  Loss:  [0.03980223]\n",
      "Epoch:  139/300  Loss:  [0.03977452]\n",
      "Epoch:  140/300  Loss:  [0.03974726]\n",
      "Epoch:  141/300  Loss:  [0.03972044]\n",
      "Epoch:  142/300  Loss:  [0.03969406]\n",
      "Epoch:  143/300  Loss:  [0.03966809]\n",
      "Epoch:  144/300  Loss:  [0.03964253]\n",
      "Epoch:  145/300  Loss:  [0.03961737]\n",
      "Epoch:  146/300  Loss:  [0.0395926]\n",
      "Epoch:  147/300  Loss:  [0.03956822]\n",
      "Epoch:  148/300  Loss:  [0.0395442]\n",
      "Epoch:  149/300  Loss:  [0.03952056]\n",
      "Epoch:  150/300  Loss:  [0.03949727]\n",
      "Epoch:  151/300  Loss:  [0.03947433]\n",
      "Epoch:  152/300  Loss:  [0.03945173]\n",
      "Epoch:  153/300  Loss:  [0.03942946]\n",
      "Epoch:  154/300  Loss:  [0.03940752]\n",
      "Epoch:  155/300  Loss:  [0.0393859]\n",
      "Epoch:  156/300  Loss:  [0.03936459]\n",
      "Epoch:  157/300  Loss:  [0.03934359]\n",
      "Epoch:  158/300  Loss:  [0.03932288]\n",
      "Epoch:  159/300  Loss:  [0.03930247]\n",
      "Epoch:  160/300  Loss:  [0.03928235]\n",
      "Epoch:  161/300  Loss:  [0.03926251]\n",
      "Epoch:  162/300  Loss:  [0.03924294]\n",
      "Epoch:  163/300  Loss:  [0.03922364]\n",
      "Epoch:  164/300  Loss:  [0.03920461]\n",
      "Epoch:  165/300  Loss:  [0.03918583]\n",
      "Epoch:  166/300  Loss:  [0.03916731]\n",
      "Epoch:  167/300  Loss:  [0.03914903]\n",
      "Epoch:  168/300  Loss:  [0.039131]\n",
      "Epoch:  169/300  Loss:  [0.03911321]\n",
      "Epoch:  170/300  Loss:  [0.03909565]\n",
      "Epoch:  171/300  Loss:  [0.03907832]\n",
      "Epoch:  172/300  Loss:  [0.03906122]\n",
      "Epoch:  173/300  Loss:  [0.03904433]\n",
      "Epoch:  174/300  Loss:  [0.03902767]\n",
      "Epoch:  175/300  Loss:  [0.03901121]\n",
      "Epoch:  176/300  Loss:  [0.03899497]\n",
      "Epoch:  177/300  Loss:  [0.03897892]\n",
      "Epoch:  178/300  Loss:  [0.03896308]\n",
      "Epoch:  179/300  Loss:  [0.03894744]\n",
      "Epoch:  180/300  Loss:  [0.03893198]\n",
      "Epoch:  181/300  Loss:  [0.03891672]\n",
      "Epoch:  182/300  Loss:  [0.03890164]\n",
      "Epoch:  183/300  Loss:  [0.03888675]\n",
      "Epoch:  184/300  Loss:  [0.03887204]\n",
      "Epoch:  185/300  Loss:  [0.0388575]\n",
      "Epoch:  186/300  Loss:  [0.03884313]\n",
      "Epoch:  187/300  Loss:  [0.03882894]\n",
      "Epoch:  188/300  Loss:  [0.03881491]\n",
      "Epoch:  189/300  Loss:  [0.03880105]\n",
      "Epoch:  190/300  Loss:  [0.03878734]\n",
      "Epoch:  191/300  Loss:  [0.0387738]\n",
      "Epoch:  192/300  Loss:  [0.03876041]\n",
      "Epoch:  193/300  Loss:  [0.03874718]\n",
      "Epoch:  194/300  Loss:  [0.03873409]\n",
      "Epoch:  195/300  Loss:  [0.03872116]\n",
      "Epoch:  196/300  Loss:  [0.03870837]\n",
      "Epoch:  197/300  Loss:  [0.03869572]\n",
      "Epoch:  198/300  Loss:  [0.03868321]\n",
      "Epoch:  199/300  Loss:  [0.03867084]\n",
      "Epoch:  200/300  Loss:  [0.03865861]\n",
      "Epoch:  201/300  Loss:  [0.03864651]\n",
      "Epoch:  202/300  Loss:  [0.03863454]\n",
      "Epoch:  203/300  Loss:  [0.0386227]\n",
      "Epoch:  204/300  Loss:  [0.03861099]\n",
      "Epoch:  205/300  Loss:  [0.03859941]\n",
      "Epoch:  206/300  Loss:  [0.03858794]\n",
      "Epoch:  207/300  Loss:  [0.0385766]\n",
      "Epoch:  208/300  Loss:  [0.03856538]\n",
      "Epoch:  209/300  Loss:  [0.03855428]\n",
      "Epoch:  210/300  Loss:  [0.03854329]\n",
      "Epoch:  211/300  Loss:  [0.03853242]\n",
      "Epoch:  212/300  Loss:  [0.03852166]\n",
      "Epoch:  213/300  Loss:  [0.038511]\n",
      "Epoch:  214/300  Loss:  [0.03850046]\n",
      "Epoch:  215/300  Loss:  [0.03849003]\n",
      "Epoch:  216/300  Loss:  [0.0384797]\n",
      "Epoch:  217/300  Loss:  [0.03846947]\n",
      "Epoch:  218/300  Loss:  [0.03845935]\n",
      "Epoch:  219/300  Loss:  [0.03844932]\n",
      "Epoch:  220/300  Loss:  [0.0384394]\n",
      "Epoch:  221/300  Loss:  [0.03842957]\n",
      "Epoch:  222/300  Loss:  [0.03841984]\n",
      "Epoch:  223/300  Loss:  [0.03841021]\n",
      "Epoch:  224/300  Loss:  [0.03840067]\n",
      "Epoch:  225/300  Loss:  [0.03839122]\n",
      "Epoch:  226/300  Loss:  [0.03838186]\n",
      "Epoch:  227/300  Loss:  [0.03837259]\n",
      "Epoch:  228/300  Loss:  [0.03836341]\n",
      "Epoch:  229/300  Loss:  [0.03835432]\n",
      "Epoch:  230/300  Loss:  [0.03834531]\n",
      "Epoch:  231/300  Loss:  [0.03833639]\n",
      "Epoch:  232/300  Loss:  [0.03832755]\n",
      "Epoch:  233/300  Loss:  [0.03831879]\n",
      "Epoch:  234/300  Loss:  [0.03831012]\n",
      "Epoch:  235/300  Loss:  [0.03830152]\n",
      "Epoch:  236/300  Loss:  [0.038293]\n",
      "Epoch:  237/300  Loss:  [0.03828456]\n",
      "Epoch:  238/300  Loss:  [0.0382762]\n",
      "Epoch:  239/300  Loss:  [0.03826791]\n",
      "Epoch:  240/300  Loss:  [0.0382597]\n",
      "Epoch:  241/300  Loss:  [0.03825156]\n",
      "Epoch:  242/300  Loss:  [0.03824349]\n",
      "Epoch:  243/300  Loss:  [0.0382355]\n",
      "Epoch:  244/300  Loss:  [0.03822758]\n",
      "Epoch:  245/300  Loss:  [0.03821972]\n",
      "Epoch:  246/300  Loss:  [0.03821194]\n",
      "Epoch:  247/300  Loss:  [0.03820422]\n",
      "Epoch:  248/300  Loss:  [0.03819657]\n",
      "Epoch:  249/300  Loss:  [0.03818898]\n",
      "Epoch:  250/300  Loss:  [0.03818146]\n",
      "Epoch:  251/300  Loss:  [0.03817401]\n",
      "Epoch:  252/300  Loss:  [0.03816662]\n",
      "Epoch:  253/300  Loss:  [0.03815929]\n",
      "Epoch:  254/300  Loss:  [0.03815202]\n",
      "Epoch:  255/300  Loss:  [0.03814482]\n",
      "Epoch:  256/300  Loss:  [0.03813767]\n",
      "Epoch:  257/300  Loss:  [0.03813059]\n",
      "Epoch:  258/300  Loss:  [0.03812356]\n",
      "Epoch:  259/300  Loss:  [0.0381166]\n",
      "Epoch:  260/300  Loss:  [0.03810968]\n",
      "Epoch:  261/300  Loss:  [0.03810283]\n",
      "Epoch:  262/300  Loss:  [0.03809603]\n",
      "Epoch:  263/300  Loss:  [0.03808929]\n",
      "Epoch:  264/300  Loss:  [0.0380826]\n",
      "Epoch:  265/300  Loss:  [0.03807597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  266/300  Loss:  [0.03806939]\n",
      "Epoch:  267/300  Loss:  [0.03806286]\n",
      "Epoch:  268/300  Loss:  [0.03805639]\n",
      "Epoch:  269/300  Loss:  [0.03804996]\n",
      "Epoch:  270/300  Loss:  [0.03804359]\n",
      "Epoch:  271/300  Loss:  [0.03803727]\n",
      "Epoch:  272/300  Loss:  [0.03803099]\n",
      "Epoch:  273/300  Loss:  [0.03802477]\n",
      "Epoch:  274/300  Loss:  [0.03801859]\n",
      "Epoch:  275/300  Loss:  [0.03801246]\n",
      "Epoch:  276/300  Loss:  [0.03800638]\n",
      "Epoch:  277/300  Loss:  [0.03800035]\n",
      "Epoch:  278/300  Loss:  [0.03799436]\n",
      "Epoch:  279/300  Loss:  [0.03798842]\n",
      "Epoch:  280/300  Loss:  [0.03798252]\n",
      "Epoch:  281/300  Loss:  [0.03797667]\n",
      "Epoch:  282/300  Loss:  [0.03797086]\n",
      "Epoch:  283/300  Loss:  [0.0379651]\n",
      "Epoch:  284/300  Loss:  [0.03795937]\n",
      "Epoch:  285/300  Loss:  [0.03795369]\n",
      "Epoch:  286/300  Loss:  [0.03794806]\n",
      "Epoch:  287/300  Loss:  [0.03794246]\n",
      "Epoch:  288/300  Loss:  [0.03793691]\n",
      "Epoch:  289/300  Loss:  [0.0379314]\n",
      "Epoch:  290/300  Loss:  [0.03792592]\n",
      "Epoch:  291/300  Loss:  [0.03792049]\n",
      "Epoch:  292/300  Loss:  [0.0379151]\n",
      "Epoch:  293/300  Loss:  [0.03790974]\n",
      "Epoch:  294/300  Loss:  [0.03790442]\n",
      "Epoch:  295/300  Loss:  [0.03789915]\n",
      "Epoch:  296/300  Loss:  [0.03789391]\n",
      "Epoch:  297/300  Loss:  [0.0378887]\n",
      "Epoch:  298/300  Loss:  [0.03788354]\n",
      "Epoch:  299/300  Loss:  [0.03787841]\n",
      "Epoch:  300/300  Loss:  [0.03787331]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 300 # number of itrations\\\n",
    "# training 005\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03417009]\n",
      " [0.61430546]\n",
      " [0.02584656]\n",
      " [0.02229733]]\n",
      "[[0]\n",
      " [1]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "0.5def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "######  Experiment with N = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "x_train, y_train = generate_trainset(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1/500  Loss:  [0.33929734]\n",
      "Epoch:  2/500  Loss:  [0.31741866]\n",
      "Epoch:  3/500  Loss:  [0.29743375]\n",
      "Epoch:  4/500  Loss:  [0.27959469]\n",
      "Epoch:  5/500  Loss:  [0.26398834]\n",
      "Epoch:  6/500  Loss:  [0.2505601]\n",
      "Epoch:  7/500  Loss:  [0.23915367]\n",
      "Epoch:  8/500  Loss:  [0.22955362]\n",
      "Epoch:  9/500  Loss:  [0.22152155]\n",
      "Epoch:  10/500  Loss:  [0.21482182]\n",
      "Epoch:  11/500  Loss:  [0.20923701]\n",
      "Epoch:  12/500  Loss:  [0.20457549]\n",
      "Epoch:  13/500  Loss:  [0.20067334]\n",
      "Epoch:  14/500  Loss:  [0.19739326]\n",
      "Epoch:  15/500  Loss:  [0.19462175]\n",
      "Epoch:  16/500  Loss:  [0.19226584]\n",
      "Epoch:  17/500  Loss:  [0.19024974]\n",
      "Epoch:  18/500  Loss:  [0.18851181]\n",
      "Epoch:  19/500  Loss:  [0.18700197]\n",
      "Epoch:  20/500  Loss:  [0.18567949]\n",
      "Epoch:  21/500  Loss:  [0.18451119]\n",
      "Epoch:  22/500  Loss:  [0.18346998]\n",
      "Epoch:  23/500  Loss:  [0.18253365]\n",
      "Epoch:  24/500  Loss:  [0.18168398]\n",
      "Epoch:  25/500  Loss:  [0.18090591]\n",
      "Epoch:  26/500  Loss:  [0.180187]\n",
      "Epoch:  27/500  Loss:  [0.17951691]\n",
      "Epoch:  28/500  Loss:  [0.17888701]\n",
      "Epoch:  29/500  Loss:  [0.17829007]\n",
      "Epoch:  30/500  Loss:  [0.17772002]\n",
      "Epoch:  31/500  Loss:  [0.17717174]\n",
      "Epoch:  32/500  Loss:  [0.1766409]\n",
      "Epoch:  33/500  Loss:  [0.1761238]\n",
      "Epoch:  34/500  Loss:  [0.17561729]\n",
      "Epoch:  35/500  Loss:  [0.17511868]\n",
      "Epoch:  36/500  Loss:  [0.17462566]\n",
      "Epoch:  37/500  Loss:  [0.17413621]\n",
      "Epoch:  38/500  Loss:  [0.17364861]\n",
      "Epoch:  39/500  Loss:  [0.17316136]\n",
      "Epoch:  40/500  Loss:  [0.17267315]\n",
      "Epoch:  41/500  Loss:  [0.17218283]\n",
      "Epoch:  42/500  Loss:  [0.1716894]\n",
      "Epoch:  43/500  Loss:  [0.171192]\n",
      "Epoch:  44/500  Loss:  [0.17068985]\n",
      "Epoch:  45/500  Loss:  [0.17018228]\n",
      "Epoch:  46/500  Loss:  [0.16966871]\n",
      "Epoch:  47/500  Loss:  [0.16914863]\n",
      "Epoch:  48/500  Loss:  [0.1686216]\n",
      "Epoch:  49/500  Loss:  [0.16808723]\n",
      "Epoch:  50/500  Loss:  [0.16754522]\n",
      "Epoch:  51/500  Loss:  [0.1669953]\n",
      "Epoch:  52/500  Loss:  [0.16643727]\n",
      "Epoch:  53/500  Loss:  [0.16587095]\n",
      "Epoch:  54/500  Loss:  [0.16529625]\n",
      "Epoch:  55/500  Loss:  [0.16471309]\n",
      "Epoch:  56/500  Loss:  [0.16412148]\n",
      "Epoch:  57/500  Loss:  [0.16352143]\n",
      "Epoch:  58/500  Loss:  [0.16291303]\n",
      "Epoch:  59/500  Loss:  [0.1622964]\n",
      "Epoch:  60/500  Loss:  [0.16167172]\n",
      "Epoch:  61/500  Loss:  [0.16103919]\n",
      "Epoch:  62/500  Loss:  [0.16039908]\n",
      "Epoch:  63/500  Loss:  [0.15975169]\n",
      "Epoch:  64/500  Loss:  [0.15909736]\n",
      "Epoch:  65/500  Loss:  [0.1584365]\n",
      "Epoch:  66/500  Loss:  [0.15776952]\n",
      "Epoch:  67/500  Loss:  [0.15709689]\n",
      "Epoch:  68/500  Loss:  [0.15641912]\n",
      "Epoch:  69/500  Loss:  [0.15573673]\n",
      "Epoch:  70/500  Loss:  [0.1550503]\n",
      "Epoch:  71/500  Loss:  [0.15436042]\n",
      "Epoch:  72/500  Loss:  [0.1536677]\n",
      "Epoch:  73/500  Loss:  [0.15297277]\n",
      "Epoch:  74/500  Loss:  [0.1522763]\n",
      "Epoch:  75/500  Loss:  [0.15157893]\n",
      "Epoch:  76/500  Loss:  [0.15088135]\n",
      "Epoch:  77/500  Loss:  [0.15018421]\n",
      "Epoch:  78/500  Loss:  [0.14948819]\n",
      "Epoch:  79/500  Loss:  [0.14879395]\n",
      "Epoch:  80/500  Loss:  [0.14810215]\n",
      "Epoch:  81/500  Loss:  [0.14741342]\n",
      "Epoch:  82/500  Loss:  [0.14672838]\n",
      "Epoch:  83/500  Loss:  [0.14604763]\n",
      "Epoch:  84/500  Loss:  [0.14537176]\n",
      "Epoch:  85/500  Loss:  [0.14470129]\n",
      "Epoch:  86/500  Loss:  [0.14403677]\n",
      "Epoch:  87/500  Loss:  [0.14337866]\n",
      "Epoch:  88/500  Loss:  [0.14272744]\n",
      "Epoch:  89/500  Loss:  [0.14208351]\n",
      "Epoch:  90/500  Loss:  [0.14144727]\n",
      "Epoch:  91/500  Loss:  [0.14081907]\n",
      "Epoch:  92/500  Loss:  [0.14019923]\n",
      "Epoch:  93/500  Loss:  [0.13958803]\n",
      "Epoch:  94/500  Loss:  [0.13898572]\n",
      "Epoch:  95/500  Loss:  [0.13839251]\n",
      "Epoch:  96/500  Loss:  [0.13780858]\n",
      "Epoch:  97/500  Loss:  [0.13723409]\n",
      "Epoch:  98/500  Loss:  [0.13666916]\n",
      "Epoch:  99/500  Loss:  [0.13611389]\n",
      "Epoch:  100/500  Loss:  [0.13556833]\n",
      "Epoch:  101/500  Loss:  [0.13503253]\n",
      "Epoch:  102/500  Loss:  [0.13450651]\n",
      "Epoch:  103/500  Loss:  [0.13399026]\n",
      "Epoch:  104/500  Loss:  [0.13348376]\n",
      "Epoch:  105/500  Loss:  [0.13298696]\n",
      "Epoch:  106/500  Loss:  [0.13249981]\n",
      "Epoch:  107/500  Loss:  [0.13202223]\n",
      "Epoch:  108/500  Loss:  [0.13155412]\n",
      "Epoch:  109/500  Loss:  [0.13109539]\n",
      "Epoch:  110/500  Loss:  [0.13064593]\n",
      "Epoch:  111/500  Loss:  [0.13020561]\n",
      "Epoch:  112/500  Loss:  [0.1297743]\n",
      "Epoch:  113/500  Loss:  [0.12935186]\n",
      "Epoch:  114/500  Loss:  [0.12893815]\n",
      "Epoch:  115/500  Loss:  [0.12853302]\n",
      "Epoch:  116/500  Loss:  [0.1281363]\n",
      "Epoch:  117/500  Loss:  [0.12774786]\n",
      "Epoch:  118/500  Loss:  [0.12736752]\n",
      "Epoch:  119/500  Loss:  [0.12699512]\n",
      "Epoch:  120/500  Loss:  [0.1266305]\n",
      "Epoch:  121/500  Loss:  [0.12627349]\n",
      "Epoch:  122/500  Loss:  [0.12592393]\n",
      "Epoch:  123/500  Loss:  [0.12558166]\n",
      "Epoch:  124/500  Loss:  [0.12524651]\n",
      "Epoch:  125/500  Loss:  [0.12491831]\n",
      "Epoch:  126/500  Loss:  [0.12459692]\n",
      "Epoch:  127/500  Loss:  [0.12428216]\n",
      "Epoch:  128/500  Loss:  [0.12397388]\n",
      "Epoch:  129/500  Loss:  [0.12367193]\n",
      "Epoch:  130/500  Loss:  [0.12337615]\n",
      "Epoch:  131/500  Loss:  [0.12308639]\n",
      "Epoch:  132/500  Loss:  [0.12280251]\n",
      "Epoch:  133/500  Loss:  [0.12252435]\n",
      "Epoch:  134/500  Loss:  [0.12225179]\n",
      "Epoch:  135/500  Loss:  [0.12198467]\n",
      "Epoch:  136/500  Loss:  [0.12172286]\n",
      "Epoch:  137/500  Loss:  [0.12146623]\n",
      "Epoch:  138/500  Loss:  [0.12121464]\n",
      "Epoch:  139/500  Loss:  [0.12096798]\n",
      "Epoch:  140/500  Loss:  [0.12072612]\n",
      "Epoch:  141/500  Loss:  [0.12048893]\n",
      "Epoch:  142/500  Loss:  [0.1202563]\n",
      "Epoch:  143/500  Loss:  [0.12002811]\n",
      "Epoch:  144/500  Loss:  [0.11980425]\n",
      "Epoch:  145/500  Loss:  [0.11958462]\n",
      "Epoch:  146/500  Loss:  [0.1193691]\n",
      "Epoch:  147/500  Loss:  [0.11915759]\n",
      "Epoch:  148/500  Loss:  [0.11895]\n",
      "Epoch:  149/500  Loss:  [0.11874621]\n",
      "Epoch:  150/500  Loss:  [0.11854615]\n",
      "Epoch:  151/500  Loss:  [0.11834971]\n",
      "Epoch:  152/500  Loss:  [0.11815681]\n",
      "Epoch:  153/500  Loss:  [0.11796736]\n",
      "Epoch:  154/500  Loss:  [0.11778128]\n",
      "Epoch:  155/500  Loss:  [0.11759847]\n",
      "Epoch:  156/500  Loss:  [0.11741887]\n",
      "Epoch:  157/500  Loss:  [0.11724238]\n",
      "Epoch:  158/500  Loss:  [0.11706895]\n",
      "Epoch:  159/500  Loss:  [0.11689849]\n",
      "Epoch:  160/500  Loss:  [0.11673093]\n",
      "Epoch:  161/500  Loss:  [0.1165662]\n",
      "Epoch:  162/500  Loss:  [0.11640424]\n",
      "Epoch:  163/500  Loss:  [0.11624498]\n",
      "Epoch:  164/500  Loss:  [0.11608836]\n",
      "Epoch:  165/500  Loss:  [0.11593431]\n",
      "Epoch:  166/500  Loss:  [0.11578277]\n",
      "Epoch:  167/500  Loss:  [0.11563368]\n",
      "Epoch:  168/500  Loss:  [0.115487]\n",
      "Epoch:  169/500  Loss:  [0.11534266]\n",
      "Epoch:  170/500  Loss:  [0.11520061]\n",
      "Epoch:  171/500  Loss:  [0.11506079]\n",
      "Epoch:  172/500  Loss:  [0.11492317]\n",
      "Epoch:  173/500  Loss:  [0.11478768]\n",
      "Epoch:  174/500  Loss:  [0.11465428]\n",
      "Epoch:  175/500  Loss:  [0.11452292]\n",
      "Epoch:  176/500  Loss:  [0.11439356]\n",
      "Epoch:  177/500  Loss:  [0.11426616]\n",
      "Epoch:  178/500  Loss:  [0.11414066]\n",
      "Epoch:  179/500  Loss:  [0.11401704]\n",
      "Epoch:  180/500  Loss:  [0.11389524]\n",
      "Epoch:  181/500  Loss:  [0.11377524]\n",
      "Epoch:  182/500  Loss:  [0.11365698]\n",
      "Epoch:  183/500  Loss:  [0.11354044]\n",
      "Epoch:  184/500  Loss:  [0.11342558]\n",
      "Epoch:  185/500  Loss:  [0.11331236]\n",
      "Epoch:  186/500  Loss:  [0.11320074]\n",
      "Epoch:  187/500  Loss:  [0.1130907]\n",
      "Epoch:  188/500  Loss:  [0.11298221]\n",
      "Epoch:  189/500  Loss:  [0.11287522]\n",
      "Epoch:  190/500  Loss:  [0.11276971]\n",
      "Epoch:  191/500  Loss:  [0.11266566]\n",
      "Epoch:  192/500  Loss:  [0.11256302]\n",
      "Epoch:  193/500  Loss:  [0.11246177]\n",
      "Epoch:  194/500  Loss:  [0.11236189]\n",
      "Epoch:  195/500  Loss:  [0.11226334]\n",
      "Epoch:  196/500  Loss:  [0.11216611]\n",
      "Epoch:  197/500  Loss:  [0.11207016]\n",
      "Epoch:  198/500  Loss:  [0.11197546]\n",
      "Epoch:  199/500  Loss:  [0.11188201]\n",
      "Epoch:  200/500  Loss:  [0.11178976]\n",
      "Epoch:  201/500  Loss:  [0.1116987]\n",
      "Epoch:  202/500  Loss:  [0.11160881]\n",
      "Epoch:  203/500  Loss:  [0.11152006]\n",
      "Epoch:  204/500  Loss:  [0.11143243]\n",
      "Epoch:  205/500  Loss:  [0.1113459]\n",
      "Epoch:  206/500  Loss:  [0.11126046]\n",
      "Epoch:  207/500  Loss:  [0.11117607]\n",
      "Epoch:  208/500  Loss:  [0.11109272]\n",
      "Epoch:  209/500  Loss:  [0.11101039]\n",
      "Epoch:  210/500  Loss:  [0.11092907]\n",
      "Epoch:  211/500  Loss:  [0.11084873]\n",
      "Epoch:  212/500  Loss:  [0.11076935]\n",
      "Epoch:  213/500  Loss:  [0.11069093]\n",
      "Epoch:  214/500  Loss:  [0.11061344]\n",
      "Epoch:  215/500  Loss:  [0.11053686]\n",
      "Epoch:  216/500  Loss:  [0.11046118]\n",
      "Epoch:  217/500  Loss:  [0.11038639]\n",
      "Epoch:  218/500  Loss:  [0.11031246]\n",
      "Epoch:  219/500  Loss:  [0.11023939]\n",
      "Epoch:  220/500  Loss:  [0.11016715]\n",
      "Epoch:  221/500  Loss:  [0.11009574]\n",
      "Epoch:  222/500  Loss:  [0.11002514]\n",
      "Epoch:  223/500  Loss:  [0.10995533]\n",
      "Epoch:  224/500  Loss:  [0.10988631]\n",
      "Epoch:  225/500  Loss:  [0.10981805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  226/500  Loss:  [0.10975055]\n",
      "Epoch:  227/500  Loss:  [0.1096838]\n",
      "Epoch:  228/500  Loss:  [0.10961777]\n",
      "Epoch:  229/500  Loss:  [0.10955246]\n",
      "Epoch:  230/500  Loss:  [0.10948786]\n",
      "Epoch:  231/500  Loss:  [0.10942396]\n",
      "Epoch:  232/500  Loss:  [0.10936074]\n",
      "Epoch:  233/500  Loss:  [0.10929819]\n",
      "Epoch:  234/500  Loss:  [0.10923631]\n",
      "Epoch:  235/500  Loss:  [0.10917507]\n",
      "Epoch:  236/500  Loss:  [0.10911448]\n",
      "Epoch:  237/500  Loss:  [0.10905452]\n",
      "Epoch:  238/500  Loss:  [0.10899518]\n",
      "Epoch:  239/500  Loss:  [0.10893644]\n",
      "Epoch:  240/500  Loss:  [0.10887831]\n",
      "Epoch:  241/500  Loss:  [0.10882078]\n",
      "Epoch:  242/500  Loss:  [0.10876382]\n",
      "Epoch:  243/500  Loss:  [0.10870744]\n",
      "Epoch:  244/500  Loss:  [0.10865162]\n",
      "Epoch:  245/500  Loss:  [0.10859636]\n",
      "Epoch:  246/500  Loss:  [0.10854164]\n",
      "Epoch:  247/500  Loss:  [0.10848746]\n",
      "Epoch:  248/500  Loss:  [0.10843382]\n",
      "Epoch:  249/500  Loss:  [0.1083807]\n",
      "Epoch:  250/500  Loss:  [0.10832809]\n",
      "Epoch:  251/500  Loss:  [0.10827599]\n",
      "Epoch:  252/500  Loss:  [0.10822438]\n",
      "Epoch:  253/500  Loss:  [0.10817327]\n",
      "Epoch:  254/500  Loss:  [0.10812265]\n",
      "Epoch:  255/500  Loss:  [0.1080725]\n",
      "Epoch:  256/500  Loss:  [0.10802282]\n",
      "Epoch:  257/500  Loss:  [0.1079736]\n",
      "Epoch:  258/500  Loss:  [0.10792485]\n",
      "Epoch:  259/500  Loss:  [0.10787654]\n",
      "Epoch:  260/500  Loss:  [0.10782867]\n",
      "Epoch:  261/500  Loss:  [0.10778125]\n",
      "Epoch:  262/500  Loss:  [0.10773425]\n",
      "Epoch:  263/500  Loss:  [0.10768768]\n",
      "Epoch:  264/500  Loss:  [0.10764152]\n",
      "Epoch:  265/500  Loss:  [0.10759578]\n",
      "Epoch:  266/500  Loss:  [0.10755044]\n",
      "Epoch:  267/500  Loss:  [0.10750551]\n",
      "Epoch:  268/500  Loss:  [0.10746097]\n",
      "Epoch:  269/500  Loss:  [0.10741682]\n",
      "Epoch:  270/500  Loss:  [0.10737305]\n",
      "Epoch:  271/500  Loss:  [0.10732967]\n",
      "Epoch:  272/500  Loss:  [0.10728665]\n",
      "Epoch:  273/500  Loss:  [0.10724401]\n",
      "Epoch:  274/500  Loss:  [0.10720173]\n",
      "Epoch:  275/500  Loss:  [0.1071598]\n",
      "Epoch:  276/500  Loss:  [0.10711823]\n",
      "Epoch:  277/500  Loss:  [0.10707701]\n",
      "Epoch:  278/500  Loss:  [0.10703613]\n",
      "Epoch:  279/500  Loss:  [0.10699559]\n",
      "Epoch:  280/500  Loss:  [0.10695539]\n",
      "Epoch:  281/500  Loss:  [0.10691551]\n",
      "Epoch:  282/500  Loss:  [0.10687596]\n",
      "Epoch:  283/500  Loss:  [0.10683673]\n",
      "Epoch:  284/500  Loss:  [0.10679782]\n",
      "Epoch:  285/500  Loss:  [0.10675922]\n",
      "Epoch:  286/500  Loss:  [0.10672093]\n",
      "Epoch:  287/500  Loss:  [0.10668294]\n",
      "Epoch:  288/500  Loss:  [0.10664525]\n",
      "Epoch:  289/500  Loss:  [0.10660786]\n",
      "Epoch:  290/500  Loss:  [0.10657076]\n",
      "Epoch:  291/500  Loss:  [0.10653395]\n",
      "Epoch:  292/500  Loss:  [0.10649742]\n",
      "Epoch:  293/500  Loss:  [0.10646118]\n",
      "Epoch:  294/500  Loss:  [0.10642521]\n",
      "Epoch:  295/500  Loss:  [0.10638952]\n",
      "Epoch:  296/500  Loss:  [0.10635409]\n",
      "Epoch:  297/500  Loss:  [0.10631893]\n",
      "Epoch:  298/500  Loss:  [0.10628404]\n",
      "Epoch:  299/500  Loss:  [0.1062494]\n",
      "Epoch:  300/500  Loss:  [0.10621503]\n",
      "Epoch:  301/500  Loss:  [0.1061809]\n",
      "Epoch:  302/500  Loss:  [0.10614703]\n",
      "Epoch:  303/500  Loss:  [0.1061134]\n",
      "Epoch:  304/500  Loss:  [0.10608001]\n",
      "Epoch:  305/500  Loss:  [0.10604687]\n",
      "Epoch:  306/500  Loss:  [0.10601396]\n",
      "Epoch:  307/500  Loss:  [0.10598129]\n",
      "Epoch:  308/500  Loss:  [0.10594885]\n",
      "Epoch:  309/500  Loss:  [0.10591663]\n",
      "Epoch:  310/500  Loss:  [0.10588465]\n",
      "Epoch:  311/500  Loss:  [0.10585288]\n",
      "Epoch:  312/500  Loss:  [0.10582134]\n",
      "Epoch:  313/500  Loss:  [0.10579002]\n",
      "Epoch:  314/500  Loss:  [0.1057589]\n",
      "Epoch:  315/500  Loss:  [0.10572801]\n",
      "Epoch:  316/500  Loss:  [0.10569732]\n",
      "Epoch:  317/500  Loss:  [0.10566683]\n",
      "Epoch:  318/500  Loss:  [0.10563655]\n",
      "Epoch:  319/500  Loss:  [0.10560648]\n",
      "Epoch:  320/500  Loss:  [0.1055766]\n",
      "Epoch:  321/500  Loss:  [0.10554692]\n",
      "Epoch:  322/500  Loss:  [0.10551743]\n",
      "Epoch:  323/500  Loss:  [0.10548814]\n",
      "Epoch:  324/500  Loss:  [0.10545904]\n",
      "Epoch:  325/500  Loss:  [0.10543012]\n",
      "Epoch:  326/500  Loss:  [0.10540139]\n",
      "Epoch:  327/500  Loss:  [0.10537284]\n",
      "Epoch:  328/500  Loss:  [0.10534447]\n",
      "Epoch:  329/500  Loss:  [0.10531628]\n",
      "Epoch:  330/500  Loss:  [0.10528827]\n",
      "Epoch:  331/500  Loss:  [0.10526044]\n",
      "Epoch:  332/500  Loss:  [0.10523277]\n",
      "Epoch:  333/500  Loss:  [0.10520527]\n",
      "Epoch:  334/500  Loss:  [0.10517795]\n",
      "Epoch:  335/500  Loss:  [0.10515079]\n",
      "Epoch:  336/500  Loss:  [0.10512379]\n",
      "Epoch:  337/500  Loss:  [0.10509696]\n",
      "Epoch:  338/500  Loss:  [0.10507028]\n",
      "Epoch:  339/500  Loss:  [0.10504377]\n",
      "Epoch:  340/500  Loss:  [0.10501741]\n",
      "Epoch:  341/500  Loss:  [0.10499121]\n",
      "Epoch:  342/500  Loss:  [0.10496516]\n",
      "Epoch:  343/500  Loss:  [0.10493926]\n",
      "Epoch:  344/500  Loss:  [0.10491351]\n",
      "Epoch:  345/500  Loss:  [0.1048879]\n",
      "Epoch:  346/500  Loss:  [0.10486245]\n",
      "Epoch:  347/500  Loss:  [0.10483714]\n",
      "Epoch:  348/500  Loss:  [0.10481197]\n",
      "Epoch:  349/500  Loss:  [0.10478694]\n",
      "Epoch:  350/500  Loss:  [0.10476205]\n",
      "Epoch:  351/500  Loss:  [0.1047373]\n",
      "Epoch:  352/500  Loss:  [0.10471268]\n",
      "Epoch:  353/500  Loss:  [0.1046882]\n",
      "Epoch:  354/500  Loss:  [0.10466386]\n",
      "Epoch:  355/500  Loss:  [0.10463964]\n",
      "Epoch:  356/500  Loss:  [0.10461555]\n",
      "Epoch:  357/500  Loss:  [0.1045916]\n",
      "Epoch:  358/500  Loss:  [0.10456777]\n",
      "Epoch:  359/500  Loss:  [0.10454406]\n",
      "Epoch:  360/500  Loss:  [0.10452048]\n",
      "Epoch:  361/500  Loss:  [0.10449702]\n",
      "Epoch:  362/500  Loss:  [0.10447369]\n",
      "Epoch:  363/500  Loss:  [0.10445047]\n",
      "Epoch:  364/500  Loss:  [0.10442738]\n",
      "Epoch:  365/500  Loss:  [0.1044044]\n",
      "Epoch:  366/500  Loss:  [0.10438153]\n",
      "Epoch:  367/500  Loss:  [0.10435878]\n",
      "Epoch:  368/500  Loss:  [0.10433615]\n",
      "Epoch:  369/500  Loss:  [0.10431362]\n",
      "Epoch:  370/500  Loss:  [0.10429121]\n",
      "Epoch:  371/500  Loss:  [0.10426891]\n",
      "Epoch:  372/500  Loss:  [0.10424672]\n",
      "Epoch:  373/500  Loss:  [0.10422463]\n",
      "Epoch:  374/500  Loss:  [0.10420265]\n",
      "Epoch:  375/500  Loss:  [0.10418077]\n",
      "Epoch:  376/500  Loss:  [0.104159]\n",
      "Epoch:  377/500  Loss:  [0.10413733]\n",
      "Epoch:  378/500  Loss:  [0.10411576]\n",
      "Epoch:  379/500  Loss:  [0.10409429]\n",
      "Epoch:  380/500  Loss:  [0.10407293]\n",
      "Epoch:  381/500  Loss:  [0.10405166]\n",
      "Epoch:  382/500  Loss:  [0.10403048]\n",
      "Epoch:  383/500  Loss:  [0.10400941]\n",
      "Epoch:  384/500  Loss:  [0.10398843]\n",
      "Epoch:  385/500  Loss:  [0.10396754]\n",
      "Epoch:  386/500  Loss:  [0.10394674]\n",
      "Epoch:  387/500  Loss:  [0.10392604]\n",
      "Epoch:  388/500  Loss:  [0.10390543]\n",
      "Epoch:  389/500  Loss:  [0.10388491]\n",
      "Epoch:  390/500  Loss:  [0.10386447]\n",
      "Epoch:  391/500  Loss:  [0.10384413]\n",
      "Epoch:  392/500  Loss:  [0.10382387]\n",
      "Epoch:  393/500  Loss:  [0.1038037]\n",
      "Epoch:  394/500  Loss:  [0.10378361]\n",
      "Epoch:  395/500  Loss:  [0.10376361]\n",
      "Epoch:  396/500  Loss:  [0.10374369]\n",
      "Epoch:  397/500  Loss:  [0.10372386]\n",
      "Epoch:  398/500  Loss:  [0.1037041]\n",
      "Epoch:  399/500  Loss:  [0.10368443]\n",
      "Epoch:  400/500  Loss:  [0.10366484]\n",
      "Epoch:  401/500  Loss:  [0.10364532]\n",
      "Epoch:  402/500  Loss:  [0.10362589]\n",
      "Epoch:  403/500  Loss:  [0.10360653]\n",
      "Epoch:  404/500  Loss:  [0.10358725]\n",
      "Epoch:  405/500  Loss:  [0.10356804]\n",
      "Epoch:  406/500  Loss:  [0.10354891]\n",
      "Epoch:  407/500  Loss:  [0.10352986]\n",
      "Epoch:  408/500  Loss:  [0.10351088]\n",
      "Epoch:  409/500  Loss:  [0.10349197]\n",
      "Epoch:  410/500  Loss:  [0.10347313]\n",
      "Epoch:  411/500  Loss:  [0.10345436]\n",
      "Epoch:  412/500  Loss:  [0.10343567]\n",
      "Epoch:  413/500  Loss:  [0.10341704]\n",
      "Epoch:  414/500  Loss:  [0.10339848]\n",
      "Epoch:  415/500  Loss:  [0.10338]\n",
      "Epoch:  416/500  Loss:  [0.10336158]\n",
      "Epoch:  417/500  Loss:  [0.10334322]\n",
      "Epoch:  418/500  Loss:  [0.10332494]\n",
      "Epoch:  419/500  Loss:  [0.10330671]\n",
      "Epoch:  420/500  Loss:  [0.10328856]\n",
      "Epoch:  421/500  Loss:  [0.10327047]\n",
      "Epoch:  422/500  Loss:  [0.10325244]\n",
      "Epoch:  423/500  Loss:  [0.10323447]\n",
      "Epoch:  424/500  Loss:  [0.10321657]\n",
      "Epoch:  425/500  Loss:  [0.10319873]\n",
      "Epoch:  426/500  Loss:  [0.10318095]\n",
      "Epoch:  427/500  Loss:  [0.10316323]\n",
      "Epoch:  428/500  Loss:  [0.10314557]\n",
      "Epoch:  429/500  Loss:  [0.10312797]\n",
      "Epoch:  430/500  Loss:  [0.10311043]\n",
      "Epoch:  431/500  Loss:  [0.10309295]\n",
      "Epoch:  432/500  Loss:  [0.10307552]\n",
      "Epoch:  433/500  Loss:  [0.10305816]\n",
      "Epoch:  434/500  Loss:  [0.10304085]\n",
      "Epoch:  435/500  Loss:  [0.10302359]\n",
      "Epoch:  436/500  Loss:  [0.10300639]\n",
      "Epoch:  437/500  Loss:  [0.10298925]\n",
      "Epoch:  438/500  Loss:  [0.10297216]\n",
      "Epoch:  439/500  Loss:  [0.10295512]\n",
      "Epoch:  440/500  Loss:  [0.10293814]\n",
      "Epoch:  441/500  Loss:  [0.10292121]\n",
      "Epoch:  442/500  Loss:  [0.10290433]\n",
      "Epoch:  443/500  Loss:  [0.10288751]\n",
      "Epoch:  444/500  Loss:  [0.10287074]\n",
      "Epoch:  445/500  Loss:  [0.10285401]\n",
      "Epoch:  446/500  Loss:  [0.10283734]\n",
      "Epoch:  447/500  Loss:  [0.10282072]\n",
      "Epoch:  448/500  Loss:  [0.10280415]\n",
      "Epoch:  449/500  Loss:  [0.10278762]\n",
      "Epoch:  450/500  Loss:  [0.10277115]\n",
      "Epoch:  451/500  Loss:  [0.10275472]\n",
      "Epoch:  452/500  Loss:  [0.10273834]\n",
      "Epoch:  453/500  Loss:  [0.10272201]\n",
      "Epoch:  454/500  Loss:  [0.10270573]\n",
      "Epoch:  455/500  Loss:  [0.10268949]\n",
      "Epoch:  456/500  Loss:  [0.1026733]\n",
      "Epoch:  457/500  Loss:  [0.10265716]\n",
      "Epoch:  458/500  Loss:  [0.10264105]\n",
      "Epoch:  459/500  Loss:  [0.102625]\n",
      "Epoch:  460/500  Loss:  [0.10260899]\n",
      "Epoch:  461/500  Loss:  [0.10259302]\n",
      "Epoch:  462/500  Loss:  [0.1025771]\n",
      "Epoch:  463/500  Loss:  [0.10256122]\n",
      "Epoch:  464/500  Loss:  [0.10254538]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  465/500  Loss:  [0.10252959]\n",
      "Epoch:  466/500  Loss:  [0.10251384]\n",
      "Epoch:  467/500  Loss:  [0.10249813]\n",
      "Epoch:  468/500  Loss:  [0.10248246]\n",
      "Epoch:  469/500  Loss:  [0.10246683]\n",
      "Epoch:  470/500  Loss:  [0.10245125]\n",
      "Epoch:  471/500  Loss:  [0.1024357]\n",
      "Epoch:  472/500  Loss:  [0.1024202]\n",
      "Epoch:  473/500  Loss:  [0.10240473]\n",
      "Epoch:  474/500  Loss:  [0.10238931]\n",
      "Epoch:  475/500  Loss:  [0.10237392]\n",
      "Epoch:  476/500  Loss:  [0.10235857]\n",
      "Epoch:  477/500  Loss:  [0.10234327]\n",
      "Epoch:  478/500  Loss:  [0.10232799]\n",
      "Epoch:  479/500  Loss:  [0.10231276]\n",
      "Epoch:  480/500  Loss:  [0.10229757]\n",
      "Epoch:  481/500  Loss:  [0.10228241]\n",
      "Epoch:  482/500  Loss:  [0.10226729]\n",
      "Epoch:  483/500  Loss:  [0.10225221]\n",
      "Epoch:  484/500  Loss:  [0.10223716]\n",
      "Epoch:  485/500  Loss:  [0.10222215]\n",
      "Epoch:  486/500  Loss:  [0.10220718]\n",
      "Epoch:  487/500  Loss:  [0.10219224]\n",
      "Epoch:  488/500  Loss:  [0.10217734]\n",
      "Epoch:  489/500  Loss:  [0.10216247]\n",
      "Epoch:  490/500  Loss:  [0.10214764]\n",
      "Epoch:  491/500  Loss:  [0.10213284]\n",
      "Epoch:  492/500  Loss:  [0.10211808]\n",
      "Epoch:  493/500  Loss:  [0.10210335]\n",
      "Epoch:  494/500  Loss:  [0.10208866]\n",
      "Epoch:  495/500  Loss:  [0.102074]\n",
      "Epoch:  496/500  Loss:  [0.10205937]\n",
      "Epoch:  497/500  Loss:  [0.10204478]\n",
      "Epoch:  498/500  Loss:  [0.10203022]\n",
      "Epoch:  499/500  Loss:  [0.10201569]\n",
      "Epoch:  500/500  Loss:  [0.1020012]\n"
     ]
    }
   ],
   "source": [
    "# initialize weights\n",
    "A  = np.random.normal(0, w_std, (hidden_dim, input_dim))\n",
    "a0 = np.random.normal(0, w_std, hidden_dim)\n",
    "b0 = np.random.normal(0, w_std, 1)\n",
    "B  = np.random.normal(0, w_std, hidden_dim)\n",
    "epochs = 500 # number of itrations\n",
    "for epoch in range(epochs):\n",
    "    dSSE_a, dSSE_b, z_bias, y_bias = np.zeros_like(A), np.zeros_like(B), np.zeros_like(B), 0\n",
    "    loss = 0\n",
    "    for i, x in enumerate(x_train):\n",
    "        z = sigmoid(np.dot(A,x)+a0)\n",
    "        y_hat = sigmoid(np.dot(B,z)+b0)\n",
    "        y_error = y_hat - y_train[i]\n",
    "        y_delta = 2* y_error * dsigmoid(np.dot(B, z) + b0)\n",
    "        s = dsigmoid(np.dot(A,x) + a0) * B * y_delta\n",
    "        # print(s.shape)\n",
    "        dSSE_b += y_delta*z\n",
    "        dSSE_a += np.tensordot(s,x, axes=0)\n",
    "        # print(dSSE_a.shape)\n",
    "        y_bias += y_delta\n",
    "        z_bias += s\n",
    "        loss += y_error**2\n",
    "\n",
    "    A  = A - learn_rate * dSSE_a\n",
    "    B  = B - learn_rate * dSSE_b\n",
    "    a0 = a0 - learn_rate * s\n",
    "    b0 = b0 - learn_rate * y_delta\n",
    "\n",
    "    print('Epoch: ', str(epoch+1) + '/'+str(epochs), ' Loss: ', loss/N)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01345305]\n",
      " [0.39295303]\n",
      " [0.13775548]\n",
      " [0.15107429]]\n",
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "def predict(x_test):\n",
    "    results =  [sigmoid(np.dot(B, sigmoid(np.dot(A, x)+a0)) + b0) for x in x_test]\n",
    "    return np.array(results)\n",
    "def decision(x_test):\n",
    "    return (predict(x_test) > 0.5).astype(int)\n",
    "print(predict(x_inputs))\n",
    "print(decision(x_inputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
