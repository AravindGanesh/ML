{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optimizers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "lGCbKXodavJH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Optimizers\n",
        "+ SGD\n",
        "+ Momentum\n",
        "+ Nestrov\n",
        "+ AdaGrad\n",
        "+ RMS Prop\n",
        "+ Adam"
      ]
    },
    {
      "metadata": {
        "id": "oz9GnzAR4o3m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Note:\n",
        "+ this notebook is written in python3\n"
      ]
    },
    {
      "metadata": {
        "id": "OMAL_nL7-fND",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j5844hfnGpmv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Activation functions"
      ]
    },
    {
      "metadata": {
        "id": "IhJKbG0L-QKh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Activations:\n",
        "# utility functions\n",
        "    @staticmethod\n",
        "    def relu(x):\n",
        "        return np.maximum(0,x)\n",
        "\n",
        "    @staticmethod\n",
        "    def d_relu(x):\n",
        "        return np.array(x>=0, dtype=np.float)\n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return (np.tanh(x/2)+1)/2 \n",
        "\n",
        "    @staticmethod\n",
        "    def d_sigmoid(x):\n",
        "        sig = Activations.sigmoid(x)\n",
        "        return sig*(1-sig)\n",
        "    \n",
        "    \n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def d_tanh(x):\n",
        "        tanhx = Activations.tanh(x)\n",
        "        return (1+tanhx)*(1-tanhx)\n",
        "    \n",
        "    @staticmethod\n",
        "    def softmax(x): \n",
        "        ex = np.exp(x)\n",
        "        return (ex.T/np.sum(ex, axis=1)).T\n",
        "    \n",
        "    @staticmethod\n",
        "    def d_softmax(x): # returns 2D array\n",
        "        p = Activations.softmax(x)\n",
        "        return p*(1-p)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bZk98-0o3DgY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Utility Functions \n",
        "+ categorical crossentropy function\n",
        "+ function to convert labels to onehot vectors (also returns the unique labels - useful for the order of classes in the onehot vector)"
      ]
    },
    {
      "metadata": {
        "id": "5pFxsQckB82w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class utils:\n",
        "    @staticmethod\n",
        "    def categorical_crossentropy(y_true, y_pred):\n",
        "        return -np.sum(y_true*np.log(y_pred))\n",
        "    \n",
        "    @staticmethod\n",
        "    def to_categorical(labels, num_classes=None):\n",
        "        y_unique = np.unique(labels)\n",
        "        if num_classes==None: num_classes = len(y_unique)\n",
        "        assert num_classes==len(y_unique), 'num_classes is not same as classes found in labels array'\n",
        "        y_onehot = np.reshape(labels, newshape=(-1,1)) == y_unique\n",
        "        return y_onehot.astype(np.float), y_unique"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EvFq9vyJ3OBf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimizer Algorithms\n",
        "Reference: Chapter 8, DeepLearningBook"
      ]
    },
    {
      "metadata": {
        "id": "dSor5DL3r9sp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class optimizer(object):\n",
        "    def __init__(self): pass\n",
        "    def interim_update(self, weights): return weights \n",
        "    def update(self, weights, gradients): pass\n",
        "\n",
        "######################################################################################################   \n",
        "class SGD(optimizer):\n",
        "    def __init__(self, start_lr, final_lr, decay_iters):\n",
        "        self.name = 'SGD'\n",
        "        self.start_lr = start_lr\n",
        "        self.final_lr = final_lr\n",
        "        self.lr = start_lr\n",
        "        self.decay_iters = decay_iters # iteration number till which learning rate should linearly decay\n",
        "        self.iter_count = 1\n",
        "        super().interim_update\n",
        "    \n",
        "    def update(self, weights, gradients):\n",
        "        new_weights = weights - (self.lr * gradients)\n",
        "        alpha = np.minimum(1, self.iter_count/self.decay_iters)\n",
        "        self.lr = (1-alpha)*self.start_lr + alpha*self.final_lr\n",
        "        self.iter_count += 1\n",
        "        return new_weights\n",
        "\n",
        "######################################################################################################      \n",
        "class SGD_Momentum(optimizer):\n",
        "    def __init__(self, lr, alpha):\n",
        "        self.name = 'sgd_momentum'\n",
        "        self.lr = lr\n",
        "        self.alpha = alpha\n",
        "        self.velocity = 0 #initial velocity\n",
        "        super().interim_update\n",
        "        \n",
        "    def update(self, weights, gradients):\n",
        "        self.velocity = self.alpha*self.velocity - self.lr*gradients\n",
        "        new_weights = weights + self.velocity\n",
        "        return new_weights\n",
        "    \n",
        "######################################################################################################   \n",
        "class SGD_NesterovMomentum(optimizer):\n",
        "    def __init__(self, lr, alpha):\n",
        "        self.name = 'sgd_nesterov_momentum'\n",
        "        self.lr = lr\n",
        "        self.alpha = alpha\n",
        "        self.velocity = 0 #initial velocity\n",
        "        \n",
        "    def interim_update(self, weights):\n",
        "        interim_weights = weights + self.alpha*self.velocity\n",
        "        return interim_weights\n",
        "    \n",
        "    def update(self, weights, gradients):\n",
        "        self.velocity = self.alpha*self.velocity - self.lr*gradients\n",
        "        new_weights = weights + self.velocity\n",
        "        return new_weights\n",
        "######################################################################################################   \n",
        "class AdaGrad(optimizer):\n",
        "    def __init__(self, lr, epsilon=None):\n",
        "        self.name = 'adagrad'\n",
        "        self.lr = lr\n",
        "        self.epsilon = 1e-7 if epsilon==None else epsilon\n",
        "        self.cum_sq_grad = 0\n",
        "        super().interim_update\n",
        "        \n",
        "    def update(self, weights, gradients):\n",
        "        self.cum_sq_grad = self.cum_sq_grad + gradients**2\n",
        "        d_weights = -self.lr*gradients/(self.epsilon + np.sqrt(self.cum_sq_grad))\n",
        "        new_weights = weights + d_weights\n",
        "        return new_weights\n",
        "    \n",
        "######################################################################################################   \n",
        "class RMSProp(optimizer):\n",
        "    def __init__(self, lr, decay_rate,  epsilon=None):\n",
        "        self.name = 'rmsprop'\n",
        "        self.lr = lr\n",
        "        self.decay_rate = decay_rate\n",
        "        self.epsilon = 1e-6 if epsilon==None else epsilon\n",
        "        self.cum_sq_grad = 0\n",
        "        super().interim_update\n",
        "        \n",
        "    def update(self, weights, gradients):\n",
        "        self.cum_sq_grad = self.decay_rate*self.cum_sq_grad + (1-self.decay_rate)*gradients**2\n",
        "        d_weights = -self.lr*gradients/np.sqrt(self.cum_sq_grad + self.epsilon)\n",
        "        new_weights = weights + d_weights\n",
        "        return new_weights\n",
        "\n",
        "######################################################################################################  \n",
        "# reference: Algorithm 8.7 of Deeplearningbook \n",
        "class Adam(optimizer):\n",
        "    def __init__(self, lr, beta1, beta2, epsilon=None):\n",
        "        self.name = 'adam'\n",
        "        self.lr = lr\n",
        "        self.beta1, self.beta2 = beta1, beta2\n",
        "        self.epsilon = 1e-8 if epsilon==None else epsilon #suggested default 1e-8\n",
        "        self.s = self.r = 0 # intialize first and second moments to zero\n",
        "        self.t = 0 # time step\n",
        "        super().interim_update\n",
        "        \n",
        "    def update(self, weights, gradients):\n",
        "        self.t += 1\n",
        "        self.s = self.beta1*self.s + (1-self.beta1)*gradients\n",
        "        self.r = self.beta2*self.r + (1-self.beta2)*(gradients**2)\n",
        "        s_hat = self.s/(1-self.beta1**self.t)\n",
        "        r_hat = self.r/(1-self.beta2**self.t)\n",
        "        d_weights = -self.lr*s_hat/(np.sqrt(r_hat)+self.epsilon)\n",
        "        new_weights = weights + d_weights\n",
        "        return new_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dXMA01e5Grqh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Multilayer Perceptron \n",
        "In this implementation one can define arbitrary number of hidden layers with desired activation functions - gradients are computed with respect to those activation functions but only for crossentropy loss function. "
      ]
    },
    {
      "metadata": {
        "id": "RHF8Qlxf9Q2z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MLP(object):\n",
        "    def __init__(self, dims, activations):\n",
        "        self.dims = dims\n",
        "        self.input_dim = self.dims[0]\n",
        "        self.hidden_dims = self.dims[1:-1]\n",
        "        self.output_dim = self.dims[-1]\n",
        "        self.weights = [np.random.normal(0, 0.5, size=(self.dims[i+1], self.dims[i])) for i in range(len(self.dims)-1)]\n",
        "        self.biases = [np.random.normal(0, 0.05, size=(self.dims[i+1])) for i in range(len(self.dims)-1)]\n",
        "        self.activations = activations\n",
        "        self.d_func = []\n",
        "        for func in self.activations:\n",
        "            if func==Activations.sigmoid: self.d_func.append(Activations.d_sigmoid)\n",
        "            elif func==Activations.relu: self.d_func.append(Activations.d_relu)\n",
        "            elif func==Activations.tanh: self.d_func.append(Activations.d_tanh)\n",
        "            elif func==Activations.softmax: self.d_func.append(Activations.d_softmax)\n",
        "        \n",
        "    def _layer(self, x_batch, W, b, activation=None):\n",
        "        if activation==None: activation= lambda a:a\n",
        "        return activation(np.matmul(x_batch, W.T)+b)\n",
        "    \n",
        "    def forward(self, batch):\n",
        "        z = batch\n",
        "        layers = [z]\n",
        "        act_maps = [z]\n",
        "        for n in range(len(self.hidden_dims)):\n",
        "            z = np.matmul(z, self.weights[n].T) + self.biases[n]\n",
        "            layers.append(z)\n",
        "            z = self.activations[n](z)\n",
        "            act_maps.append(z)\n",
        "        logits = np.matmul(z, self.weights[-1].T) + self.biases[-1]\n",
        "        layers.append(logits)\n",
        "        act_maps.append(self.activations[-1](logits))\n",
        "        return layers, act_maps\n",
        "        \n",
        "    # gradients are comupted for crossentropy loss \n",
        "    def back_propogate(self, batch, layers, act_maps, y_true, weights, biases):\n",
        "        \n",
        "        m = len(batch)\n",
        "        W_grad = [np.zeros_like(w) for w in weights]\n",
        "        bias_grad = [np.zeros_like(b) for b in biases]\n",
        "        \n",
        "        delta = -(y_true/act_maps[-1]) * self.d_func[-1](layers[-1])\n",
        "        \n",
        "        for n in np.arange(1, len(W_grad)+1):\n",
        "            bias_grad[-n] = np.mean(delta, axis=0) / m\n",
        "            W_grad[-n] = np.tensordot(delta, act_maps[-n-1], axes=[0,0]) / m\n",
        "            delta = np.matmul(delta, weights[-n]) * self.d_func[-n](layers[-n-1])\n",
        "        return W_grad, bias_grad\n",
        "    \n",
        "    def train(self, x_train, y_train, epochs, batch_size, optimizer, opt_kwargs, shuffle=True):\n",
        "        epoch = 1\n",
        "        N = len(x_train)\n",
        "        \n",
        "        # optimizers \n",
        "        w_opt, bias_opt = [optimizer(**opt_kwargs) for _ in range(len(self.weights))], [optimizer(**opt_kwargs) for _ in range(len(self.biases))]\n",
        "        \n",
        "        while(epoch <= epochs):\n",
        "            if shuffle:\n",
        "                indices = np.arange(N)\n",
        "                np.random.shuffle(indices)\n",
        "                x_train, y_train = x_train[indices], y_train[indices]\n",
        "            loss = 0\n",
        "            for batch in np.arange(0, N, batch_size):\n",
        "                X, Y = x_train[batch:batch+batch_size], y_train[batch:batch+batch_size]\n",
        "                \n",
        "                layers, act_maps = self.forward(X)\n",
        "                loss += utils.categorical_crossentropy(Y, act_maps[-1])\n",
        "                weights = [w_opt[i].interim_update(self.weights[i]) for i in range(len(self.weights))]\n",
        "                biases = [bias_opt[i].interim_update(self.biases[i]) for i in range(len(self.biases))]\n",
        "                w_gradients, bias_gradients = self.back_propogate(batch=X, layers=layers, act_maps=act_maps, y_true=Y, weights=weights, biases=biases)\n",
        "                self.weights = [w_opt[i].update(weights[i], w_gradients[i]) for i in range(len(self.weights))]\n",
        "                self.biases =  [bias_opt[i].update(biases[i], bias_gradients[i]) for i in range(len(self.biases))]\n",
        "                       \n",
        "            print('Epoch:', epoch, ' Loss:', loss)\n",
        "            epoch += 1\n",
        "        print('Done Training')\n",
        "        \n",
        "    def predict(self, x):\n",
        "        z = x\n",
        "        for n in range(len(self.hidden_dims)):\n",
        "            print(self.weights[n].shape)\n",
        "            z = np.matmul(z, self.weights[n].T) + self.biases[n]\n",
        "            z = self.activations[n](z)\n",
        "        logits = np.matmul(z, self.weights[-1].T) + self.biases[-1]\n",
        "        return self.activations[-1](logits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FvzRZX9fgpv4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and testing"
      ]
    },
    {
      "metadata": {
        "id": "M2tjsRBl36Be",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Dataset \n",
        "Dataset chosen - iris dataset from UCI dataset repository - http://archive.ics.uci.edu/ml/datasets/Iris"
      ]
    },
    {
      "metadata": {
        "id": "aQD836WPgih7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data_dir = 'Iris'\n",
        "df = pd.read_table(os.path.join(data_dir, 'iris.data'), sep=',', header=None)\n",
        "x_data = np.asarray(df.iloc[:, 0:4]).astype(np.float)\n",
        "y_data = np.asarray(df.iloc[:, -1])\n",
        "y_onehot, unique_labels = utils.to_categorical(y_data)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0gsu1IStgj9Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(x_data, y_onehot, test_size=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zCX7tfNR4G8_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training with different optimizers\n",
        "Training of same network with different optimizers "
      ]
    },
    {
      "metadata": {
        "id": "S0QYezC6stMv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SGD"
      ]
    },
    {
      "metadata": {
        "id": "yMqpOiD8mcQR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model1 = MLP(dims=[4, 10, 10, 3], activations = [Activations.sigmoid]*2+[Activations.softmax])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Cs5i5XLRhIGG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9126
        },
        "outputId": "f1bf5f2b-d0c1-46df-ed0f-3ea103f03d85"
      },
      "cell_type": "code",
      "source": [
        "model1.train(x_train=x_train, y_train=y_train, epochs=500, batch_size=1, optimizer=SGD, opt_kwargs=dict(start_lr=0.01, final_lr=0.001, decay_iters=500))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  Loss: 184.3920572684449\n",
            "Epoch: 2  Loss: 158.75679717503942\n",
            "Epoch: 3  Loss: 151.70341203909936\n",
            "Epoch: 4  Loss: 149.9981535627772\n",
            "Epoch: 5  Loss: 149.35691803717154\n",
            "Epoch: 6  Loss: 149.07869660186984\n",
            "Epoch: 7  Loss: 148.7495101754555\n",
            "Epoch: 8  Loss: 148.50396877798866\n",
            "Epoch: 9  Loss: 148.3090703084251\n",
            "Epoch: 10  Loss: 148.07269495348112\n",
            "Epoch: 11  Loss: 147.92906433644353\n",
            "Epoch: 12  Loss: 147.83338598033498\n",
            "Epoch: 13  Loss: 147.7910366344366\n",
            "Epoch: 14  Loss: 147.65218713595428\n",
            "Epoch: 15  Loss: 147.54235261931598\n",
            "Epoch: 16  Loss: 147.49275775062554\n",
            "Epoch: 17  Loss: 147.47926958255175\n",
            "Epoch: 18  Loss: 147.3441336290091\n",
            "Epoch: 19  Loss: 147.3312866402224\n",
            "Epoch: 20  Loss: 147.34488679747537\n",
            "Epoch: 21  Loss: 147.29710773504297\n",
            "Epoch: 22  Loss: 147.27424232308442\n",
            "Epoch: 23  Loss: 147.28109776134048\n",
            "Epoch: 24  Loss: 147.25997010537364\n",
            "Epoch: 25  Loss: 147.29478219509087\n",
            "Epoch: 26  Loss: 147.2451000285618\n",
            "Epoch: 27  Loss: 147.2013882861328\n",
            "Epoch: 28  Loss: 147.22635193610898\n",
            "Epoch: 29  Loss: 147.2254052753496\n",
            "Epoch: 30  Loss: 147.22146711940144\n",
            "Epoch: 31  Loss: 147.24492913581923\n",
            "Epoch: 32  Loss: 147.29305404911838\n",
            "Epoch: 33  Loss: 147.2629145081596\n",
            "Epoch: 34  Loss: 147.22216987214438\n",
            "Epoch: 35  Loss: 147.25457988501\n",
            "Epoch: 36  Loss: 147.29309250019307\n",
            "Epoch: 37  Loss: 147.25060120363324\n",
            "Epoch: 38  Loss: 147.2532819504129\n",
            "Epoch: 39  Loss: 147.23289364616124\n",
            "Epoch: 40  Loss: 147.3121387264553\n",
            "Epoch: 41  Loss: 147.30392894843806\n",
            "Epoch: 42  Loss: 147.3246238309681\n",
            "Epoch: 43  Loss: 147.29257853463926\n",
            "Epoch: 44  Loss: 147.31355154002097\n",
            "Epoch: 45  Loss: 147.33068337436447\n",
            "Epoch: 46  Loss: 147.3560023353174\n",
            "Epoch: 47  Loss: 147.35556614421844\n",
            "Epoch: 48  Loss: 147.31423220365906\n",
            "Epoch: 49  Loss: 147.34901406608026\n",
            "Epoch: 50  Loss: 147.406782586613\n",
            "Epoch: 51  Loss: 147.41374684965737\n",
            "Epoch: 52  Loss: 147.371787097486\n",
            "Epoch: 53  Loss: 147.4159903884968\n",
            "Epoch: 54  Loss: 147.43201124291846\n",
            "Epoch: 55  Loss: 147.42185484710737\n",
            "Epoch: 56  Loss: 147.4192863402086\n",
            "Epoch: 57  Loss: 147.43269946572156\n",
            "Epoch: 58  Loss: 147.46258664863527\n",
            "Epoch: 59  Loss: 147.4907143769736\n",
            "Epoch: 60  Loss: 147.46464327960496\n",
            "Epoch: 61  Loss: 147.435331165805\n",
            "Epoch: 62  Loss: 147.56484312214292\n",
            "Epoch: 63  Loss: 147.48730907991563\n",
            "Epoch: 64  Loss: 147.47058347561628\n",
            "Epoch: 65  Loss: 147.51342670213083\n",
            "Epoch: 66  Loss: 147.54329548205857\n",
            "Epoch: 67  Loss: 147.52016035522146\n",
            "Epoch: 68  Loss: 147.56779939184818\n",
            "Epoch: 69  Loss: 147.54793665711517\n",
            "Epoch: 70  Loss: 147.55977482019918\n",
            "Epoch: 71  Loss: 147.54152379637537\n",
            "Epoch: 72  Loss: 147.58430174723333\n",
            "Epoch: 73  Loss: 147.58846286914286\n",
            "Epoch: 74  Loss: 147.5490047402878\n",
            "Epoch: 75  Loss: 147.59372220819895\n",
            "Epoch: 76  Loss: 147.58397449365376\n",
            "Epoch: 77  Loss: 147.5966483988794\n",
            "Epoch: 78  Loss: 147.57677361300077\n",
            "Epoch: 79  Loss: 147.64765626046227\n",
            "Epoch: 80  Loss: 147.65100688317676\n",
            "Epoch: 81  Loss: 147.61015279509795\n",
            "Epoch: 82  Loss: 147.64203994637938\n",
            "Epoch: 83  Loss: 147.64172241242053\n",
            "Epoch: 84  Loss: 147.66709506002914\n",
            "Epoch: 85  Loss: 147.65593422526644\n",
            "Epoch: 86  Loss: 147.68711933551106\n",
            "Epoch: 87  Loss: 147.6676447672299\n",
            "Epoch: 88  Loss: 147.72038311202763\n",
            "Epoch: 89  Loss: 147.674514939419\n",
            "Epoch: 90  Loss: 147.64540990987325\n",
            "Epoch: 91  Loss: 147.69738437753193\n",
            "Epoch: 92  Loss: 147.69709110174162\n",
            "Epoch: 93  Loss: 147.74638926459795\n",
            "Epoch: 94  Loss: 147.71546404161973\n",
            "Epoch: 95  Loss: 147.7416509819924\n",
            "Epoch: 96  Loss: 147.7400606378021\n",
            "Epoch: 97  Loss: 147.7610539011207\n",
            "Epoch: 98  Loss: 147.7424936663112\n",
            "Epoch: 99  Loss: 147.71447181695018\n",
            "Epoch: 100  Loss: 147.73782589462755\n",
            "Epoch: 101  Loss: 147.78304776657646\n",
            "Epoch: 102  Loss: 147.75751834996808\n",
            "Epoch: 103  Loss: 147.7719255227207\n",
            "Epoch: 104  Loss: 147.79269160255257\n",
            "Epoch: 105  Loss: 147.83135738184646\n",
            "Epoch: 106  Loss: 147.79492436392908\n",
            "Epoch: 107  Loss: 147.8382688242285\n",
            "Epoch: 108  Loss: 147.8181784276048\n",
            "Epoch: 109  Loss: 147.81828592936756\n",
            "Epoch: 110  Loss: 147.82338798747617\n",
            "Epoch: 111  Loss: 147.7819111048222\n",
            "Epoch: 112  Loss: 147.78760788516232\n",
            "Epoch: 113  Loss: 147.80654033619496\n",
            "Epoch: 114  Loss: 147.84330032599797\n",
            "Epoch: 115  Loss: 147.85005556115652\n",
            "Epoch: 116  Loss: 147.83527951100763\n",
            "Epoch: 117  Loss: 147.84773029233088\n",
            "Epoch: 118  Loss: 147.81471669593827\n",
            "Epoch: 119  Loss: 147.83126992502557\n",
            "Epoch: 120  Loss: 147.85681415812553\n",
            "Epoch: 121  Loss: 147.85222399019054\n",
            "Epoch: 122  Loss: 147.87296789917835\n",
            "Epoch: 123  Loss: 147.85423744310847\n",
            "Epoch: 124  Loss: 147.85715898618193\n",
            "Epoch: 125  Loss: 147.8521659304231\n",
            "Epoch: 126  Loss: 147.92711240372836\n",
            "Epoch: 127  Loss: 147.87918681868882\n",
            "Epoch: 128  Loss: 147.8684059702667\n",
            "Epoch: 129  Loss: 147.91744766043266\n",
            "Epoch: 130  Loss: 147.89375715649498\n",
            "Epoch: 131  Loss: 147.87171744129193\n",
            "Epoch: 132  Loss: 147.83116620525402\n",
            "Epoch: 133  Loss: 147.90545674687178\n",
            "Epoch: 134  Loss: 147.8904385942954\n",
            "Epoch: 135  Loss: 147.89254895507773\n",
            "Epoch: 136  Loss: 147.9434962922384\n",
            "Epoch: 137  Loss: 147.9165230377759\n",
            "Epoch: 138  Loss: 147.90732278966883\n",
            "Epoch: 139  Loss: 147.88542659803585\n",
            "Epoch: 140  Loss: 147.92998564852493\n",
            "Epoch: 141  Loss: 147.92377397577684\n",
            "Epoch: 142  Loss: 147.8962498413332\n",
            "Epoch: 143  Loss: 147.91871594731532\n",
            "Epoch: 144  Loss: 147.90834691759548\n",
            "Epoch: 145  Loss: 147.94375846916682\n",
            "Epoch: 146  Loss: 147.91786821239216\n",
            "Epoch: 147  Loss: 147.95100157711553\n",
            "Epoch: 148  Loss: 147.94615754932258\n",
            "Epoch: 149  Loss: 147.97517653930973\n",
            "Epoch: 150  Loss: 147.9411132401708\n",
            "Epoch: 151  Loss: 147.95534057696125\n",
            "Epoch: 152  Loss: 147.92609141082724\n",
            "Epoch: 153  Loss: 147.9352989701102\n",
            "Epoch: 154  Loss: 147.97014989788366\n",
            "Epoch: 155  Loss: 147.91861782096456\n",
            "Epoch: 156  Loss: 147.92187461236674\n",
            "Epoch: 157  Loss: 147.949104313099\n",
            "Epoch: 158  Loss: 147.94938705921666\n",
            "Epoch: 159  Loss: 147.91797901929118\n",
            "Epoch: 160  Loss: 147.95499540178045\n",
            "Epoch: 161  Loss: 147.96910269161765\n",
            "Epoch: 162  Loss: 147.9389171641376\n",
            "Epoch: 163  Loss: 147.9461598520196\n",
            "Epoch: 164  Loss: 147.96975885309658\n",
            "Epoch: 165  Loss: 147.94303293606566\n",
            "Epoch: 166  Loss: 148.0191039248628\n",
            "Epoch: 167  Loss: 147.95327732142823\n",
            "Epoch: 168  Loss: 148.00856869076824\n",
            "Epoch: 169  Loss: 147.95165148157565\n",
            "Epoch: 170  Loss: 148.01211750022568\n",
            "Epoch: 171  Loss: 147.9644226207991\n",
            "Epoch: 172  Loss: 147.92074626423081\n",
            "Epoch: 173  Loss: 147.97481186213696\n",
            "Epoch: 174  Loss: 147.95253553994328\n",
            "Epoch: 175  Loss: 147.96552954554258\n",
            "Epoch: 176  Loss: 147.99715207145965\n",
            "Epoch: 177  Loss: 147.96852404334186\n",
            "Epoch: 178  Loss: 147.93946977095675\n",
            "Epoch: 179  Loss: 147.97819548392852\n",
            "Epoch: 180  Loss: 147.97232014258344\n",
            "Epoch: 181  Loss: 147.94776898308388\n",
            "Epoch: 182  Loss: 148.00848863356438\n",
            "Epoch: 183  Loss: 147.97336048963763\n",
            "Epoch: 184  Loss: 147.95954051696998\n",
            "Epoch: 185  Loss: 147.94702290887034\n",
            "Epoch: 186  Loss: 148.00697393805328\n",
            "Epoch: 187  Loss: 147.98247379541544\n",
            "Epoch: 188  Loss: 147.9779128922269\n",
            "Epoch: 189  Loss: 147.9862618466091\n",
            "Epoch: 190  Loss: 147.95465113476084\n",
            "Epoch: 191  Loss: 148.0124049976489\n",
            "Epoch: 192  Loss: 147.98485555799974\n",
            "Epoch: 193  Loss: 147.99827361026425\n",
            "Epoch: 194  Loss: 147.9732385041443\n",
            "Epoch: 195  Loss: 147.95773387131712\n",
            "Epoch: 196  Loss: 148.00232024733674\n",
            "Epoch: 197  Loss: 148.00445949740714\n",
            "Epoch: 198  Loss: 147.93642145899787\n",
            "Epoch: 199  Loss: 147.98068889945387\n",
            "Epoch: 200  Loss: 148.0037785650774\n",
            "Epoch: 201  Loss: 147.95681640458434\n",
            "Epoch: 202  Loss: 147.9716252502334\n",
            "Epoch: 203  Loss: 147.9949710678403\n",
            "Epoch: 204  Loss: 147.96097308829297\n",
            "Epoch: 205  Loss: 147.93685161336433\n",
            "Epoch: 206  Loss: 147.9561803018661\n",
            "Epoch: 207  Loss: 147.9910847975809\n",
            "Epoch: 208  Loss: 147.98412116929063\n",
            "Epoch: 209  Loss: 147.97278362833478\n",
            "Epoch: 210  Loss: 147.95980468748823\n",
            "Epoch: 211  Loss: 147.99189283031174\n",
            "Epoch: 212  Loss: 147.94946865394837\n",
            "Epoch: 213  Loss: 147.99864241604934\n",
            "Epoch: 214  Loss: 147.95522037606625\n",
            "Epoch: 215  Loss: 148.00649348800934\n",
            "Epoch: 216  Loss: 148.0172252562703\n",
            "Epoch: 217  Loss: 147.9289594243017\n",
            "Epoch: 218  Loss: 147.9824606140873\n",
            "Epoch: 219  Loss: 147.94760495490993\n",
            "Epoch: 220  Loss: 147.9330175788764\n",
            "Epoch: 221  Loss: 147.99471187968967\n",
            "Epoch: 222  Loss: 147.97053849596355\n",
            "Epoch: 223  Loss: 148.00235555523741\n",
            "Epoch: 224  Loss: 147.94746620896106\n",
            "Epoch: 225  Loss: 147.90002946957017\n",
            "Epoch: 226  Loss: 147.97017110305697\n",
            "Epoch: 227  Loss: 147.93349558488265\n",
            "Epoch: 228  Loss: 148.00374686412968\n",
            "Epoch: 229  Loss: 147.9546311388254\n",
            "Epoch: 230  Loss: 147.9905334586006\n",
            "Epoch: 231  Loss: 147.93506746156353\n",
            "Epoch: 232  Loss: 147.94137254459153\n",
            "Epoch: 233  Loss: 147.9826277039988\n",
            "Epoch: 234  Loss: 147.9740625334328\n",
            "Epoch: 235  Loss: 147.9674152698582\n",
            "Epoch: 236  Loss: 147.93871473695773\n",
            "Epoch: 237  Loss: 147.96316446619477\n",
            "Epoch: 238  Loss: 147.95683953242394\n",
            "Epoch: 239  Loss: 147.9546424801776\n",
            "Epoch: 240  Loss: 147.98574946934718\n",
            "Epoch: 241  Loss: 147.93283606753698\n",
            "Epoch: 242  Loss: 147.95301303182268\n",
            "Epoch: 243  Loss: 147.9435336703176\n",
            "Epoch: 244  Loss: 147.96209999429269\n",
            "Epoch: 245  Loss: 147.95019559333122\n",
            "Epoch: 246  Loss: 147.94247125389765\n",
            "Epoch: 247  Loss: 147.93735776499096\n",
            "Epoch: 248  Loss: 147.94230164057805\n",
            "Epoch: 249  Loss: 147.93218910880995\n",
            "Epoch: 250  Loss: 147.94586360706927\n",
            "Epoch: 251  Loss: 147.9574137350809\n",
            "Epoch: 252  Loss: 147.9297628414362\n",
            "Epoch: 253  Loss: 148.00638224990178\n",
            "Epoch: 254  Loss: 147.92877044400205\n",
            "Epoch: 255  Loss: 147.99698085856897\n",
            "Epoch: 256  Loss: 147.94288291368514\n",
            "Epoch: 257  Loss: 147.8802470408676\n",
            "Epoch: 258  Loss: 147.8987286406698\n",
            "Epoch: 259  Loss: 147.92120851798276\n",
            "Epoch: 260  Loss: 147.94037250395604\n",
            "Epoch: 261  Loss: 147.9417315763708\n",
            "Epoch: 262  Loss: 147.89009330693057\n",
            "Epoch: 263  Loss: 147.92606384204154\n",
            "Epoch: 264  Loss: 147.93783837222261\n",
            "Epoch: 265  Loss: 147.9019503869792\n",
            "Epoch: 266  Loss: 147.93216541066684\n",
            "Epoch: 267  Loss: 147.88072826359942\n",
            "Epoch: 268  Loss: 147.94553919958628\n",
            "Epoch: 269  Loss: 147.88739805748816\n",
            "Epoch: 270  Loss: 147.89373645361746\n",
            "Epoch: 271  Loss: 147.93073767576988\n",
            "Epoch: 272  Loss: 147.9119603918207\n",
            "Epoch: 273  Loss: 147.89401365621472\n",
            "Epoch: 274  Loss: 147.90697344074601\n",
            "Epoch: 275  Loss: 147.8481720102706\n",
            "Epoch: 276  Loss: 147.88308293436532\n",
            "Epoch: 277  Loss: 147.90149960758885\n",
            "Epoch: 278  Loss: 147.9398172016542\n",
            "Epoch: 279  Loss: 147.8718561756032\n",
            "Epoch: 280  Loss: 147.89598231749827\n",
            "Epoch: 281  Loss: 147.87406746415812\n",
            "Epoch: 282  Loss: 147.9367619978844\n",
            "Epoch: 283  Loss: 147.84736478413637\n",
            "Epoch: 284  Loss: 147.89149632493792\n",
            "Epoch: 285  Loss: 147.89412240388955\n",
            "Epoch: 286  Loss: 147.88452551575045\n",
            "Epoch: 287  Loss: 147.90078012091524\n",
            "Epoch: 288  Loss: 147.88298690028026\n",
            "Epoch: 289  Loss: 147.89509238713947\n",
            "Epoch: 290  Loss: 147.87049071410178\n",
            "Epoch: 291  Loss: 147.8537493856265\n",
            "Epoch: 292  Loss: 147.83460290385838\n",
            "Epoch: 293  Loss: 147.88465027053226\n",
            "Epoch: 294  Loss: 147.8524040105717\n",
            "Epoch: 295  Loss: 147.84527345033365\n",
            "Epoch: 296  Loss: 147.8734866323747\n",
            "Epoch: 297  Loss: 147.8402426854823\n",
            "Epoch: 298  Loss: 147.83070529624754\n",
            "Epoch: 299  Loss: 147.87161302413548\n",
            "Epoch: 300  Loss: 147.88854567896522\n",
            "Epoch: 301  Loss: 147.8663766795856\n",
            "Epoch: 302  Loss: 147.84474441582836\n",
            "Epoch: 303  Loss: 147.87594854940056\n",
            "Epoch: 304  Loss: 147.84742230270032\n",
            "Epoch: 305  Loss: 147.84868862579677\n",
            "Epoch: 306  Loss: 147.8142502674217\n",
            "Epoch: 307  Loss: 147.77661911298418\n",
            "Epoch: 308  Loss: 147.8017314438343\n",
            "Epoch: 309  Loss: 147.82084861273006\n",
            "Epoch: 310  Loss: 147.80997496699513\n",
            "Epoch: 311  Loss: 147.84760529445327\n",
            "Epoch: 312  Loss: 147.82736590484404\n",
            "Epoch: 313  Loss: 147.79779739237998\n",
            "Epoch: 314  Loss: 147.87033416931993\n",
            "Epoch: 315  Loss: 147.790604679628\n",
            "Epoch: 316  Loss: 147.82469355468584\n",
            "Epoch: 317  Loss: 147.80802283723816\n",
            "Epoch: 318  Loss: 147.81391168149253\n",
            "Epoch: 319  Loss: 147.8234073231794\n",
            "Epoch: 320  Loss: 147.77413519979427\n",
            "Epoch: 321  Loss: 147.76883484531848\n",
            "Epoch: 322  Loss: 147.82122433015567\n",
            "Epoch: 323  Loss: 147.79806337107806\n",
            "Epoch: 324  Loss: 147.7923719875038\n",
            "Epoch: 325  Loss: 147.76135914920292\n",
            "Epoch: 326  Loss: 147.78177406471028\n",
            "Epoch: 327  Loss: 147.7541692679507\n",
            "Epoch: 328  Loss: 147.77840530307859\n",
            "Epoch: 329  Loss: 147.7116137532625\n",
            "Epoch: 330  Loss: 147.7884415316655\n",
            "Epoch: 331  Loss: 147.75175872703568\n",
            "Epoch: 332  Loss: 147.76313859991538\n",
            "Epoch: 333  Loss: 147.74925101431683\n",
            "Epoch: 334  Loss: 147.74687307290068\n",
            "Epoch: 335  Loss: 147.76554077953742\n",
            "Epoch: 336  Loss: 147.7667343854411\n",
            "Epoch: 337  Loss: 147.7716636959829\n",
            "Epoch: 338  Loss: 147.75937462472345\n",
            "Epoch: 339  Loss: 147.77464250916864\n",
            "Epoch: 340  Loss: 147.71585994146255\n",
            "Epoch: 341  Loss: 147.75332698364934\n",
            "Epoch: 342  Loss: 147.7632250076002\n",
            "Epoch: 343  Loss: 147.77088809499267\n",
            "Epoch: 344  Loss: 147.68121752374927\n",
            "Epoch: 345  Loss: 147.7675636787419\n",
            "Epoch: 346  Loss: 147.69739221169542\n",
            "Epoch: 347  Loss: 147.80986728909247\n",
            "Epoch: 348  Loss: 147.76821133269388\n",
            "Epoch: 349  Loss: 147.66639969001665\n",
            "Epoch: 350  Loss: 147.75589207280484\n",
            "Epoch: 351  Loss: 147.67432126352102\n",
            "Epoch: 352  Loss: 147.73174382749653\n",
            "Epoch: 353  Loss: 147.70729628306583\n",
            "Epoch: 354  Loss: 147.66034793829925\n",
            "Epoch: 355  Loss: 147.69067232365498\n",
            "Epoch: 356  Loss: 147.714945792939\n",
            "Epoch: 357  Loss: 147.73815796281784\n",
            "Epoch: 358  Loss: 147.68141649027484\n",
            "Epoch: 359  Loss: 147.6837994132501\n",
            "Epoch: 360  Loss: 147.71648768850838\n",
            "Epoch: 361  Loss: 147.65802752779553\n",
            "Epoch: 362  Loss: 147.69861870421437\n",
            "Epoch: 363  Loss: 147.66195309540643\n",
            "Epoch: 364  Loss: 147.66596098656277\n",
            "Epoch: 365  Loss: 147.69523473462615\n",
            "Epoch: 366  Loss: 147.6960586723876\n",
            "Epoch: 367  Loss: 147.68797600360688\n",
            "Epoch: 368  Loss: 147.6111314995338\n",
            "Epoch: 369  Loss: 147.67672939642554\n",
            "Epoch: 370  Loss: 147.66004907998413\n",
            "Epoch: 371  Loss: 147.67658149867484\n",
            "Epoch: 372  Loss: 147.62057750711236\n",
            "Epoch: 373  Loss: 147.62324379232967\n",
            "Epoch: 374  Loss: 147.63605461863222\n",
            "Epoch: 375  Loss: 147.6526289417476\n",
            "Epoch: 376  Loss: 147.60662280060308\n",
            "Epoch: 377  Loss: 147.67619379230615\n",
            "Epoch: 378  Loss: 147.6366440771847\n",
            "Epoch: 379  Loss: 147.59046203599348\n",
            "Epoch: 380  Loss: 147.56479395447403\n",
            "Epoch: 381  Loss: 147.61128795849334\n",
            "Epoch: 382  Loss: 147.60411592060655\n",
            "Epoch: 383  Loss: 147.62671724174007\n",
            "Epoch: 384  Loss: 147.58089777534448\n",
            "Epoch: 385  Loss: 147.63740026451455\n",
            "Epoch: 386  Loss: 147.64706972505658\n",
            "Epoch: 387  Loss: 147.5726808162315\n",
            "Epoch: 388  Loss: 147.5717719783529\n",
            "Epoch: 389  Loss: 147.59631587044643\n",
            "Epoch: 390  Loss: 147.58342366078827\n",
            "Epoch: 391  Loss: 147.61828509814387\n",
            "Epoch: 392  Loss: 147.56152136301867\n",
            "Epoch: 393  Loss: 147.54201812221257\n",
            "Epoch: 394  Loss: 147.58542226799742\n",
            "Epoch: 395  Loss: 147.56433696026227\n",
            "Epoch: 396  Loss: 147.60943565850897\n",
            "Epoch: 397  Loss: 147.5424547762358\n",
            "Epoch: 398  Loss: 147.53303751965169\n",
            "Epoch: 399  Loss: 147.5591224739051\n",
            "Epoch: 400  Loss: 147.51473133396667\n",
            "Epoch: 401  Loss: 147.59078200341898\n",
            "Epoch: 402  Loss: 147.47460811536388\n",
            "Epoch: 403  Loss: 147.51638594005775\n",
            "Epoch: 404  Loss: 147.51079290079383\n",
            "Epoch: 405  Loss: 147.562533776599\n",
            "Epoch: 406  Loss: 147.49401587828783\n",
            "Epoch: 407  Loss: 147.45743698749226\n",
            "Epoch: 408  Loss: 147.50472060029168\n",
            "Epoch: 409  Loss: 147.5203754455109\n",
            "Epoch: 410  Loss: 147.49051017454318\n",
            "Epoch: 411  Loss: 147.50444406722485\n",
            "Epoch: 412  Loss: 147.4882684115926\n",
            "Epoch: 413  Loss: 147.48858713026547\n",
            "Epoch: 414  Loss: 147.4655627254382\n",
            "Epoch: 415  Loss: 147.43089867649633\n",
            "Epoch: 416  Loss: 147.5325828090565\n",
            "Epoch: 417  Loss: 147.47919302289915\n",
            "Epoch: 418  Loss: 147.47496411703935\n",
            "Epoch: 419  Loss: 147.46812875372936\n",
            "Epoch: 420  Loss: 147.45910470391868\n",
            "Epoch: 421  Loss: 147.51072844778832\n",
            "Epoch: 422  Loss: 147.47635654026934\n",
            "Epoch: 423  Loss: 147.446654325215\n",
            "Epoch: 424  Loss: 147.38353789174576\n",
            "Epoch: 425  Loss: 147.47248024839666\n",
            "Epoch: 426  Loss: 147.42546404680962\n",
            "Epoch: 427  Loss: 147.41199444854843\n",
            "Epoch: 428  Loss: 147.41830262691383\n",
            "Epoch: 429  Loss: 147.40608493393114\n",
            "Epoch: 430  Loss: 147.3933906613597\n",
            "Epoch: 431  Loss: 147.36313471739152\n",
            "Epoch: 432  Loss: 147.43148409499244\n",
            "Epoch: 433  Loss: 147.37780813965108\n",
            "Epoch: 434  Loss: 147.3823162537539\n",
            "Epoch: 435  Loss: 147.42247585178376\n",
            "Epoch: 436  Loss: 147.3638946998909\n",
            "Epoch: 437  Loss: 147.39320578197916\n",
            "Epoch: 438  Loss: 147.4059892406122\n",
            "Epoch: 439  Loss: 147.3698960246857\n",
            "Epoch: 440  Loss: 147.3708830488503\n",
            "Epoch: 441  Loss: 147.3589868831088\n",
            "Epoch: 442  Loss: 147.37843969109818\n",
            "Epoch: 443  Loss: 147.404898910693\n",
            "Epoch: 444  Loss: 147.39000962386086\n",
            "Epoch: 445  Loss: 147.3150066652108\n",
            "Epoch: 446  Loss: 147.3383067958439\n",
            "Epoch: 447  Loss: 147.30755751738823\n",
            "Epoch: 448  Loss: 147.30589544361968\n",
            "Epoch: 449  Loss: 147.3339581744407\n",
            "Epoch: 450  Loss: 147.27255225434743\n",
            "Epoch: 451  Loss: 147.28125193468367\n",
            "Epoch: 452  Loss: 147.30437089325858\n",
            "Epoch: 453  Loss: 147.26678319838928\n",
            "Epoch: 454  Loss: 147.25291143124957\n",
            "Epoch: 455  Loss: 147.27338325616023\n",
            "Epoch: 456  Loss: 147.27738154679662\n",
            "Epoch: 457  Loss: 147.29769332700795\n",
            "Epoch: 458  Loss: 147.24870955110478\n",
            "Epoch: 459  Loss: 147.25532383737044\n",
            "Epoch: 460  Loss: 147.2842212209388\n",
            "Epoch: 461  Loss: 147.292960635494\n",
            "Epoch: 462  Loss: 147.27939142387305\n",
            "Epoch: 463  Loss: 147.27027878187178\n",
            "Epoch: 464  Loss: 147.22622673463638\n",
            "Epoch: 465  Loss: 147.1838299176365\n",
            "Epoch: 466  Loss: 147.14182700639148\n",
            "Epoch: 467  Loss: 147.23051670923948\n",
            "Epoch: 468  Loss: 147.2182627454663\n",
            "Epoch: 469  Loss: 147.2330203420502\n",
            "Epoch: 470  Loss: 147.2324647257325\n",
            "Epoch: 471  Loss: 147.19165854008054\n",
            "Epoch: 472  Loss: 147.11068532915755\n",
            "Epoch: 473  Loss: 147.23128563956794\n",
            "Epoch: 474  Loss: 147.2357161905015\n",
            "Epoch: 475  Loss: 147.20288647223802\n",
            "Epoch: 476  Loss: 147.17396800539794\n",
            "Epoch: 477  Loss: 147.14866760207337\n",
            "Epoch: 478  Loss: 147.16111185230747\n",
            "Epoch: 479  Loss: 147.15761260006767\n",
            "Epoch: 480  Loss: 147.10824467539945\n",
            "Epoch: 481  Loss: 147.13130304864583\n",
            "Epoch: 482  Loss: 147.05564319409544\n",
            "Epoch: 483  Loss: 147.09603582465226\n",
            "Epoch: 484  Loss: 147.1637292327367\n",
            "Epoch: 485  Loss: 147.05310384229801\n",
            "Epoch: 486  Loss: 147.1530707307411\n",
            "Epoch: 487  Loss: 147.18010559997632\n",
            "Epoch: 488  Loss: 147.06286910178812\n",
            "Epoch: 489  Loss: 147.08891076876955\n",
            "Epoch: 490  Loss: 147.14292728227926\n",
            "Epoch: 491  Loss: 147.01747402090362\n",
            "Epoch: 492  Loss: 147.05273496107048\n",
            "Epoch: 493  Loss: 147.031653133692\n",
            "Epoch: 494  Loss: 147.1194089322861\n",
            "Epoch: 495  Loss: 147.06106012311523\n",
            "Epoch: 496  Loss: 147.05380951397558\n",
            "Epoch: 497  Loss: 147.05275994803475\n",
            "Epoch: 498  Loss: 147.01861977950054\n",
            "Epoch: 499  Loss: 147.0596640773207\n",
            "Epoch: 500  Loss: 146.96529344175582\n",
            "Done Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mhkQaZI1xn1W",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SGD Momentum"
      ]
    },
    {
      "metadata": {
        "id": "laCpV7ArxqJz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model2 = MLP(dims=[4, 10, 10, 3], activations = [Activations.sigmoid]*2+[Activations.softmax])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a8wIxRgwx1R3",
        "colab_type": "code",
        "outputId": "2e93f754-7246-4b61-b902-c52cd5e1ab6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9126
        }
      },
      "cell_type": "code",
      "source": [
        "model2.train(x_train=x_train, y_train=y_train, epochs=500, batch_size=10, optimizer=SGD_Momentum, opt_kwargs=dict(lr=0.001, alpha=0.9))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  Loss: 165.83219722578602\n",
            "Epoch: 2  Loss: 164.13202673298537\n",
            "Epoch: 3  Loss: 162.1595235088416\n",
            "Epoch: 4  Loss: 160.30915289286173\n",
            "Epoch: 5  Loss: 158.71538122696666\n",
            "Epoch: 6  Loss: 157.34061098856264\n",
            "Epoch: 7  Loss: 156.0001579054844\n",
            "Epoch: 8  Loss: 154.81985364512218\n",
            "Epoch: 9  Loss: 153.97098042640593\n",
            "Epoch: 10  Loss: 153.1765048207699\n",
            "Epoch: 11  Loss: 152.44832511407972\n",
            "Epoch: 12  Loss: 151.86905777591866\n",
            "Epoch: 13  Loss: 151.3557174969172\n",
            "Epoch: 14  Loss: 150.92714554458763\n",
            "Epoch: 15  Loss: 150.52287962587937\n",
            "Epoch: 16  Loss: 150.0892389756402\n",
            "Epoch: 17  Loss: 149.792239423721\n",
            "Epoch: 18  Loss: 149.57093281925418\n",
            "Epoch: 19  Loss: 149.35830412020638\n",
            "Epoch: 20  Loss: 149.13819849089188\n",
            "Epoch: 21  Loss: 149.04387593875393\n",
            "Epoch: 22  Loss: 148.91568504845114\n",
            "Epoch: 23  Loss: 148.84772090796892\n",
            "Epoch: 24  Loss: 148.83495706914778\n",
            "Epoch: 25  Loss: 148.80218112162618\n",
            "Epoch: 26  Loss: 148.73504793912014\n",
            "Epoch: 27  Loss: 148.63544387896448\n",
            "Epoch: 28  Loss: 148.56200444353118\n",
            "Epoch: 29  Loss: 148.48390888756148\n",
            "Epoch: 30  Loss: 148.43508782080403\n",
            "Epoch: 31  Loss: 148.35420498336848\n",
            "Epoch: 32  Loss: 148.26081568079945\n",
            "Epoch: 33  Loss: 148.22835729749258\n",
            "Epoch: 34  Loss: 148.21622605187247\n",
            "Epoch: 35  Loss: 148.10520567698035\n",
            "Epoch: 36  Loss: 148.00555654886395\n",
            "Epoch: 37  Loss: 147.95744316271467\n",
            "Epoch: 38  Loss: 147.8503479971493\n",
            "Epoch: 39  Loss: 147.7788417736717\n",
            "Epoch: 40  Loss: 147.75120425724268\n",
            "Epoch: 41  Loss: 147.81483325090485\n",
            "Epoch: 42  Loss: 147.72111649959754\n",
            "Epoch: 43  Loss: 147.6756842873412\n",
            "Epoch: 44  Loss: 147.59243947483088\n",
            "Epoch: 45  Loss: 147.63294523982123\n",
            "Epoch: 46  Loss: 147.51469801407626\n",
            "Epoch: 47  Loss: 147.4761284055525\n",
            "Epoch: 48  Loss: 147.40477940693242\n",
            "Epoch: 49  Loss: 147.37834137210336\n",
            "Epoch: 50  Loss: 147.36557833490562\n",
            "Epoch: 51  Loss: 147.28495293059723\n",
            "Epoch: 52  Loss: 147.2903820189689\n",
            "Epoch: 53  Loss: 147.32000241537585\n",
            "Epoch: 54  Loss: 147.2846480443541\n",
            "Epoch: 55  Loss: 147.21853898177574\n",
            "Epoch: 56  Loss: 147.1943944431016\n",
            "Epoch: 57  Loss: 147.17636836301307\n",
            "Epoch: 58  Loss: 147.1587464427467\n",
            "Epoch: 59  Loss: 147.08912569369417\n",
            "Epoch: 60  Loss: 147.1000675453139\n",
            "Epoch: 61  Loss: 147.14964431946453\n",
            "Epoch: 62  Loss: 147.12878302300842\n",
            "Epoch: 63  Loss: 147.16334953571328\n",
            "Epoch: 64  Loss: 147.12896638986945\n",
            "Epoch: 65  Loss: 147.1375225553656\n",
            "Epoch: 66  Loss: 147.13406670837097\n",
            "Epoch: 67  Loss: 147.11496014003512\n",
            "Epoch: 68  Loss: 147.1366507053918\n",
            "Epoch: 69  Loss: 147.2381434507211\n",
            "Epoch: 70  Loss: 147.18625628927575\n",
            "Epoch: 71  Loss: 147.14887914999275\n",
            "Epoch: 72  Loss: 147.12160428212667\n",
            "Epoch: 73  Loss: 147.15928418968318\n",
            "Epoch: 74  Loss: 147.12260974710765\n",
            "Epoch: 75  Loss: 147.17926585898408\n",
            "Epoch: 76  Loss: 147.06639095822223\n",
            "Epoch: 77  Loss: 147.1219807599153\n",
            "Epoch: 78  Loss: 147.12829496011577\n",
            "Epoch: 79  Loss: 147.13475510622166\n",
            "Epoch: 80  Loss: 147.15726463019422\n",
            "Epoch: 81  Loss: 147.1599668521474\n",
            "Epoch: 82  Loss: 147.21961896173264\n",
            "Epoch: 83  Loss: 147.14257519048255\n",
            "Epoch: 84  Loss: 147.09193241890142\n",
            "Epoch: 85  Loss: 147.1171909332143\n",
            "Epoch: 86  Loss: 147.0948615552805\n",
            "Epoch: 87  Loss: 147.04574054900507\n",
            "Epoch: 88  Loss: 147.08430977491156\n",
            "Epoch: 89  Loss: 147.06266846433286\n",
            "Epoch: 90  Loss: 147.0247887240527\n",
            "Epoch: 91  Loss: 147.02768778414153\n",
            "Epoch: 92  Loss: 147.0627029687032\n",
            "Epoch: 93  Loss: 147.06750037609928\n",
            "Epoch: 94  Loss: 147.0829700309671\n",
            "Epoch: 95  Loss: 147.10196598212346\n",
            "Epoch: 96  Loss: 147.10390536502626\n",
            "Epoch: 97  Loss: 147.07933083011858\n",
            "Epoch: 98  Loss: 147.13227034401137\n",
            "Epoch: 99  Loss: 147.03787475769101\n",
            "Epoch: 100  Loss: 147.04952267172067\n",
            "Epoch: 101  Loss: 147.0799239940466\n",
            "Epoch: 102  Loss: 147.03681321114198\n",
            "Epoch: 103  Loss: 147.04467470017806\n",
            "Epoch: 104  Loss: 147.06578455985635\n",
            "Epoch: 105  Loss: 147.02037745859747\n",
            "Epoch: 106  Loss: 147.10731834832964\n",
            "Epoch: 107  Loss: 147.1047932215191\n",
            "Epoch: 108  Loss: 147.0409094982139\n",
            "Epoch: 109  Loss: 147.0566012722193\n",
            "Epoch: 110  Loss: 147.0641727076388\n",
            "Epoch: 111  Loss: 147.13589987768697\n",
            "Epoch: 112  Loss: 147.12064896496142\n",
            "Epoch: 113  Loss: 147.0700840580083\n",
            "Epoch: 114  Loss: 147.1196745298684\n",
            "Epoch: 115  Loss: 147.18514818972923\n",
            "Epoch: 116  Loss: 147.10770341469618\n",
            "Epoch: 117  Loss: 147.08512926706666\n",
            "Epoch: 118  Loss: 147.03566527035514\n",
            "Epoch: 119  Loss: 147.0345957403611\n",
            "Epoch: 120  Loss: 147.0472146848591\n",
            "Epoch: 121  Loss: 147.06105267151105\n",
            "Epoch: 122  Loss: 147.0878670473467\n",
            "Epoch: 123  Loss: 147.0777678779761\n",
            "Epoch: 124  Loss: 147.0421349862007\n",
            "Epoch: 125  Loss: 147.04463190461544\n",
            "Epoch: 126  Loss: 147.1410573967318\n",
            "Epoch: 127  Loss: 147.119150520441\n",
            "Epoch: 128  Loss: 147.05601980208053\n",
            "Epoch: 129  Loss: 147.05784429761917\n",
            "Epoch: 130  Loss: 147.0453484635468\n",
            "Epoch: 131  Loss: 147.05468086249581\n",
            "Epoch: 132  Loss: 147.04384170945346\n",
            "Epoch: 133  Loss: 147.00136223245514\n",
            "Epoch: 134  Loss: 146.97026439849935\n",
            "Epoch: 135  Loss: 146.93640566527586\n",
            "Epoch: 136  Loss: 146.94115774873907\n",
            "Epoch: 137  Loss: 146.94781899806605\n",
            "Epoch: 138  Loss: 146.92942772037935\n",
            "Epoch: 139  Loss: 146.93475221663417\n",
            "Epoch: 140  Loss: 146.90295202592532\n",
            "Epoch: 141  Loss: 146.96023372136037\n",
            "Epoch: 142  Loss: 146.97312432226244\n",
            "Epoch: 143  Loss: 146.9363519799315\n",
            "Epoch: 144  Loss: 146.95274602182874\n",
            "Epoch: 145  Loss: 146.9978124228339\n",
            "Epoch: 146  Loss: 146.91178383484763\n",
            "Epoch: 147  Loss: 146.8858166639991\n",
            "Epoch: 148  Loss: 146.89648570235758\n",
            "Epoch: 149  Loss: 146.89253882648745\n",
            "Epoch: 150  Loss: 146.86508812092836\n",
            "Epoch: 151  Loss: 146.93611941740147\n",
            "Epoch: 152  Loss: 146.88489963853084\n",
            "Epoch: 153  Loss: 146.8816200852075\n",
            "Epoch: 154  Loss: 146.93205739923343\n",
            "Epoch: 155  Loss: 146.870289434951\n",
            "Epoch: 156  Loss: 146.82658368487924\n",
            "Epoch: 157  Loss: 146.83823020508993\n",
            "Epoch: 158  Loss: 146.82885376081714\n",
            "Epoch: 159  Loss: 146.78133666427823\n",
            "Epoch: 160  Loss: 146.8438346117007\n",
            "Epoch: 161  Loss: 146.78510306463644\n",
            "Epoch: 162  Loss: 146.7252048031665\n",
            "Epoch: 163  Loss: 146.70200882773736\n",
            "Epoch: 164  Loss: 146.73228815634525\n",
            "Epoch: 165  Loss: 146.69491522776758\n",
            "Epoch: 166  Loss: 146.67239705521345\n",
            "Epoch: 167  Loss: 146.6554885125126\n",
            "Epoch: 168  Loss: 146.65674369765534\n",
            "Epoch: 169  Loss: 146.62943209600223\n",
            "Epoch: 170  Loss: 146.6115378337272\n",
            "Epoch: 171  Loss: 146.60150970145423\n",
            "Epoch: 172  Loss: 146.580028560736\n",
            "Epoch: 173  Loss: 146.58498068517136\n",
            "Epoch: 174  Loss: 146.6439763463846\n",
            "Epoch: 175  Loss: 146.52086360559605\n",
            "Epoch: 176  Loss: 146.5297616820038\n",
            "Epoch: 177  Loss: 146.5033527899928\n",
            "Epoch: 178  Loss: 146.50576783339037\n",
            "Epoch: 179  Loss: 146.42821077920047\n",
            "Epoch: 180  Loss: 146.41837654088087\n",
            "Epoch: 181  Loss: 146.3685644683543\n",
            "Epoch: 182  Loss: 146.34386312219368\n",
            "Epoch: 183  Loss: 146.3071167760874\n",
            "Epoch: 184  Loss: 146.30984335371625\n",
            "Epoch: 185  Loss: 146.26887233543948\n",
            "Epoch: 186  Loss: 146.23779118306544\n",
            "Epoch: 187  Loss: 146.24214621105918\n",
            "Epoch: 188  Loss: 146.18493845233303\n",
            "Epoch: 189  Loss: 146.17846558250966\n",
            "Epoch: 190  Loss: 146.14708390314473\n",
            "Epoch: 191  Loss: 146.14596132174512\n",
            "Epoch: 192  Loss: 146.13939942522248\n",
            "Epoch: 193  Loss: 146.1225502160968\n",
            "Epoch: 194  Loss: 146.08056177095978\n",
            "Epoch: 195  Loss: 146.0732035509772\n",
            "Epoch: 196  Loss: 146.02981466590734\n",
            "Epoch: 197  Loss: 145.98542815225707\n",
            "Epoch: 198  Loss: 146.01789335259215\n",
            "Epoch: 199  Loss: 146.01008916010642\n",
            "Epoch: 200  Loss: 145.93769464504336\n",
            "Epoch: 201  Loss: 145.870423009039\n",
            "Epoch: 202  Loss: 145.8463342631387\n",
            "Epoch: 203  Loss: 145.84516450883928\n",
            "Epoch: 204  Loss: 145.76056009527716\n",
            "Epoch: 205  Loss: 145.77837987283345\n",
            "Epoch: 206  Loss: 145.72748251311816\n",
            "Epoch: 207  Loss: 145.6833817523644\n",
            "Epoch: 208  Loss: 145.6341272970923\n",
            "Epoch: 209  Loss: 145.58925495008842\n",
            "Epoch: 210  Loss: 145.58536538333814\n",
            "Epoch: 211  Loss: 145.5070870177239\n",
            "Epoch: 212  Loss: 145.45379033752283\n",
            "Epoch: 213  Loss: 145.50911863158674\n",
            "Epoch: 214  Loss: 145.43716184238022\n",
            "Epoch: 215  Loss: 145.37800634041452\n",
            "Epoch: 216  Loss: 145.36878117459366\n",
            "Epoch: 217  Loss: 145.32805183919646\n",
            "Epoch: 218  Loss: 145.36387526146243\n",
            "Epoch: 219  Loss: 145.28573461299322\n",
            "Epoch: 220  Loss: 145.28822570504235\n",
            "Epoch: 221  Loss: 145.29015215071084\n",
            "Epoch: 222  Loss: 145.22568325443234\n",
            "Epoch: 223  Loss: 145.1775573296165\n",
            "Epoch: 224  Loss: 145.22041883805582\n",
            "Epoch: 225  Loss: 145.14821590512835\n",
            "Epoch: 226  Loss: 145.16106535239444\n",
            "Epoch: 227  Loss: 145.14710316812443\n",
            "Epoch: 228  Loss: 145.09057710403474\n",
            "Epoch: 229  Loss: 145.0495789248358\n",
            "Epoch: 230  Loss: 144.99570880513554\n",
            "Epoch: 231  Loss: 145.07262467845663\n",
            "Epoch: 232  Loss: 145.10558995087712\n",
            "Epoch: 233  Loss: 145.03773354242594\n",
            "Epoch: 234  Loss: 144.94908789800868\n",
            "Epoch: 235  Loss: 144.90817392778706\n",
            "Epoch: 236  Loss: 144.87980438820776\n",
            "Epoch: 237  Loss: 144.85742352690525\n",
            "Epoch: 238  Loss: 144.81067617957285\n",
            "Epoch: 239  Loss: 144.82697482418277\n",
            "Epoch: 240  Loss: 144.77569239555254\n",
            "Epoch: 241  Loss: 144.75918274562835\n",
            "Epoch: 242  Loss: 144.74128524709346\n",
            "Epoch: 243  Loss: 144.71500251922626\n",
            "Epoch: 244  Loss: 144.69740960001624\n",
            "Epoch: 245  Loss: 144.64093496022582\n",
            "Epoch: 246  Loss: 144.62628280667812\n",
            "Epoch: 247  Loss: 144.5558383374842\n",
            "Epoch: 248  Loss: 144.52611715473154\n",
            "Epoch: 249  Loss: 144.462644234339\n",
            "Epoch: 250  Loss: 144.40546084128522\n",
            "Epoch: 251  Loss: 144.39242302275306\n",
            "Epoch: 252  Loss: 144.40461895513224\n",
            "Epoch: 253  Loss: 144.36340293434088\n",
            "Epoch: 254  Loss: 144.39631741364363\n",
            "Epoch: 255  Loss: 144.34424192969857\n",
            "Epoch: 256  Loss: 144.3169721614843\n",
            "Epoch: 257  Loss: 144.34195955720836\n",
            "Epoch: 258  Loss: 144.28829518070657\n",
            "Epoch: 259  Loss: 144.2444461357389\n",
            "Epoch: 260  Loss: 144.22670880260745\n",
            "Epoch: 261  Loss: 144.18032936799835\n",
            "Epoch: 262  Loss: 144.1909184942423\n",
            "Epoch: 263  Loss: 144.1271989312346\n",
            "Epoch: 264  Loss: 144.10501856143878\n",
            "Epoch: 265  Loss: 144.09010457797967\n",
            "Epoch: 266  Loss: 144.05539201311404\n",
            "Epoch: 267  Loss: 144.02451373773988\n",
            "Epoch: 268  Loss: 144.04726740964273\n",
            "Epoch: 269  Loss: 143.98315647348983\n",
            "Epoch: 270  Loss: 143.9742907484671\n",
            "Epoch: 271  Loss: 144.03272868103622\n",
            "Epoch: 272  Loss: 143.95141942242452\n",
            "Epoch: 273  Loss: 144.0195011651232\n",
            "Epoch: 274  Loss: 143.93510713539058\n",
            "Epoch: 275  Loss: 143.86756296935448\n",
            "Epoch: 276  Loss: 143.8501199144196\n",
            "Epoch: 277  Loss: 143.79762383469458\n",
            "Epoch: 278  Loss: 143.72857115113246\n",
            "Epoch: 279  Loss: 143.7432920146238\n",
            "Epoch: 280  Loss: 143.76859273658272\n",
            "Epoch: 281  Loss: 143.69960824041536\n",
            "Epoch: 282  Loss: 143.73441960928548\n",
            "Epoch: 283  Loss: 143.61429748447662\n",
            "Epoch: 284  Loss: 143.55784821668456\n",
            "Epoch: 285  Loss: 143.62122661778545\n",
            "Epoch: 286  Loss: 143.5035254857265\n",
            "Epoch: 287  Loss: 143.48838801783535\n",
            "Epoch: 288  Loss: 143.51646762682088\n",
            "Epoch: 289  Loss: 143.47593465099595\n",
            "Epoch: 290  Loss: 143.44999697857145\n",
            "Epoch: 291  Loss: 143.43276698625917\n",
            "Epoch: 292  Loss: 143.42611140277037\n",
            "Epoch: 293  Loss: 143.3233244702886\n",
            "Epoch: 294  Loss: 143.28274131357253\n",
            "Epoch: 295  Loss: 143.4096064579352\n",
            "Epoch: 296  Loss: 143.26274439774235\n",
            "Epoch: 297  Loss: 143.18190271291385\n",
            "Epoch: 298  Loss: 143.13268936288696\n",
            "Epoch: 299  Loss: 143.18306977017662\n",
            "Epoch: 300  Loss: 143.10878899534708\n",
            "Epoch: 301  Loss: 143.08922031007398\n",
            "Epoch: 302  Loss: 143.086529419679\n",
            "Epoch: 303  Loss: 143.043719750075\n",
            "Epoch: 304  Loss: 143.0251555048319\n",
            "Epoch: 305  Loss: 142.99114069113512\n",
            "Epoch: 306  Loss: 142.96676253799282\n",
            "Epoch: 307  Loss: 142.94243740701407\n",
            "Epoch: 308  Loss: 142.89436181868535\n",
            "Epoch: 309  Loss: 142.88913814949854\n",
            "Epoch: 310  Loss: 142.89827180456282\n",
            "Epoch: 311  Loss: 142.81600333601915\n",
            "Epoch: 312  Loss: 142.79841813811535\n",
            "Epoch: 313  Loss: 142.78919868011025\n",
            "Epoch: 314  Loss: 142.80710120425667\n",
            "Epoch: 315  Loss: 142.68868538855253\n",
            "Epoch: 316  Loss: 142.662204255295\n",
            "Epoch: 317  Loss: 142.71024278420148\n",
            "Epoch: 318  Loss: 142.63506985666126\n",
            "Epoch: 319  Loss: 142.70764193983797\n",
            "Epoch: 320  Loss: 142.6896441703464\n",
            "Epoch: 321  Loss: 142.59299015758802\n",
            "Epoch: 322  Loss: 142.56323634009613\n",
            "Epoch: 323  Loss: 142.51203097012967\n",
            "Epoch: 324  Loss: 142.49850624679405\n",
            "Epoch: 325  Loss: 142.43350237227622\n",
            "Epoch: 326  Loss: 142.53337803528876\n",
            "Epoch: 327  Loss: 142.50494595180555\n",
            "Epoch: 328  Loss: 142.4645754498782\n",
            "Epoch: 329  Loss: 142.44433336649487\n",
            "Epoch: 330  Loss: 142.41261298896387\n",
            "Epoch: 331  Loss: 142.39346390825006\n",
            "Epoch: 332  Loss: 142.37641534201765\n",
            "Epoch: 333  Loss: 142.42708068123108\n",
            "Epoch: 334  Loss: 142.41005697367927\n",
            "Epoch: 335  Loss: 142.38150514152298\n",
            "Epoch: 336  Loss: 142.35762350745037\n",
            "Epoch: 337  Loss: 142.33358686998287\n",
            "Epoch: 338  Loss: 142.33143226113492\n",
            "Epoch: 339  Loss: 142.39981417818396\n",
            "Epoch: 340  Loss: 142.3392338794212\n",
            "Epoch: 341  Loss: 142.3160665003783\n",
            "Epoch: 342  Loss: 142.2809543754163\n",
            "Epoch: 343  Loss: 142.32228063106035\n",
            "Epoch: 344  Loss: 142.28469286319353\n",
            "Epoch: 345  Loss: 142.2528465858282\n",
            "Epoch: 346  Loss: 142.22801594165398\n",
            "Epoch: 347  Loss: 142.38708007554624\n",
            "Epoch: 348  Loss: 142.24270594105255\n",
            "Epoch: 349  Loss: 142.2039412758325\n",
            "Epoch: 350  Loss: 142.20366485786067\n",
            "Epoch: 351  Loss: 142.16964640929282\n",
            "Epoch: 352  Loss: 142.20608418861414\n",
            "Epoch: 353  Loss: 142.25295280861482\n",
            "Epoch: 354  Loss: 142.23667288952626\n",
            "Epoch: 355  Loss: 142.28423955810993\n",
            "Epoch: 356  Loss: 142.25137649770556\n",
            "Epoch: 357  Loss: 142.2850685818721\n",
            "Epoch: 358  Loss: 142.30528805276654\n",
            "Epoch: 359  Loss: 142.31179844741087\n",
            "Epoch: 360  Loss: 142.29526932771873\n",
            "Epoch: 361  Loss: 142.32848134361927\n",
            "Epoch: 362  Loss: 142.40239431239903\n",
            "Epoch: 363  Loss: 142.3866208566298\n",
            "Epoch: 364  Loss: 142.33946029470098\n",
            "Epoch: 365  Loss: 142.42207479784494\n",
            "Epoch: 366  Loss: 142.44063268523337\n",
            "Epoch: 367  Loss: 142.5036300856874\n",
            "Epoch: 368  Loss: 142.5662293412794\n",
            "Epoch: 369  Loss: 142.58383073055018\n",
            "Epoch: 370  Loss: 142.62856430834037\n",
            "Epoch: 371  Loss: 142.6684809564362\n",
            "Epoch: 372  Loss: 142.7108938304883\n",
            "Epoch: 373  Loss: 142.79178030663985\n",
            "Epoch: 374  Loss: 142.83894536065137\n",
            "Epoch: 375  Loss: 142.8669987486717\n",
            "Epoch: 376  Loss: 142.89379284851893\n",
            "Epoch: 377  Loss: 142.92884944938064\n",
            "Epoch: 378  Loss: 142.9913849537174\n",
            "Epoch: 379  Loss: 143.10209838493634\n",
            "Epoch: 380  Loss: 143.11568086385077\n",
            "Epoch: 381  Loss: 143.16465257976253\n",
            "Epoch: 382  Loss: 143.30800590748052\n",
            "Epoch: 383  Loss: 143.3658154599416\n",
            "Epoch: 384  Loss: 143.4250051404697\n",
            "Epoch: 385  Loss: 143.48583175741464\n",
            "Epoch: 386  Loss: 143.56717828033172\n",
            "Epoch: 387  Loss: 143.7006535494543\n",
            "Epoch: 388  Loss: 143.67798356150894\n",
            "Epoch: 389  Loss: 143.75790922994312\n",
            "Epoch: 390  Loss: 143.84855250588035\n",
            "Epoch: 391  Loss: 143.95829097519146\n",
            "Epoch: 392  Loss: 144.0575166890205\n",
            "Epoch: 393  Loss: 144.1266321136001\n",
            "Epoch: 394  Loss: 144.20726127648933\n",
            "Epoch: 395  Loss: 144.3251380877208\n",
            "Epoch: 396  Loss: 144.49627108903522\n",
            "Epoch: 397  Loss: 144.4905997026138\n",
            "Epoch: 398  Loss: 144.57132143845124\n",
            "Epoch: 399  Loss: 144.65755417751015\n",
            "Epoch: 400  Loss: 144.74731333152917\n",
            "Epoch: 401  Loss: 144.78360729705705\n",
            "Epoch: 402  Loss: 144.85882356679673\n",
            "Epoch: 403  Loss: 144.92217596602532\n",
            "Epoch: 404  Loss: 144.99384331441075\n",
            "Epoch: 405  Loss: 145.0580919717996\n",
            "Epoch: 406  Loss: 145.13888118875914\n",
            "Epoch: 407  Loss: 145.2425557973472\n",
            "Epoch: 408  Loss: 145.31335870232888\n",
            "Epoch: 409  Loss: 145.39199800286775\n",
            "Epoch: 410  Loss: 145.47586669212888\n",
            "Epoch: 411  Loss: 145.48314919143644\n",
            "Epoch: 412  Loss: 145.5165820629717\n",
            "Epoch: 413  Loss: 145.59958247488242\n",
            "Epoch: 414  Loss: 145.6222458637999\n",
            "Epoch: 415  Loss: 145.69984228069535\n",
            "Epoch: 416  Loss: 145.79365733902827\n",
            "Epoch: 417  Loss: 145.85241109029494\n",
            "Epoch: 418  Loss: 145.8446561475091\n",
            "Epoch: 419  Loss: 145.8506106657428\n",
            "Epoch: 420  Loss: 145.89141538127282\n",
            "Epoch: 421  Loss: 145.96337666891606\n",
            "Epoch: 422  Loss: 145.98250162353665\n",
            "Epoch: 423  Loss: 146.00889776952883\n",
            "Epoch: 424  Loss: 146.09806559179196\n",
            "Epoch: 425  Loss: 146.1327178273954\n",
            "Epoch: 426  Loss: 146.14836128361185\n",
            "Epoch: 427  Loss: 146.1720767554041\n",
            "Epoch: 428  Loss: 146.292769145895\n",
            "Epoch: 429  Loss: 146.33334529641826\n",
            "Epoch: 430  Loss: 146.26414084953538\n",
            "Epoch: 431  Loss: 146.28675599895126\n",
            "Epoch: 432  Loss: 146.35390781763277\n",
            "Epoch: 433  Loss: 146.38151985170197\n",
            "Epoch: 434  Loss: 146.4093734385996\n",
            "Epoch: 435  Loss: 146.4395993429615\n",
            "Epoch: 436  Loss: 146.45431055002368\n",
            "Epoch: 437  Loss: 146.53698190481333\n",
            "Epoch: 438  Loss: 146.5676366208639\n",
            "Epoch: 439  Loss: 146.57088569269706\n",
            "Epoch: 440  Loss: 146.65355489213044\n",
            "Epoch: 441  Loss: 146.68842861445444\n",
            "Epoch: 442  Loss: 146.6872517020012\n",
            "Epoch: 443  Loss: 146.69190296172633\n",
            "Epoch: 444  Loss: 146.77681339937573\n",
            "Epoch: 445  Loss: 146.82782827787048\n",
            "Epoch: 446  Loss: 146.8325268187529\n",
            "Epoch: 447  Loss: 146.8247740147898\n",
            "Epoch: 448  Loss: 146.8437328343957\n",
            "Epoch: 449  Loss: 146.87434206316513\n",
            "Epoch: 450  Loss: 146.82679418559448\n",
            "Epoch: 451  Loss: 146.80062397594187\n",
            "Epoch: 452  Loss: 146.81915460422837\n",
            "Epoch: 453  Loss: 146.80576694592258\n",
            "Epoch: 454  Loss: 146.8491800753077\n",
            "Epoch: 455  Loss: 146.8082335743186\n",
            "Epoch: 456  Loss: 146.8028155976741\n",
            "Epoch: 457  Loss: 146.83304220786079\n",
            "Epoch: 458  Loss: 146.87010343322052\n",
            "Epoch: 459  Loss: 146.8397201652156\n",
            "Epoch: 460  Loss: 146.88288017667074\n",
            "Epoch: 461  Loss: 146.9080625587812\n",
            "Epoch: 462  Loss: 146.93533312705262\n",
            "Epoch: 463  Loss: 146.93027637779397\n",
            "Epoch: 464  Loss: 146.93512399091554\n",
            "Epoch: 465  Loss: 146.92641728686922\n",
            "Epoch: 466  Loss: 147.0284995568746\n",
            "Epoch: 467  Loss: 147.00419383071053\n",
            "Epoch: 468  Loss: 146.96203062465355\n",
            "Epoch: 469  Loss: 146.984381275787\n",
            "Epoch: 470  Loss: 147.00080166721392\n",
            "Epoch: 471  Loss: 147.00268863531093\n",
            "Epoch: 472  Loss: 147.01794714184473\n",
            "Epoch: 473  Loss: 147.0701888218527\n",
            "Epoch: 474  Loss: 147.02993531954394\n",
            "Epoch: 475  Loss: 147.02367770933017\n",
            "Epoch: 476  Loss: 147.07822473568672\n",
            "Epoch: 477  Loss: 147.0363240710819\n",
            "Epoch: 478  Loss: 147.01146912983194\n",
            "Epoch: 479  Loss: 147.05458647769598\n",
            "Epoch: 480  Loss: 147.03100847133265\n",
            "Epoch: 481  Loss: 147.03174713968832\n",
            "Epoch: 482  Loss: 147.03576863071314\n",
            "Epoch: 483  Loss: 147.08903230655312\n",
            "Epoch: 484  Loss: 147.0370983880573\n",
            "Epoch: 485  Loss: 147.00777358295775\n",
            "Epoch: 486  Loss: 147.0238934701336\n",
            "Epoch: 487  Loss: 147.0304939180839\n",
            "Epoch: 488  Loss: 147.07206079920581\n",
            "Epoch: 489  Loss: 147.03024548915005\n",
            "Epoch: 490  Loss: 147.04182695708585\n",
            "Epoch: 491  Loss: 147.02075652241862\n",
            "Epoch: 492  Loss: 147.05578109764758\n",
            "Epoch: 493  Loss: 147.07462324339926\n",
            "Epoch: 494  Loss: 147.06924733024633\n",
            "Epoch: 495  Loss: 147.0443237135314\n",
            "Epoch: 496  Loss: 147.06237006029045\n",
            "Epoch: 497  Loss: 147.06174292029917\n",
            "Epoch: 498  Loss: 147.05664953920896\n",
            "Epoch: 499  Loss: 147.11498766948415\n",
            "Epoch: 500  Loss: 147.05981853346276\n",
            "Done Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ovPIVY-axqvx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### SGD Nesterov Momentum"
      ]
    },
    {
      "metadata": {
        "id": "n6Z6JFCZxuFJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model3 = MLP(dims=[4, 10, 10, 3], activations = [Activations.sigmoid]*2+[Activations.softmax])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ilR9XXP8x17O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9126
        },
        "outputId": "72bc3daf-d612-457e-8b64-e9f1a9eda9dc"
      },
      "cell_type": "code",
      "source": [
        "model3.train(x_train=x_train, y_train=y_train, epochs=500, batch_size=10, optimizer=SGD_NesterovMomentum, opt_kwargs=dict(lr=0.001, alpha=0.9))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  Loss: 186.5513057303786\n",
            "Epoch: 2  Loss: 181.42287995213582\n",
            "Epoch: 3  Loss: 175.49224478881428\n",
            "Epoch: 4  Loss: 169.72368171033756\n",
            "Epoch: 5  Loss: 165.5460499552294\n",
            "Epoch: 6  Loss: 161.81334510026406\n",
            "Epoch: 7  Loss: 158.99711475429794\n",
            "Epoch: 8  Loss: 156.95192388397282\n",
            "Epoch: 9  Loss: 155.0855441783096\n",
            "Epoch: 10  Loss: 153.88577343572695\n",
            "Epoch: 11  Loss: 152.98651932430124\n",
            "Epoch: 12  Loss: 151.68812065987606\n",
            "Epoch: 13  Loss: 151.22726391245016\n",
            "Epoch: 14  Loss: 150.77591889270963\n",
            "Epoch: 15  Loss: 150.26367390278136\n",
            "Epoch: 16  Loss: 149.81456498716773\n",
            "Epoch: 17  Loss: 149.39323472197796\n",
            "Epoch: 18  Loss: 148.8002856769523\n",
            "Epoch: 19  Loss: 148.08487906765333\n",
            "Epoch: 20  Loss: 147.67919642100455\n",
            "Epoch: 21  Loss: 147.39080057178592\n",
            "Epoch: 22  Loss: 147.1207807521418\n",
            "Epoch: 23  Loss: 146.79324537404378\n",
            "Epoch: 24  Loss: 146.55183344633505\n",
            "Epoch: 25  Loss: 146.3932664419168\n",
            "Epoch: 26  Loss: 146.27148179237284\n",
            "Epoch: 27  Loss: 146.24235292520632\n",
            "Epoch: 28  Loss: 146.1328504820067\n",
            "Epoch: 29  Loss: 146.1220408652524\n",
            "Epoch: 30  Loss: 146.076268640185\n",
            "Epoch: 31  Loss: 146.05421978308863\n",
            "Epoch: 32  Loss: 146.09920065452235\n",
            "Epoch: 33  Loss: 146.05370119300707\n",
            "Epoch: 34  Loss: 146.12025111387936\n",
            "Epoch: 35  Loss: 146.2358904938849\n",
            "Epoch: 36  Loss: 146.2131719332274\n",
            "Epoch: 37  Loss: 146.26901531818007\n",
            "Epoch: 38  Loss: 146.2330151700729\n",
            "Epoch: 39  Loss: 146.29145673049663\n",
            "Epoch: 40  Loss: 146.33230903312713\n",
            "Epoch: 41  Loss: 146.48116573863553\n",
            "Epoch: 42  Loss: 146.44640188662203\n",
            "Epoch: 43  Loss: 146.3430916224389\n",
            "Epoch: 44  Loss: 146.4631314895657\n",
            "Epoch: 45  Loss: 146.44063929269842\n",
            "Epoch: 46  Loss: 146.45040653299577\n",
            "Epoch: 47  Loss: 146.48271657170335\n",
            "Epoch: 48  Loss: 146.54208646769675\n",
            "Epoch: 49  Loss: 146.4697219291949\n",
            "Epoch: 50  Loss: 146.5430966039588\n",
            "Epoch: 51  Loss: 146.52933653825596\n",
            "Epoch: 52  Loss: 146.4862336965644\n",
            "Epoch: 53  Loss: 146.5699622853546\n",
            "Epoch: 54  Loss: 146.64409715719287\n",
            "Epoch: 55  Loss: 146.74027978487638\n",
            "Epoch: 56  Loss: 146.7118863219187\n",
            "Epoch: 57  Loss: 146.73444026576894\n",
            "Epoch: 58  Loss: 146.90566336711927\n",
            "Epoch: 59  Loss: 146.79658353383203\n",
            "Epoch: 60  Loss: 146.73330173885844\n",
            "Epoch: 61  Loss: 146.70818329799366\n",
            "Epoch: 62  Loss: 146.7075026963692\n",
            "Epoch: 63  Loss: 146.73374480978015\n",
            "Epoch: 64  Loss: 146.74499304134557\n",
            "Epoch: 65  Loss: 146.885710836142\n",
            "Epoch: 66  Loss: 146.78925875345726\n",
            "Epoch: 67  Loss: 146.786091620828\n",
            "Epoch: 68  Loss: 146.80708505053815\n",
            "Epoch: 69  Loss: 146.89155764338864\n",
            "Epoch: 70  Loss: 146.92460680237767\n",
            "Epoch: 71  Loss: 146.91573630122292\n",
            "Epoch: 72  Loss: 146.98755704628238\n",
            "Epoch: 73  Loss: 146.91913317020143\n",
            "Epoch: 74  Loss: 147.25191251895916\n",
            "Epoch: 75  Loss: 147.07056184083604\n",
            "Epoch: 76  Loss: 147.09182012644928\n",
            "Epoch: 77  Loss: 147.20036435214817\n",
            "Epoch: 78  Loss: 147.14918403933768\n",
            "Epoch: 79  Loss: 147.11120356511796\n",
            "Epoch: 80  Loss: 147.04337553167338\n",
            "Epoch: 81  Loss: 147.00867215992304\n",
            "Epoch: 82  Loss: 147.0132571770548\n",
            "Epoch: 83  Loss: 147.01801497388647\n",
            "Epoch: 84  Loss: 147.03257646544355\n",
            "Epoch: 85  Loss: 147.1484338489842\n",
            "Epoch: 86  Loss: 147.1087020340013\n",
            "Epoch: 87  Loss: 147.07930020417828\n",
            "Epoch: 88  Loss: 147.10371485856209\n",
            "Epoch: 89  Loss: 147.2322947809314\n",
            "Epoch: 90  Loss: 147.0719409806075\n",
            "Epoch: 91  Loss: 147.12892027490977\n",
            "Epoch: 92  Loss: 147.198727793006\n",
            "Epoch: 93  Loss: 147.1419080174744\n",
            "Epoch: 94  Loss: 147.14654885403556\n",
            "Epoch: 95  Loss: 147.15527489781908\n",
            "Epoch: 96  Loss: 147.2152381695635\n",
            "Epoch: 97  Loss: 147.18658098538108\n",
            "Epoch: 98  Loss: 147.17156116062387\n",
            "Epoch: 99  Loss: 147.16250442065967\n",
            "Epoch: 100  Loss: 147.2563664524078\n",
            "Epoch: 101  Loss: 147.27829250423474\n",
            "Epoch: 102  Loss: 147.32421143997868\n",
            "Epoch: 103  Loss: 147.47871876168904\n",
            "Epoch: 104  Loss: 147.42001865385757\n",
            "Epoch: 105  Loss: 147.2559226122533\n",
            "Epoch: 106  Loss: 147.28307040646726\n",
            "Epoch: 107  Loss: 147.24372303828423\n",
            "Epoch: 108  Loss: 147.34060219841257\n",
            "Epoch: 109  Loss: 147.38858377231952\n",
            "Epoch: 110  Loss: 147.42234156599756\n",
            "Epoch: 111  Loss: 147.53662145267927\n",
            "Epoch: 112  Loss: 147.466387171448\n",
            "Epoch: 113  Loss: 147.53488276968363\n",
            "Epoch: 114  Loss: 147.4755177925743\n",
            "Epoch: 115  Loss: 147.45944930980755\n",
            "Epoch: 116  Loss: 147.48655539718925\n",
            "Epoch: 117  Loss: 147.4539679699599\n",
            "Epoch: 118  Loss: 147.43163647473932\n",
            "Epoch: 119  Loss: 147.54452750773285\n",
            "Epoch: 120  Loss: 147.39759533793043\n",
            "Epoch: 121  Loss: 147.36812027994858\n",
            "Epoch: 122  Loss: 147.36461632772836\n",
            "Epoch: 123  Loss: 147.42333539818912\n",
            "Epoch: 124  Loss: 147.40895753054565\n",
            "Epoch: 125  Loss: 147.37593124211995\n",
            "Epoch: 126  Loss: 147.3301025596066\n",
            "Epoch: 127  Loss: 147.32758234845346\n",
            "Epoch: 128  Loss: 147.2954540144199\n",
            "Epoch: 129  Loss: 147.31424612624326\n",
            "Epoch: 130  Loss: 147.2965737776943\n",
            "Epoch: 131  Loss: 147.29132760193815\n",
            "Epoch: 132  Loss: 147.3073945111838\n",
            "Epoch: 133  Loss: 147.37872352319354\n",
            "Epoch: 134  Loss: 147.4621535205898\n",
            "Epoch: 135  Loss: 147.4114433519658\n",
            "Epoch: 136  Loss: 147.37635922227702\n",
            "Epoch: 137  Loss: 147.7296102495776\n",
            "Epoch: 138  Loss: 147.57557074855202\n",
            "Epoch: 139  Loss: 147.42064839326812\n",
            "Epoch: 140  Loss: 147.45015299652974\n",
            "Epoch: 141  Loss: 147.45521657195823\n",
            "Epoch: 142  Loss: 147.46703499876708\n",
            "Epoch: 143  Loss: 147.45576411179562\n",
            "Epoch: 144  Loss: 147.52653903181636\n",
            "Epoch: 145  Loss: 147.40823694077872\n",
            "Epoch: 146  Loss: 147.5105509579361\n",
            "Epoch: 147  Loss: 147.56185715578567\n",
            "Epoch: 148  Loss: 147.65991018522126\n",
            "Epoch: 149  Loss: 147.52415430059017\n",
            "Epoch: 150  Loss: 147.561032592133\n",
            "Epoch: 151  Loss: 147.5932733588932\n",
            "Epoch: 152  Loss: 147.55453185954886\n",
            "Epoch: 153  Loss: 147.5864341063426\n",
            "Epoch: 154  Loss: 147.6282833318\n",
            "Epoch: 155  Loss: 147.58559006170887\n",
            "Epoch: 156  Loss: 147.54461104355877\n",
            "Epoch: 157  Loss: 147.48451291568716\n",
            "Epoch: 158  Loss: 147.51765986760606\n",
            "Epoch: 159  Loss: 147.5493095808651\n",
            "Epoch: 160  Loss: 147.52245670915264\n",
            "Epoch: 161  Loss: 147.4938546663636\n",
            "Epoch: 162  Loss: 147.5357401463021\n",
            "Epoch: 163  Loss: 147.60550021074283\n",
            "Epoch: 164  Loss: 147.61303719235346\n",
            "Epoch: 165  Loss: 147.56891875582966\n",
            "Epoch: 166  Loss: 147.50222451167875\n",
            "Epoch: 167  Loss: 147.43600188402647\n",
            "Epoch: 168  Loss: 147.4413326536817\n",
            "Epoch: 169  Loss: 147.51363232665574\n",
            "Epoch: 170  Loss: 147.54898785412675\n",
            "Epoch: 171  Loss: 147.49974342931387\n",
            "Epoch: 172  Loss: 147.68663907843373\n",
            "Epoch: 173  Loss: 147.8178188061623\n",
            "Epoch: 174  Loss: 147.6770587211787\n",
            "Epoch: 175  Loss: 147.55227379769767\n",
            "Epoch: 176  Loss: 147.48896890101489\n",
            "Epoch: 177  Loss: 147.45664971966247\n",
            "Epoch: 178  Loss: 147.487651842449\n",
            "Epoch: 179  Loss: 147.41375918107715\n",
            "Epoch: 180  Loss: 147.42833717687284\n",
            "Epoch: 181  Loss: 147.35412209099368\n",
            "Epoch: 182  Loss: 147.34063496015813\n",
            "Epoch: 183  Loss: 147.38399937718177\n",
            "Epoch: 184  Loss: 147.37666144430284\n",
            "Epoch: 185  Loss: 147.35640528484598\n",
            "Epoch: 186  Loss: 147.32840804833273\n",
            "Epoch: 187  Loss: 147.3918849301387\n",
            "Epoch: 188  Loss: 147.36196759443465\n",
            "Epoch: 189  Loss: 147.40721565834033\n",
            "Epoch: 190  Loss: 147.3595659112715\n",
            "Epoch: 191  Loss: 147.45604717392482\n",
            "Epoch: 192  Loss: 147.43562625625665\n",
            "Epoch: 193  Loss: 147.50092715197997\n",
            "Epoch: 194  Loss: 147.46391131166433\n",
            "Epoch: 195  Loss: 147.59317107727352\n",
            "Epoch: 196  Loss: 147.60597037619453\n",
            "Epoch: 197  Loss: 147.4617336202534\n",
            "Epoch: 198  Loss: 147.45453313763278\n",
            "Epoch: 199  Loss: 147.47178183105447\n",
            "Epoch: 200  Loss: 147.30710787068645\n",
            "Epoch: 201  Loss: 147.41981900894677\n",
            "Epoch: 202  Loss: 147.46621716382742\n",
            "Epoch: 203  Loss: 147.28714053029626\n",
            "Epoch: 204  Loss: 147.42761154900077\n",
            "Epoch: 205  Loss: 147.46253642219366\n",
            "Epoch: 206  Loss: 147.66454403440457\n",
            "Epoch: 207  Loss: 147.58460241623467\n",
            "Epoch: 208  Loss: 147.48198681022055\n",
            "Epoch: 209  Loss: 147.5129708293866\n",
            "Epoch: 210  Loss: 147.70102506928643\n",
            "Epoch: 211  Loss: 147.6849655696123\n",
            "Epoch: 212  Loss: 147.58072969533126\n",
            "Epoch: 213  Loss: 147.69319133142326\n",
            "Epoch: 214  Loss: 147.8115349598658\n",
            "Epoch: 215  Loss: 147.7082453718846\n",
            "Epoch: 216  Loss: 147.70229551767213\n",
            "Epoch: 217  Loss: 147.75432609419238\n",
            "Epoch: 218  Loss: 147.672357798491\n",
            "Epoch: 219  Loss: 147.7125993366859\n",
            "Epoch: 220  Loss: 147.5180875794106\n",
            "Epoch: 221  Loss: 147.67857519149868\n",
            "Epoch: 222  Loss: 147.67027196607089\n",
            "Epoch: 223  Loss: 147.63109273786236\n",
            "Epoch: 224  Loss: 147.63948516926004\n",
            "Epoch: 225  Loss: 147.79125216271223\n",
            "Epoch: 226  Loss: 147.83826494751085\n",
            "Epoch: 227  Loss: 147.68910760761466\n",
            "Epoch: 228  Loss: 147.63520675525206\n",
            "Epoch: 229  Loss: 147.66567641277268\n",
            "Epoch: 230  Loss: 147.72383453954117\n",
            "Epoch: 231  Loss: 147.59330383481748\n",
            "Epoch: 232  Loss: 147.48134527368225\n",
            "Epoch: 233  Loss: 147.56730440998456\n",
            "Epoch: 234  Loss: 147.48207020921137\n",
            "Epoch: 235  Loss: 147.6827268946132\n",
            "Epoch: 236  Loss: 147.59907163791613\n",
            "Epoch: 237  Loss: 147.5612223135341\n",
            "Epoch: 238  Loss: 147.56264083748223\n",
            "Epoch: 239  Loss: 147.82929650312593\n",
            "Epoch: 240  Loss: 147.7049044369743\n",
            "Epoch: 241  Loss: 147.68168563389136\n",
            "Epoch: 242  Loss: 147.69669113269205\n",
            "Epoch: 243  Loss: 147.61080657545168\n",
            "Epoch: 244  Loss: 147.53711513218747\n",
            "Epoch: 245  Loss: 147.5546189708733\n",
            "Epoch: 246  Loss: 147.644087452992\n",
            "Epoch: 247  Loss: 147.54788887933535\n",
            "Epoch: 248  Loss: 147.46128474316296\n",
            "Epoch: 249  Loss: 147.482378977523\n",
            "Epoch: 250  Loss: 147.34266267333845\n",
            "Epoch: 251  Loss: 147.3860996989905\n",
            "Epoch: 252  Loss: 147.46274574285502\n",
            "Epoch: 253  Loss: 147.599108123959\n",
            "Epoch: 254  Loss: 147.50710639764873\n",
            "Epoch: 255  Loss: 147.59618491334308\n",
            "Epoch: 256  Loss: 147.71145539981055\n",
            "Epoch: 257  Loss: 147.59823837453402\n",
            "Epoch: 258  Loss: 147.6294805127585\n",
            "Epoch: 259  Loss: 147.62794332578193\n",
            "Epoch: 260  Loss: 147.64621503394523\n",
            "Epoch: 261  Loss: 147.62986143694508\n",
            "Epoch: 262  Loss: 147.66137134259174\n",
            "Epoch: 263  Loss: 147.70293403410105\n",
            "Epoch: 264  Loss: 147.6746802736466\n",
            "Epoch: 265  Loss: 147.6412435856878\n",
            "Epoch: 266  Loss: 147.60952764779438\n",
            "Epoch: 267  Loss: 147.6867730531554\n",
            "Epoch: 268  Loss: 147.6347899034847\n",
            "Epoch: 269  Loss: 147.7329099490197\n",
            "Epoch: 270  Loss: 147.72525417043965\n",
            "Epoch: 271  Loss: 147.64843113260827\n",
            "Epoch: 272  Loss: 147.6654689893227\n",
            "Epoch: 273  Loss: 147.79803889386778\n",
            "Epoch: 274  Loss: 147.6825222137569\n",
            "Epoch: 275  Loss: 147.67913341837576\n",
            "Epoch: 276  Loss: 147.51402190035526\n",
            "Epoch: 277  Loss: 147.64344947647703\n",
            "Epoch: 278  Loss: 147.62481609108266\n",
            "Epoch: 279  Loss: 147.53141148160114\n",
            "Epoch: 280  Loss: 147.48599146113233\n",
            "Epoch: 281  Loss: 147.57980086962542\n",
            "Epoch: 282  Loss: 147.4877068775439\n",
            "Epoch: 283  Loss: 147.3714411341861\n",
            "Epoch: 284  Loss: 147.42244909502577\n",
            "Epoch: 285  Loss: 147.46513409743875\n",
            "Epoch: 286  Loss: 147.48416696939765\n",
            "Epoch: 287  Loss: 147.4992118653184\n",
            "Epoch: 288  Loss: 147.5110005508224\n",
            "Epoch: 289  Loss: 147.53373751088338\n",
            "Epoch: 290  Loss: 147.7812769849234\n",
            "Epoch: 291  Loss: 147.60555819162468\n",
            "Epoch: 292  Loss: 147.75907529932044\n",
            "Epoch: 293  Loss: 147.64245355470828\n",
            "Epoch: 294  Loss: 147.7064673653078\n",
            "Epoch: 295  Loss: 147.75169103760845\n",
            "Epoch: 296  Loss: 147.98778444121675\n",
            "Epoch: 297  Loss: 147.80409086273065\n",
            "Epoch: 298  Loss: 147.8449679508586\n",
            "Epoch: 299  Loss: 147.85915085763423\n",
            "Epoch: 300  Loss: 147.72388320421715\n",
            "Epoch: 301  Loss: 147.6037270487845\n",
            "Epoch: 302  Loss: 147.6066372878525\n",
            "Epoch: 303  Loss: 147.59884519652866\n",
            "Epoch: 304  Loss: 147.7519059295631\n",
            "Epoch: 305  Loss: 147.75825298622263\n",
            "Epoch: 306  Loss: 147.75017254888974\n",
            "Epoch: 307  Loss: 147.72481020722824\n",
            "Epoch: 308  Loss: 147.65670848295636\n",
            "Epoch: 309  Loss: 147.66410761087073\n",
            "Epoch: 310  Loss: 147.66421440434146\n",
            "Epoch: 311  Loss: 147.6967990741838\n",
            "Epoch: 312  Loss: 147.7396858468066\n",
            "Epoch: 313  Loss: 147.67570577592622\n",
            "Epoch: 314  Loss: 147.85172942286133\n",
            "Epoch: 315  Loss: 147.81765527168184\n",
            "Epoch: 316  Loss: 147.6873910206204\n",
            "Epoch: 317  Loss: 147.77300956358047\n",
            "Epoch: 318  Loss: 147.7463289890718\n",
            "Epoch: 319  Loss: 147.63430525380502\n",
            "Epoch: 320  Loss: 147.7131885830041\n",
            "Epoch: 321  Loss: 147.61655061097474\n",
            "Epoch: 322  Loss: 147.79813470118486\n",
            "Epoch: 323  Loss: 147.68655466353107\n",
            "Epoch: 324  Loss: 147.68582083237504\n",
            "Epoch: 325  Loss: 147.7452973843999\n",
            "Epoch: 326  Loss: 147.61359388497712\n",
            "Epoch: 327  Loss: 147.6246870856797\n",
            "Epoch: 328  Loss: 147.9532272128141\n",
            "Epoch: 329  Loss: 147.7025118019011\n",
            "Epoch: 330  Loss: 147.72960995838696\n",
            "Epoch: 331  Loss: 147.96817425351253\n",
            "Epoch: 332  Loss: 147.812740270962\n",
            "Epoch: 333  Loss: 147.7370250805305\n",
            "Epoch: 334  Loss: 147.65199463556675\n",
            "Epoch: 335  Loss: 147.65199584092477\n",
            "Epoch: 336  Loss: 147.7398109751556\n",
            "Epoch: 337  Loss: 147.80300561619467\n",
            "Epoch: 338  Loss: 147.80999728115552\n",
            "Epoch: 339  Loss: 147.83039644553767\n",
            "Epoch: 340  Loss: 147.85306184035048\n",
            "Epoch: 341  Loss: 147.86881521364296\n",
            "Epoch: 342  Loss: 147.76977230641452\n",
            "Epoch: 343  Loss: 147.86982678413838\n",
            "Epoch: 344  Loss: 147.94579789217482\n",
            "Epoch: 345  Loss: 147.85967317942004\n",
            "Epoch: 346  Loss: 147.69402471683202\n",
            "Epoch: 347  Loss: 147.6857566379268\n",
            "Epoch: 348  Loss: 147.67054641321187\n",
            "Epoch: 349  Loss: 147.70500006802348\n",
            "Epoch: 350  Loss: 147.6422370821342\n",
            "Epoch: 351  Loss: 147.9153725447425\n",
            "Epoch: 352  Loss: 147.65461504559028\n",
            "Epoch: 353  Loss: 147.52940521181077\n",
            "Epoch: 354  Loss: 147.53211671502677\n",
            "Epoch: 355  Loss: 147.47987249762573\n",
            "Epoch: 356  Loss: 147.48427383159628\n",
            "Epoch: 357  Loss: 147.59451253286883\n",
            "Epoch: 358  Loss: 147.46205035877017\n",
            "Epoch: 359  Loss: 147.45356232132588\n",
            "Epoch: 360  Loss: 147.44342755130663\n",
            "Epoch: 361  Loss: 147.69282484585872\n",
            "Epoch: 362  Loss: 147.4841701497225\n",
            "Epoch: 363  Loss: 147.67704825705337\n",
            "Epoch: 364  Loss: 147.51515273040218\n",
            "Epoch: 365  Loss: 147.5244449200581\n",
            "Epoch: 366  Loss: 147.52655891463334\n",
            "Epoch: 367  Loss: 147.49532668113451\n",
            "Epoch: 368  Loss: 147.4848475619804\n",
            "Epoch: 369  Loss: 147.5501052019336\n",
            "Epoch: 370  Loss: 147.60259800427778\n",
            "Epoch: 371  Loss: 147.6484630098431\n",
            "Epoch: 372  Loss: 147.46686050966287\n",
            "Epoch: 373  Loss: 147.49716275143982\n",
            "Epoch: 374  Loss: 147.5891097613008\n",
            "Epoch: 375  Loss: 147.6216774747727\n",
            "Epoch: 376  Loss: 147.51878608223277\n",
            "Epoch: 377  Loss: 147.5154055497635\n",
            "Epoch: 378  Loss: 147.55485266528387\n",
            "Epoch: 379  Loss: 147.66893212251802\n",
            "Epoch: 380  Loss: 147.60121046108813\n",
            "Epoch: 381  Loss: 147.57019231944307\n",
            "Epoch: 382  Loss: 147.54599919399092\n",
            "Epoch: 383  Loss: 147.55161251578346\n",
            "Epoch: 384  Loss: 147.55805974805807\n",
            "Epoch: 385  Loss: 147.56742389578594\n",
            "Epoch: 386  Loss: 147.63207323399715\n",
            "Epoch: 387  Loss: 147.5869628292486\n",
            "Epoch: 388  Loss: 147.58007356356438\n",
            "Epoch: 389  Loss: 147.5402420000858\n",
            "Epoch: 390  Loss: 147.58070064775282\n",
            "Epoch: 391  Loss: 147.59488324558336\n",
            "Epoch: 392  Loss: 147.7638353990576\n",
            "Epoch: 393  Loss: 147.5722504998719\n",
            "Epoch: 394  Loss: 147.5378326768544\n",
            "Epoch: 395  Loss: 147.57221277954199\n",
            "Epoch: 396  Loss: 147.6233613279564\n",
            "Epoch: 397  Loss: 147.4981783337668\n",
            "Epoch: 398  Loss: 147.59405339745246\n",
            "Epoch: 399  Loss: 147.76948172134936\n",
            "Epoch: 400  Loss: 147.63809308953742\n",
            "Epoch: 401  Loss: 147.6262487657281\n",
            "Epoch: 402  Loss: 147.6775313962089\n",
            "Epoch: 403  Loss: 147.64374617785603\n",
            "Epoch: 404  Loss: 147.61161584965993\n",
            "Epoch: 405  Loss: 147.59855934601518\n",
            "Epoch: 406  Loss: 147.664119606618\n",
            "Epoch: 407  Loss: 147.61513854045552\n",
            "Epoch: 408  Loss: 147.67445953008487\n",
            "Epoch: 409  Loss: 147.55717692160903\n",
            "Epoch: 410  Loss: 147.59149561626626\n",
            "Epoch: 411  Loss: 147.6702938670457\n",
            "Epoch: 412  Loss: 147.74511264069895\n",
            "Epoch: 413  Loss: 147.68067843754957\n",
            "Epoch: 414  Loss: 147.6396509974943\n",
            "Epoch: 415  Loss: 147.6129276463846\n",
            "Epoch: 416  Loss: 147.58060853413673\n",
            "Epoch: 417  Loss: 147.71079190763834\n",
            "Epoch: 418  Loss: 147.64349688085636\n",
            "Epoch: 419  Loss: 147.726776430356\n",
            "Epoch: 420  Loss: 147.73739435945643\n",
            "Epoch: 421  Loss: 147.58111914573212\n",
            "Epoch: 422  Loss: 147.52949326717365\n",
            "Epoch: 423  Loss: 147.69786171457645\n",
            "Epoch: 424  Loss: 147.6297340639521\n",
            "Epoch: 425  Loss: 147.63718346994912\n",
            "Epoch: 426  Loss: 147.6047692440353\n",
            "Epoch: 427  Loss: 147.64017842800914\n",
            "Epoch: 428  Loss: 147.6530261121554\n",
            "Epoch: 429  Loss: 147.75326458599957\n",
            "Epoch: 430  Loss: 147.7780816221076\n",
            "Epoch: 431  Loss: 147.66141225854085\n",
            "Epoch: 432  Loss: 147.76228293977644\n",
            "Epoch: 433  Loss: 147.65912011301828\n",
            "Epoch: 434  Loss: 147.69066584001015\n",
            "Epoch: 435  Loss: 147.64355985886118\n",
            "Epoch: 436  Loss: 147.75478134229155\n",
            "Epoch: 437  Loss: 147.6984756847515\n",
            "Epoch: 438  Loss: 147.6142180127374\n",
            "Epoch: 439  Loss: 147.63594105949457\n",
            "Epoch: 440  Loss: 147.62466857422558\n",
            "Epoch: 441  Loss: 147.59289333851538\n",
            "Epoch: 442  Loss: 147.68988092003826\n",
            "Epoch: 443  Loss: 147.7053176329406\n",
            "Epoch: 444  Loss: 147.73371913715712\n",
            "Epoch: 445  Loss: 147.70926561069712\n",
            "Epoch: 446  Loss: 147.70207346702315\n",
            "Epoch: 447  Loss: 147.71363077995377\n",
            "Epoch: 448  Loss: 147.83878972528393\n",
            "Epoch: 449  Loss: 147.85415716688638\n",
            "Epoch: 450  Loss: 147.78286387957738\n",
            "Epoch: 451  Loss: 147.87583282206995\n",
            "Epoch: 452  Loss: 147.88899113965562\n",
            "Epoch: 453  Loss: 147.87555431912799\n",
            "Epoch: 454  Loss: 147.92860376440746\n",
            "Epoch: 455  Loss: 147.91701506766748\n",
            "Epoch: 456  Loss: 147.97085979037107\n",
            "Epoch: 457  Loss: 147.96119249851722\n",
            "Epoch: 458  Loss: 147.90565547975345\n",
            "Epoch: 459  Loss: 148.0306326495391\n",
            "Epoch: 460  Loss: 147.94312511933882\n",
            "Epoch: 461  Loss: 147.87268262129473\n",
            "Epoch: 462  Loss: 147.79531931213864\n",
            "Epoch: 463  Loss: 147.86871770087447\n",
            "Epoch: 464  Loss: 147.94864209156117\n",
            "Epoch: 465  Loss: 147.83515542230722\n",
            "Epoch: 466  Loss: 147.8127627883451\n",
            "Epoch: 467  Loss: 147.80067809064715\n",
            "Epoch: 468  Loss: 147.79996003210186\n",
            "Epoch: 469  Loss: 147.84380503454554\n",
            "Epoch: 470  Loss: 147.77536164849207\n",
            "Epoch: 471  Loss: 147.80328445118496\n",
            "Epoch: 472  Loss: 147.84422899185154\n",
            "Epoch: 473  Loss: 147.89495659724838\n",
            "Epoch: 474  Loss: 147.8091447181927\n",
            "Epoch: 475  Loss: 147.73929519095068\n",
            "Epoch: 476  Loss: 147.7175325356624\n",
            "Epoch: 477  Loss: 147.69128255423163\n",
            "Epoch: 478  Loss: 147.7686608110096\n",
            "Epoch: 479  Loss: 147.71751624570499\n",
            "Epoch: 480  Loss: 147.75663307931572\n",
            "Epoch: 481  Loss: 147.70212675517925\n",
            "Epoch: 482  Loss: 147.7866896876173\n",
            "Epoch: 483  Loss: 147.79248648407747\n",
            "Epoch: 484  Loss: 147.92579315742145\n",
            "Epoch: 485  Loss: 148.0828618299951\n",
            "Epoch: 486  Loss: 147.88088182820422\n",
            "Epoch: 487  Loss: 147.82389948546123\n",
            "Epoch: 488  Loss: 147.69918449294\n",
            "Epoch: 489  Loss: 147.7513238688149\n",
            "Epoch: 490  Loss: 147.80526136908566\n",
            "Epoch: 491  Loss: 147.70140591288592\n",
            "Epoch: 492  Loss: 147.7784071228405\n",
            "Epoch: 493  Loss: 147.71175023643187\n",
            "Epoch: 494  Loss: 147.76532918428026\n",
            "Epoch: 495  Loss: 147.87032212615992\n",
            "Epoch: 496  Loss: 147.9390273345594\n",
            "Epoch: 497  Loss: 147.83260160178102\n",
            "Epoch: 498  Loss: 147.72909646097315\n",
            "Epoch: 499  Loss: 147.68804433561627\n",
            "Epoch: 500  Loss: 147.71609539591023\n",
            "Done Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cy2T_EPMxtc9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### AdaGrad"
      ]
    },
    {
      "metadata": {
        "id": "LQCIzyEFxwhx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model4 = MLP(dims=[4, 10, 10, 3], activations = [Activations.sigmoid]*2+[Activations.softmax])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9-6EImTfx2c2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9126
        },
        "outputId": "a8898f1f-e67f-4809-a627-7346dc847cc2"
      },
      "cell_type": "code",
      "source": [
        "model4.train(x_train=x_train, y_train=y_train, epochs=500, batch_size=10, optimizer=AdaGrad, opt_kwargs=dict(lr=0.001))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  Loss: 177.99231313676864\n",
            "Epoch: 2  Loss: 177.70943886929197\n",
            "Epoch: 3  Loss: 177.4946158609255\n",
            "Epoch: 4  Loss: 177.3045792520855\n",
            "Epoch: 5  Loss: 177.13756383557583\n",
            "Epoch: 6  Loss: 176.91721584537888\n",
            "Epoch: 7  Loss: 176.87553315152954\n",
            "Epoch: 8  Loss: 176.79874891735344\n",
            "Epoch: 9  Loss: 176.68926552507963\n",
            "Epoch: 10  Loss: 176.58572067480384\n",
            "Epoch: 11  Loss: 176.45188182680675\n",
            "Epoch: 12  Loss: 176.40415384023527\n",
            "Epoch: 13  Loss: 176.328183326931\n",
            "Epoch: 14  Loss: 176.24601002521086\n",
            "Epoch: 15  Loss: 176.15330572402735\n",
            "Epoch: 16  Loss: 176.07834137397023\n",
            "Epoch: 17  Loss: 176.03787820558435\n",
            "Epoch: 18  Loss: 176.00223660686652\n",
            "Epoch: 19  Loss: 175.9465330659627\n",
            "Epoch: 20  Loss: 175.82808478268754\n",
            "Epoch: 21  Loss: 175.75567493420414\n",
            "Epoch: 22  Loss: 175.73330886834356\n",
            "Epoch: 23  Loss: 175.63324815069143\n",
            "Epoch: 24  Loss: 175.5886769276922\n",
            "Epoch: 25  Loss: 175.54598182023662\n",
            "Epoch: 26  Loss: 175.45962636004535\n",
            "Epoch: 27  Loss: 175.4519124184603\n",
            "Epoch: 28  Loss: 175.3802075073928\n",
            "Epoch: 29  Loss: 175.35958374620517\n",
            "Epoch: 30  Loss: 175.33729279342268\n",
            "Epoch: 31  Loss: 175.32723611006753\n",
            "Epoch: 32  Loss: 175.24398178987929\n",
            "Epoch: 33  Loss: 175.19134580874942\n",
            "Epoch: 34  Loss: 175.16172659281327\n",
            "Epoch: 35  Loss: 175.09813447825772\n",
            "Epoch: 36  Loss: 175.05634289237057\n",
            "Epoch: 37  Loss: 174.96566626402836\n",
            "Epoch: 38  Loss: 174.94334653421123\n",
            "Epoch: 39  Loss: 174.9209087197766\n",
            "Epoch: 40  Loss: 174.88044374901045\n",
            "Epoch: 41  Loss: 174.8077679113101\n",
            "Epoch: 42  Loss: 174.79038036946608\n",
            "Epoch: 43  Loss: 174.76007698668946\n",
            "Epoch: 44  Loss: 174.72587479595347\n",
            "Epoch: 45  Loss: 174.71156975728383\n",
            "Epoch: 46  Loss: 174.67954971128384\n",
            "Epoch: 47  Loss: 174.5970837571569\n",
            "Epoch: 48  Loss: 174.55938266785466\n",
            "Epoch: 49  Loss: 174.52633968979774\n",
            "Epoch: 50  Loss: 174.5230588679821\n",
            "Epoch: 51  Loss: 174.48278584476913\n",
            "Epoch: 52  Loss: 174.48043970012378\n",
            "Epoch: 53  Loss: 174.43194253217612\n",
            "Epoch: 54  Loss: 174.41053311392577\n",
            "Epoch: 55  Loss: 174.3623361654325\n",
            "Epoch: 56  Loss: 174.35308732011174\n",
            "Epoch: 57  Loss: 174.32121886668614\n",
            "Epoch: 58  Loss: 174.2641197462794\n",
            "Epoch: 59  Loss: 174.26133846410534\n",
            "Epoch: 60  Loss: 174.24141170584576\n",
            "Epoch: 61  Loss: 174.210024955916\n",
            "Epoch: 62  Loss: 174.1670901559074\n",
            "Epoch: 63  Loss: 174.15353768801813\n",
            "Epoch: 64  Loss: 174.14634696067316\n",
            "Epoch: 65  Loss: 174.1391362472398\n",
            "Epoch: 66  Loss: 174.09491098544294\n",
            "Epoch: 67  Loss: 174.0571614313289\n",
            "Epoch: 68  Loss: 174.0580323585085\n",
            "Epoch: 69  Loss: 174.02557220294764\n",
            "Epoch: 70  Loss: 173.9974113092312\n",
            "Epoch: 71  Loss: 173.98716800445138\n",
            "Epoch: 72  Loss: 173.961450131239\n",
            "Epoch: 73  Loss: 173.93273551271486\n",
            "Epoch: 74  Loss: 173.9289977036775\n",
            "Epoch: 75  Loss: 173.92285973674007\n",
            "Epoch: 76  Loss: 173.87128284401038\n",
            "Epoch: 77  Loss: 173.82261845629702\n",
            "Epoch: 78  Loss: 173.86140156687006\n",
            "Epoch: 79  Loss: 173.83091796894777\n",
            "Epoch: 80  Loss: 173.81058058377462\n",
            "Epoch: 81  Loss: 173.798410344977\n",
            "Epoch: 82  Loss: 173.77823388096454\n",
            "Epoch: 83  Loss: 173.75174155528885\n",
            "Epoch: 84  Loss: 173.70287553927355\n",
            "Epoch: 85  Loss: 173.70593249051888\n",
            "Epoch: 86  Loss: 173.6876311332575\n",
            "Epoch: 87  Loss: 173.67288062173233\n",
            "Epoch: 88  Loss: 173.62442172375037\n",
            "Epoch: 89  Loss: 173.61768778020684\n",
            "Epoch: 90  Loss: 173.58934898077342\n",
            "Epoch: 91  Loss: 173.58815562949383\n",
            "Epoch: 92  Loss: 173.58759225112402\n",
            "Epoch: 93  Loss: 173.57902321653503\n",
            "Epoch: 94  Loss: 173.53547257456333\n",
            "Epoch: 95  Loss: 173.546955842931\n",
            "Epoch: 96  Loss: 173.5261824345567\n",
            "Epoch: 97  Loss: 173.52982276834905\n",
            "Epoch: 98  Loss: 173.48289587419416\n",
            "Epoch: 99  Loss: 173.47381007564582\n",
            "Epoch: 100  Loss: 173.45726851124422\n",
            "Epoch: 101  Loss: 173.4090184009185\n",
            "Epoch: 102  Loss: 173.41635372686363\n",
            "Epoch: 103  Loss: 173.40431259396414\n",
            "Epoch: 104  Loss: 173.4102646968094\n",
            "Epoch: 105  Loss: 173.37849997343017\n",
            "Epoch: 106  Loss: 173.35795261752153\n",
            "Epoch: 107  Loss: 173.34085484955895\n",
            "Epoch: 108  Loss: 173.3206650696827\n",
            "Epoch: 109  Loss: 173.3093175222708\n",
            "Epoch: 110  Loss: 173.2959460974846\n",
            "Epoch: 111  Loss: 173.29005719499398\n",
            "Epoch: 112  Loss: 173.2580636097412\n",
            "Epoch: 113  Loss: 173.2608870581219\n",
            "Epoch: 114  Loss: 173.25439765565235\n",
            "Epoch: 115  Loss: 173.2330180967263\n",
            "Epoch: 116  Loss: 173.21462984738235\n",
            "Epoch: 117  Loss: 173.18927749766107\n",
            "Epoch: 118  Loss: 173.1796187772183\n",
            "Epoch: 119  Loss: 173.18458903410547\n",
            "Epoch: 120  Loss: 173.1749297417198\n",
            "Epoch: 121  Loss: 173.1539083384075\n",
            "Epoch: 122  Loss: 173.13610048854707\n",
            "Epoch: 123  Loss: 173.14950768728082\n",
            "Epoch: 124  Loss: 173.12189648951423\n",
            "Epoch: 125  Loss: 173.11310886429516\n",
            "Epoch: 126  Loss: 173.1004649469007\n",
            "Epoch: 127  Loss: 173.08025796214102\n",
            "Epoch: 128  Loss: 173.07690885168975\n",
            "Epoch: 129  Loss: 173.0514543356067\n",
            "Epoch: 130  Loss: 173.03374213185023\n",
            "Epoch: 131  Loss: 173.02019673377933\n",
            "Epoch: 132  Loss: 173.01274484789565\n",
            "Epoch: 133  Loss: 173.01599052402875\n",
            "Epoch: 134  Loss: 173.02036996275885\n",
            "Epoch: 135  Loss: 172.99087303101018\n",
            "Epoch: 136  Loss: 173.00242795229298\n",
            "Epoch: 137  Loss: 173.00046281395353\n",
            "Epoch: 138  Loss: 172.97660866709\n",
            "Epoch: 139  Loss: 172.96811934317563\n",
            "Epoch: 140  Loss: 172.94137424609812\n",
            "Epoch: 141  Loss: 172.94193548028906\n",
            "Epoch: 142  Loss: 172.93566864247632\n",
            "Epoch: 143  Loss: 172.92986706060623\n",
            "Epoch: 144  Loss: 172.92099160347541\n",
            "Epoch: 145  Loss: 172.92023895068235\n",
            "Epoch: 146  Loss: 172.9210381778163\n",
            "Epoch: 147  Loss: 172.90007649017804\n",
            "Epoch: 148  Loss: 172.8722716017102\n",
            "Epoch: 149  Loss: 172.85274512927612\n",
            "Epoch: 150  Loss: 172.84861061347667\n",
            "Epoch: 151  Loss: 172.84344495458234\n",
            "Epoch: 152  Loss: 172.82292078288688\n",
            "Epoch: 153  Loss: 172.81271363641605\n",
            "Epoch: 154  Loss: 172.80799510913093\n",
            "Epoch: 155  Loss: 172.79729435479948\n",
            "Epoch: 156  Loss: 172.7896783750036\n",
            "Epoch: 157  Loss: 172.79758847517138\n",
            "Epoch: 158  Loss: 172.79374866374613\n",
            "Epoch: 159  Loss: 172.78997565885217\n",
            "Epoch: 160  Loss: 172.78513242155853\n",
            "Epoch: 161  Loss: 172.77000127072927\n",
            "Epoch: 162  Loss: 172.76738942569213\n",
            "Epoch: 163  Loss: 172.75757673902731\n",
            "Epoch: 164  Loss: 172.75454889713865\n",
            "Epoch: 165  Loss: 172.7386650788449\n",
            "Epoch: 166  Loss: 172.73375299459954\n",
            "Epoch: 167  Loss: 172.7185173015368\n",
            "Epoch: 168  Loss: 172.73302234645857\n",
            "Epoch: 169  Loss: 172.71458211926057\n",
            "Epoch: 170  Loss: 172.70130248708466\n",
            "Epoch: 171  Loss: 172.7124812502381\n",
            "Epoch: 172  Loss: 172.7084671440017\n",
            "Epoch: 173  Loss: 172.69914052265932\n",
            "Epoch: 174  Loss: 172.6896511754118\n",
            "Epoch: 175  Loss: 172.67890821116032\n",
            "Epoch: 176  Loss: 172.68059032295483\n",
            "Epoch: 177  Loss: 172.67589196682803\n",
            "Epoch: 178  Loss: 172.66373056845927\n",
            "Epoch: 179  Loss: 172.6461588709425\n",
            "Epoch: 180  Loss: 172.65073484266185\n",
            "Epoch: 181  Loss: 172.64849775703667\n",
            "Epoch: 182  Loss: 172.63572933764144\n",
            "Epoch: 183  Loss: 172.62773207313137\n",
            "Epoch: 184  Loss: 172.6226062064559\n",
            "Epoch: 185  Loss: 172.62801119498863\n",
            "Epoch: 186  Loss: 172.61558454425574\n",
            "Epoch: 187  Loss: 172.60571798431778\n",
            "Epoch: 188  Loss: 172.61177002265805\n",
            "Epoch: 189  Loss: 172.59509507284037\n",
            "Epoch: 190  Loss: 172.57952092190297\n",
            "Epoch: 191  Loss: 172.5752144187945\n",
            "Epoch: 192  Loss: 172.5707842563345\n",
            "Epoch: 193  Loss: 172.56296704519895\n",
            "Epoch: 194  Loss: 172.56331726921118\n",
            "Epoch: 195  Loss: 172.5639972225431\n",
            "Epoch: 196  Loss: 172.55429833194418\n",
            "Epoch: 197  Loss: 172.55035652095557\n",
            "Epoch: 198  Loss: 172.54566840320058\n",
            "Epoch: 199  Loss: 172.5488329259042\n",
            "Epoch: 200  Loss: 172.53473433028103\n",
            "Epoch: 201  Loss: 172.53643740977017\n",
            "Epoch: 202  Loss: 172.51080038530822\n",
            "Epoch: 203  Loss: 172.51169185742722\n",
            "Epoch: 204  Loss: 172.49467592495137\n",
            "Epoch: 205  Loss: 172.4889128596394\n",
            "Epoch: 206  Loss: 172.48866502144315\n",
            "Epoch: 207  Loss: 172.49761571969944\n",
            "Epoch: 208  Loss: 172.47784070805395\n",
            "Epoch: 209  Loss: 172.46049154706324\n",
            "Epoch: 210  Loss: 172.45239374413197\n",
            "Epoch: 211  Loss: 172.44372767070328\n",
            "Epoch: 212  Loss: 172.43515295261895\n",
            "Epoch: 213  Loss: 172.4272608500169\n",
            "Epoch: 214  Loss: 172.4313536427707\n",
            "Epoch: 215  Loss: 172.42538588529646\n",
            "Epoch: 216  Loss: 172.4311863795118\n",
            "Epoch: 217  Loss: 172.41995029685293\n",
            "Epoch: 218  Loss: 172.41115809237095\n",
            "Epoch: 219  Loss: 172.417803953639\n",
            "Epoch: 220  Loss: 172.41177406725765\n",
            "Epoch: 221  Loss: 172.3970934262529\n",
            "Epoch: 222  Loss: 172.38432603863805\n",
            "Epoch: 223  Loss: 172.3834808017062\n",
            "Epoch: 224  Loss: 172.39883164582437\n",
            "Epoch: 225  Loss: 172.3859517769238\n",
            "Epoch: 226  Loss: 172.37349106519164\n",
            "Epoch: 227  Loss: 172.37261298196074\n",
            "Epoch: 228  Loss: 172.38049114551524\n",
            "Epoch: 229  Loss: 172.35930007223874\n",
            "Epoch: 230  Loss: 172.36957140463588\n",
            "Epoch: 231  Loss: 172.35240925364565\n",
            "Epoch: 232  Loss: 172.34754203634694\n",
            "Epoch: 233  Loss: 172.35246973013415\n",
            "Epoch: 234  Loss: 172.33782189765378\n",
            "Epoch: 235  Loss: 172.3551624915338\n",
            "Epoch: 236  Loss: 172.3464210220202\n",
            "Epoch: 237  Loss: 172.34701896501878\n",
            "Epoch: 238  Loss: 172.3389009307507\n",
            "Epoch: 239  Loss: 172.34573131431213\n",
            "Epoch: 240  Loss: 172.3357592155751\n",
            "Epoch: 241  Loss: 172.33081598735657\n",
            "Epoch: 242  Loss: 172.32911405635528\n",
            "Epoch: 243  Loss: 172.31685608151133\n",
            "Epoch: 244  Loss: 172.32362903262344\n",
            "Epoch: 245  Loss: 172.29853372780747\n",
            "Epoch: 246  Loss: 172.30082037624703\n",
            "Epoch: 247  Loss: 172.29709726372025\n",
            "Epoch: 248  Loss: 172.29673773402314\n",
            "Epoch: 249  Loss: 172.27789092802809\n",
            "Epoch: 250  Loss: 172.27182455399603\n",
            "Epoch: 251  Loss: 172.27037022991482\n",
            "Epoch: 252  Loss: 172.25000160577565\n",
            "Epoch: 253  Loss: 172.256156404167\n",
            "Epoch: 254  Loss: 172.25191217229252\n",
            "Epoch: 255  Loss: 172.24987204109152\n",
            "Epoch: 256  Loss: 172.2391220447168\n",
            "Epoch: 257  Loss: 172.24617891375385\n",
            "Epoch: 258  Loss: 172.2399163377862\n",
            "Epoch: 259  Loss: 172.22506093643327\n",
            "Epoch: 260  Loss: 172.22874252566027\n",
            "Epoch: 261  Loss: 172.2212819736386\n",
            "Epoch: 262  Loss: 172.21699824003204\n",
            "Epoch: 263  Loss: 172.22808892133264\n",
            "Epoch: 264  Loss: 172.22868908794197\n",
            "Epoch: 265  Loss: 172.21004791108984\n",
            "Epoch: 266  Loss: 172.21119212506704\n",
            "Epoch: 267  Loss: 172.20191781405018\n",
            "Epoch: 268  Loss: 172.2055033175318\n",
            "Epoch: 269  Loss: 172.1980198902869\n",
            "Epoch: 270  Loss: 172.2034503066658\n",
            "Epoch: 271  Loss: 172.2077141130659\n",
            "Epoch: 272  Loss: 172.1916425444358\n",
            "Epoch: 273  Loss: 172.18300149093218\n",
            "Epoch: 274  Loss: 172.164906129775\n",
            "Epoch: 275  Loss: 172.17085345069046\n",
            "Epoch: 276  Loss: 172.15657685658832\n",
            "Epoch: 277  Loss: 172.16118266014445\n",
            "Epoch: 278  Loss: 172.14611664779338\n",
            "Epoch: 279  Loss: 172.13875527806783\n",
            "Epoch: 280  Loss: 172.13855132695522\n",
            "Epoch: 281  Loss: 172.12290201584446\n",
            "Epoch: 282  Loss: 172.1340469439154\n",
            "Epoch: 283  Loss: 172.11986263583123\n",
            "Epoch: 284  Loss: 172.13972042921472\n",
            "Epoch: 285  Loss: 172.1199369211675\n",
            "Epoch: 286  Loss: 172.1236745650023\n",
            "Epoch: 287  Loss: 172.11436252534986\n",
            "Epoch: 288  Loss: 172.11569266829702\n",
            "Epoch: 289  Loss: 172.10205815248545\n",
            "Epoch: 290  Loss: 172.11015707889564\n",
            "Epoch: 291  Loss: 172.09809367237705\n",
            "Epoch: 292  Loss: 172.0958181063194\n",
            "Epoch: 293  Loss: 172.0913515640134\n",
            "Epoch: 294  Loss: 172.0876905883944\n",
            "Epoch: 295  Loss: 172.08253839403872\n",
            "Epoch: 296  Loss: 172.0848199367393\n",
            "Epoch: 297  Loss: 172.08431036111307\n",
            "Epoch: 298  Loss: 172.07851472999226\n",
            "Epoch: 299  Loss: 172.08452252001447\n",
            "Epoch: 300  Loss: 172.08119338338724\n",
            "Epoch: 301  Loss: 172.0856761978037\n",
            "Epoch: 302  Loss: 172.0827638150068\n",
            "Epoch: 303  Loss: 172.06314887836817\n",
            "Epoch: 304  Loss: 172.06908167175465\n",
            "Epoch: 305  Loss: 172.06136016709698\n",
            "Epoch: 306  Loss: 172.05353143025553\n",
            "Epoch: 307  Loss: 172.07197886879942\n",
            "Epoch: 308  Loss: 172.07302503849962\n",
            "Epoch: 309  Loss: 172.06222209906792\n",
            "Epoch: 310  Loss: 172.0585961811818\n",
            "Epoch: 311  Loss: 172.05387826141254\n",
            "Epoch: 312  Loss: 172.05646299427295\n",
            "Epoch: 313  Loss: 172.07183462120133\n",
            "Epoch: 314  Loss: 172.0602646917206\n",
            "Epoch: 315  Loss: 172.04928308009244\n",
            "Epoch: 316  Loss: 172.06064283978563\n",
            "Epoch: 317  Loss: 172.05034580527717\n",
            "Epoch: 318  Loss: 172.04802184488872\n",
            "Epoch: 319  Loss: 172.0494674665453\n",
            "Epoch: 320  Loss: 172.05203973053526\n",
            "Epoch: 321  Loss: 172.04594013413646\n",
            "Epoch: 322  Loss: 172.05665458529057\n",
            "Epoch: 323  Loss: 172.05284870568056\n",
            "Epoch: 324  Loss: 172.050258431092\n",
            "Epoch: 325  Loss: 172.03933992621842\n",
            "Epoch: 326  Loss: 172.03431191208574\n",
            "Epoch: 327  Loss: 172.02553245662634\n",
            "Epoch: 328  Loss: 172.02414127624888\n",
            "Epoch: 329  Loss: 172.01114271628418\n",
            "Epoch: 330  Loss: 172.01480266763582\n",
            "Epoch: 331  Loss: 172.01913254026834\n",
            "Epoch: 332  Loss: 172.01500119548396\n",
            "Epoch: 333  Loss: 172.00355733851148\n",
            "Epoch: 334  Loss: 172.00497350560386\n",
            "Epoch: 335  Loss: 171.99588588103643\n",
            "Epoch: 336  Loss: 172.0082401596685\n",
            "Epoch: 337  Loss: 171.99306568890896\n",
            "Epoch: 338  Loss: 171.99875107594096\n",
            "Epoch: 339  Loss: 171.97548873456577\n",
            "Epoch: 340  Loss: 171.98781406364122\n",
            "Epoch: 341  Loss: 171.97022241456963\n",
            "Epoch: 342  Loss: 171.96652668141104\n",
            "Epoch: 343  Loss: 171.9851062105641\n",
            "Epoch: 344  Loss: 171.9884617307195\n",
            "Epoch: 345  Loss: 171.98214699005356\n",
            "Epoch: 346  Loss: 171.98408440640185\n",
            "Epoch: 347  Loss: 171.97440462526288\n",
            "Epoch: 348  Loss: 171.97120947125225\n",
            "Epoch: 349  Loss: 171.97397457257907\n",
            "Epoch: 350  Loss: 171.9768681360207\n",
            "Epoch: 351  Loss: 171.96655957300285\n",
            "Epoch: 352  Loss: 171.96683522232328\n",
            "Epoch: 353  Loss: 171.9664632181855\n",
            "Epoch: 354  Loss: 171.96082757290517\n",
            "Epoch: 355  Loss: 171.95870931223791\n",
            "Epoch: 356  Loss: 171.95809972661573\n",
            "Epoch: 357  Loss: 171.95030735168288\n",
            "Epoch: 358  Loss: 171.93805780645104\n",
            "Epoch: 359  Loss: 171.9431077998025\n",
            "Epoch: 360  Loss: 171.94583326383008\n",
            "Epoch: 361  Loss: 171.9506838404886\n",
            "Epoch: 362  Loss: 171.95189219162896\n",
            "Epoch: 363  Loss: 171.94547226544665\n",
            "Epoch: 364  Loss: 171.94657974285658\n",
            "Epoch: 365  Loss: 171.93622405200276\n",
            "Epoch: 366  Loss: 171.9384788576294\n",
            "Epoch: 367  Loss: 171.92253581413402\n",
            "Epoch: 368  Loss: 171.91621472575883\n",
            "Epoch: 369  Loss: 171.91871056650882\n",
            "Epoch: 370  Loss: 171.9100036779214\n",
            "Epoch: 371  Loss: 171.90364213036398\n",
            "Epoch: 372  Loss: 171.91193325771093\n",
            "Epoch: 373  Loss: 171.91008611421032\n",
            "Epoch: 374  Loss: 171.91884644575882\n",
            "Epoch: 375  Loss: 171.9128818020762\n",
            "Epoch: 376  Loss: 171.91516918517385\n",
            "Epoch: 377  Loss: 171.9236599249353\n",
            "Epoch: 378  Loss: 171.9151429761708\n",
            "Epoch: 379  Loss: 171.91870753306105\n",
            "Epoch: 380  Loss: 171.9092593099525\n",
            "Epoch: 381  Loss: 171.89947904967113\n",
            "Epoch: 382  Loss: 171.89805238234015\n",
            "Epoch: 383  Loss: 171.8966716004373\n",
            "Epoch: 384  Loss: 171.8923523272853\n",
            "Epoch: 385  Loss: 171.8858466185673\n",
            "Epoch: 386  Loss: 171.88782350375436\n",
            "Epoch: 387  Loss: 171.878646006234\n",
            "Epoch: 388  Loss: 171.8757580819674\n",
            "Epoch: 389  Loss: 171.8836716362756\n",
            "Epoch: 390  Loss: 171.87556703744613\n",
            "Epoch: 391  Loss: 171.87193663286251\n",
            "Epoch: 392  Loss: 171.87129668677844\n",
            "Epoch: 393  Loss: 171.8658919773045\n",
            "Epoch: 394  Loss: 171.8641510874917\n",
            "Epoch: 395  Loss: 171.86662056136885\n",
            "Epoch: 396  Loss: 171.85952248252522\n",
            "Epoch: 397  Loss: 171.8596408808636\n",
            "Epoch: 398  Loss: 171.8511046575302\n",
            "Epoch: 399  Loss: 171.85305735143464\n",
            "Epoch: 400  Loss: 171.84821257661486\n",
            "Epoch: 401  Loss: 171.84416978275715\n",
            "Epoch: 402  Loss: 171.83736200151893\n",
            "Epoch: 403  Loss: 171.83516198581916\n",
            "Epoch: 404  Loss: 171.8388198381592\n",
            "Epoch: 405  Loss: 171.82766714113012\n",
            "Epoch: 406  Loss: 171.82755029182655\n",
            "Epoch: 407  Loss: 171.8136642681739\n",
            "Epoch: 408  Loss: 171.82069624728908\n",
            "Epoch: 409  Loss: 171.81591558043132\n",
            "Epoch: 410  Loss: 171.81565823625627\n",
            "Epoch: 411  Loss: 171.82154142026855\n",
            "Epoch: 412  Loss: 171.808833701301\n",
            "Epoch: 413  Loss: 171.81100240867138\n",
            "Epoch: 414  Loss: 171.81376446441433\n",
            "Epoch: 415  Loss: 171.81098730762156\n",
            "Epoch: 416  Loss: 171.82022951333218\n",
            "Epoch: 417  Loss: 171.82418505183767\n",
            "Epoch: 418  Loss: 171.82537803104444\n",
            "Epoch: 419  Loss: 171.82728111464252\n",
            "Epoch: 420  Loss: 171.83381835590708\n",
            "Epoch: 421  Loss: 171.82640828941885\n",
            "Epoch: 422  Loss: 171.8320995064535\n",
            "Epoch: 423  Loss: 171.83472001037\n",
            "Epoch: 424  Loss: 171.84082750436733\n",
            "Epoch: 425  Loss: 171.8379690866789\n",
            "Epoch: 426  Loss: 171.8244783267648\n",
            "Epoch: 427  Loss: 171.82972552621248\n",
            "Epoch: 428  Loss: 171.81809749145287\n",
            "Epoch: 429  Loss: 171.8278195482171\n",
            "Epoch: 430  Loss: 171.81994313129601\n",
            "Epoch: 431  Loss: 171.81894381554577\n",
            "Epoch: 432  Loss: 171.81539976108763\n",
            "Epoch: 433  Loss: 171.82429746813204\n",
            "Epoch: 434  Loss: 171.81826929122616\n",
            "Epoch: 435  Loss: 171.81377803704333\n",
            "Epoch: 436  Loss: 171.80734089769194\n",
            "Epoch: 437  Loss: 171.80787861044954\n",
            "Epoch: 438  Loss: 171.8050484814699\n",
            "Epoch: 439  Loss: 171.8066339642466\n",
            "Epoch: 440  Loss: 171.80546787598936\n",
            "Epoch: 441  Loss: 171.80276817088716\n",
            "Epoch: 442  Loss: 171.794699535855\n",
            "Epoch: 443  Loss: 171.80177507482284\n",
            "Epoch: 444  Loss: 171.7971296492112\n",
            "Epoch: 445  Loss: 171.79363618493068\n",
            "Epoch: 446  Loss: 171.80440746091392\n",
            "Epoch: 447  Loss: 171.80030135390783\n",
            "Epoch: 448  Loss: 171.8029932909185\n",
            "Epoch: 449  Loss: 171.80333893119294\n",
            "Epoch: 450  Loss: 171.81109708226305\n",
            "Epoch: 451  Loss: 171.7995409774506\n",
            "Epoch: 452  Loss: 171.81439992684196\n",
            "Epoch: 453  Loss: 171.80981723371224\n",
            "Epoch: 454  Loss: 171.80476785979303\n",
            "Epoch: 455  Loss: 171.80712261335924\n",
            "Epoch: 456  Loss: 171.80535084520713\n",
            "Epoch: 457  Loss: 171.80953251470592\n",
            "Epoch: 458  Loss: 171.8171111081238\n",
            "Epoch: 459  Loss: 171.8102821577781\n",
            "Epoch: 460  Loss: 171.81011060166222\n",
            "Epoch: 461  Loss: 171.81036385253577\n",
            "Epoch: 462  Loss: 171.81291332511807\n",
            "Epoch: 463  Loss: 171.80532857984934\n",
            "Epoch: 464  Loss: 171.81255476511055\n",
            "Epoch: 465  Loss: 171.79503603166154\n",
            "Epoch: 466  Loss: 171.79835777319\n",
            "Epoch: 467  Loss: 171.780583911244\n",
            "Epoch: 468  Loss: 171.7773286713468\n",
            "Epoch: 469  Loss: 171.77774053115346\n",
            "Epoch: 470  Loss: 171.79859373615088\n",
            "Epoch: 471  Loss: 171.7883251664263\n",
            "Epoch: 472  Loss: 171.77374007977102\n",
            "Epoch: 473  Loss: 171.7779947079474\n",
            "Epoch: 474  Loss: 171.77630217268555\n",
            "Epoch: 475  Loss: 171.78006384838008\n",
            "Epoch: 476  Loss: 171.77752001444838\n",
            "Epoch: 477  Loss: 171.78124962032123\n",
            "Epoch: 478  Loss: 171.78849438556767\n",
            "Epoch: 479  Loss: 171.7784266733728\n",
            "Epoch: 480  Loss: 171.77183459144211\n",
            "Epoch: 481  Loss: 171.77414420956427\n",
            "Epoch: 482  Loss: 171.77602906791543\n",
            "Epoch: 483  Loss: 171.7652555683231\n",
            "Epoch: 484  Loss: 171.76980096177095\n",
            "Epoch: 485  Loss: 171.77526265485784\n",
            "Epoch: 486  Loss: 171.77519054859383\n",
            "Epoch: 487  Loss: 171.76875158071726\n",
            "Epoch: 488  Loss: 171.77047001467847\n",
            "Epoch: 489  Loss: 171.7733525271888\n",
            "Epoch: 490  Loss: 171.77000593984428\n",
            "Epoch: 491  Loss: 171.7708209754161\n",
            "Epoch: 492  Loss: 171.75270405737388\n",
            "Epoch: 493  Loss: 171.76445188811618\n",
            "Epoch: 494  Loss: 171.75908909531248\n",
            "Epoch: 495  Loss: 171.76432257525877\n",
            "Epoch: 496  Loss: 171.77190024398834\n",
            "Epoch: 497  Loss: 171.78093532237463\n",
            "Epoch: 498  Loss: 171.76874640375496\n",
            "Epoch: 499  Loss: 171.77826914611023\n",
            "Epoch: 500  Loss: 171.77612411211484\n",
            "Done Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M-XMbul3xxFn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### RMSProp"
      ]
    },
    {
      "metadata": {
        "id": "NXbr_HN3xy0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model5 = MLP(dims=[4, 10, 10, 3], activations = [Activations.sigmoid]*2+[Activations.softmax])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bOg-PQUxx28m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5490
        },
        "outputId": "881cd9dc-faf2-477f-c218-53c536472987"
      },
      "cell_type": "code",
      "source": [
        "model5.train(x_train=x_train, y_train=y_train, epochs=300, batch_size=10, optimizer=RMSProp, opt_kwargs=dict(lr=0.0001, decay_rate=0.9))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  Loss: 167.110803620579\n",
            "Epoch: 2  Loss: 167.12202473612962\n",
            "Epoch: 3  Loss: 167.10331929657605\n",
            "Epoch: 4  Loss: 167.1290522717776\n",
            "Epoch: 5  Loss: 167.13856895136772\n",
            "Epoch: 6  Loss: 167.1333415510698\n",
            "Epoch: 7  Loss: 167.1365047628325\n",
            "Epoch: 8  Loss: 167.17409052950396\n",
            "Epoch: 9  Loss: 167.17088186813604\n",
            "Epoch: 10  Loss: 167.19333168281258\n",
            "Epoch: 11  Loss: 167.1894346826619\n",
            "Epoch: 12  Loss: 167.21136486020055\n",
            "Epoch: 13  Loss: 167.20914640595083\n",
            "Epoch: 14  Loss: 167.23520867233688\n",
            "Epoch: 15  Loss: 167.1736722137018\n",
            "Epoch: 16  Loss: 167.24617396677002\n",
            "Epoch: 17  Loss: 167.20446513095078\n",
            "Epoch: 18  Loss: 167.2402698116144\n",
            "Epoch: 19  Loss: 167.25962141726126\n",
            "Epoch: 20  Loss: 167.28766838290503\n",
            "Epoch: 21  Loss: 167.2736322103703\n",
            "Epoch: 22  Loss: 167.2461533983442\n",
            "Epoch: 23  Loss: 167.2716214395055\n",
            "Epoch: 24  Loss: 167.29896395050056\n",
            "Epoch: 25  Loss: 167.2621278009352\n",
            "Epoch: 26  Loss: 167.300800504341\n",
            "Epoch: 27  Loss: 167.2982536906611\n",
            "Epoch: 28  Loss: 167.29218348719726\n",
            "Epoch: 29  Loss: 167.30910660242628\n",
            "Epoch: 30  Loss: 167.32257505193022\n",
            "Epoch: 31  Loss: 167.3258587963991\n",
            "Epoch: 32  Loss: 167.3278762132963\n",
            "Epoch: 33  Loss: 167.3076152834951\n",
            "Epoch: 34  Loss: 167.35141597942956\n",
            "Epoch: 35  Loss: 167.3579301984513\n",
            "Epoch: 36  Loss: 167.29279112013282\n",
            "Epoch: 37  Loss: 167.3346337527258\n",
            "Epoch: 38  Loss: 167.2855462510384\n",
            "Epoch: 39  Loss: 167.31866230441594\n",
            "Epoch: 40  Loss: 167.30199726758343\n",
            "Epoch: 41  Loss: 167.30637844544017\n",
            "Epoch: 42  Loss: 167.30012387601667\n",
            "Epoch: 43  Loss: 167.28664495573085\n",
            "Epoch: 44  Loss: 167.29474199627\n",
            "Epoch: 45  Loss: 167.28655457847827\n",
            "Epoch: 46  Loss: 167.2597515097887\n",
            "Epoch: 47  Loss: 167.28043505472363\n",
            "Epoch: 48  Loss: 167.2508650174485\n",
            "Epoch: 49  Loss: 167.25086564052566\n",
            "Epoch: 50  Loss: 167.29857727040542\n",
            "Epoch: 51  Loss: 167.2467956642451\n",
            "Epoch: 52  Loss: 167.29145955410618\n",
            "Epoch: 53  Loss: 167.29392432990696\n",
            "Epoch: 54  Loss: 167.29105947214103\n",
            "Epoch: 55  Loss: 167.29510376072773\n",
            "Epoch: 56  Loss: 167.31369683822982\n",
            "Epoch: 57  Loss: 167.2774183458474\n",
            "Epoch: 58  Loss: 167.2860362248462\n",
            "Epoch: 59  Loss: 167.31635164234152\n",
            "Epoch: 60  Loss: 167.29172560555415\n",
            "Epoch: 61  Loss: 167.3075427460195\n",
            "Epoch: 62  Loss: 167.3199239271295\n",
            "Epoch: 63  Loss: 167.27971481621628\n",
            "Epoch: 64  Loss: 167.30141958902382\n",
            "Epoch: 65  Loss: 167.2910016509035\n",
            "Epoch: 66  Loss: 167.2680924842878\n",
            "Epoch: 67  Loss: 167.33739644760183\n",
            "Epoch: 68  Loss: 167.32409466840758\n",
            "Epoch: 69  Loss: 167.3088355125334\n",
            "Epoch: 70  Loss: 167.30289752023944\n",
            "Epoch: 71  Loss: 167.27428559197526\n",
            "Epoch: 72  Loss: 167.30406770909164\n",
            "Epoch: 73  Loss: 167.27622419004123\n",
            "Epoch: 74  Loss: 167.29347745651881\n",
            "Epoch: 75  Loss: 167.3080850841451\n",
            "Epoch: 76  Loss: 167.28155696457276\n",
            "Epoch: 77  Loss: 167.30956926671357\n",
            "Epoch: 78  Loss: 167.2925757763502\n",
            "Epoch: 79  Loss: 167.28143137514547\n",
            "Epoch: 80  Loss: 167.29324221332868\n",
            "Epoch: 81  Loss: 167.27047277600934\n",
            "Epoch: 82  Loss: 167.29107605460624\n",
            "Epoch: 83  Loss: 167.2936738110797\n",
            "Epoch: 84  Loss: 167.27554454165823\n",
            "Epoch: 85  Loss: 167.2910280372387\n",
            "Epoch: 86  Loss: 167.21530190258144\n",
            "Epoch: 87  Loss: 167.24943948178986\n",
            "Epoch: 88  Loss: 167.21042951925313\n",
            "Epoch: 89  Loss: 167.22578691879528\n",
            "Epoch: 90  Loss: 167.19194865173856\n",
            "Epoch: 91  Loss: 167.21979327088354\n",
            "Epoch: 92  Loss: 167.2474344365319\n",
            "Epoch: 93  Loss: 167.20480485894805\n",
            "Epoch: 94  Loss: 167.26800763084208\n",
            "Epoch: 95  Loss: 167.25631409083445\n",
            "Epoch: 96  Loss: 167.26282868363097\n",
            "Epoch: 97  Loss: 167.28302848197296\n",
            "Epoch: 98  Loss: 167.27468835053608\n",
            "Epoch: 99  Loss: 167.26434302945205\n",
            "Epoch: 100  Loss: 167.3138327342936\n",
            "Epoch: 101  Loss: 167.30722000751553\n",
            "Epoch: 102  Loss: 167.28054343942833\n",
            "Epoch: 103  Loss: 167.2933586431071\n",
            "Epoch: 104  Loss: 167.28595641903553\n",
            "Epoch: 105  Loss: 167.28217047345873\n",
            "Epoch: 106  Loss: 167.29500477568422\n",
            "Epoch: 107  Loss: 167.2866611139373\n",
            "Epoch: 108  Loss: 167.2926347204097\n",
            "Epoch: 109  Loss: 167.2995076081857\n",
            "Epoch: 110  Loss: 167.28501404187693\n",
            "Epoch: 111  Loss: 167.24623560252988\n",
            "Epoch: 112  Loss: 167.2188411565408\n",
            "Epoch: 113  Loss: 167.18348531404402\n",
            "Epoch: 114  Loss: 167.20431928309898\n",
            "Epoch: 115  Loss: 167.18522347592395\n",
            "Epoch: 116  Loss: 167.14978027712334\n",
            "Epoch: 117  Loss: 167.19628424639086\n",
            "Epoch: 118  Loss: 167.205936741896\n",
            "Epoch: 119  Loss: 167.19049961660747\n",
            "Epoch: 120  Loss: 167.2226257808689\n",
            "Epoch: 121  Loss: 167.2110970892332\n",
            "Epoch: 122  Loss: 167.20988684987938\n",
            "Epoch: 123  Loss: 167.20532133205884\n",
            "Epoch: 124  Loss: 167.2248185079641\n",
            "Epoch: 125  Loss: 167.23634075201164\n",
            "Epoch: 126  Loss: 167.22902380684693\n",
            "Epoch: 127  Loss: 167.19051345423227\n",
            "Epoch: 128  Loss: 167.2487166557809\n",
            "Epoch: 129  Loss: 167.23596472642498\n",
            "Epoch: 130  Loss: 167.24135692009\n",
            "Epoch: 131  Loss: 167.2322146210528\n",
            "Epoch: 132  Loss: 167.2213330743175\n",
            "Epoch: 133  Loss: 167.20886341288926\n",
            "Epoch: 134  Loss: 167.23791242477984\n",
            "Epoch: 135  Loss: 167.24988609275485\n",
            "Epoch: 136  Loss: 167.2480400849078\n",
            "Epoch: 137  Loss: 167.23865894373031\n",
            "Epoch: 138  Loss: 167.2558662432867\n",
            "Epoch: 139  Loss: 167.23579277483574\n",
            "Epoch: 140  Loss: 167.22938599797803\n",
            "Epoch: 141  Loss: 167.24938523806253\n",
            "Epoch: 142  Loss: 167.27524125241652\n",
            "Epoch: 143  Loss: 167.27795762333176\n",
            "Epoch: 144  Loss: 167.25987991971098\n",
            "Epoch: 145  Loss: 167.23096629433434\n",
            "Epoch: 146  Loss: 167.26966530315508\n",
            "Epoch: 147  Loss: 167.21706469966765\n",
            "Epoch: 148  Loss: 167.20911050305682\n",
            "Epoch: 149  Loss: 167.2309572384703\n",
            "Epoch: 150  Loss: 167.21027083006481\n",
            "Epoch: 151  Loss: 167.19905334726235\n",
            "Epoch: 152  Loss: 167.21655525671235\n",
            "Epoch: 153  Loss: 167.197226167045\n",
            "Epoch: 154  Loss: 167.1752210946308\n",
            "Epoch: 155  Loss: 167.18509888829863\n",
            "Epoch: 156  Loss: 167.19151656523442\n",
            "Epoch: 157  Loss: 167.1896563032249\n",
            "Epoch: 158  Loss: 167.2246014976143\n",
            "Epoch: 159  Loss: 167.20187176671004\n",
            "Epoch: 160  Loss: 167.20942455407948\n",
            "Epoch: 161  Loss: 167.22248820128252\n",
            "Epoch: 162  Loss: 167.21580437488373\n",
            "Epoch: 163  Loss: 167.19023792198828\n",
            "Epoch: 164  Loss: 167.1679649563842\n",
            "Epoch: 165  Loss: 167.1554703350947\n",
            "Epoch: 166  Loss: 167.13269173524952\n",
            "Epoch: 167  Loss: 167.1839822874941\n",
            "Epoch: 168  Loss: 167.17489074386685\n",
            "Epoch: 169  Loss: 167.16354030346008\n",
            "Epoch: 170  Loss: 167.17555093819763\n",
            "Epoch: 171  Loss: 167.19646570134682\n",
            "Epoch: 172  Loss: 167.20086220542748\n",
            "Epoch: 173  Loss: 167.17180990670235\n",
            "Epoch: 174  Loss: 167.18004103443906\n",
            "Epoch: 175  Loss: 167.17869070935834\n",
            "Epoch: 176  Loss: 167.170523859107\n",
            "Epoch: 177  Loss: 167.17597779516376\n",
            "Epoch: 178  Loss: 167.22995408518847\n",
            "Epoch: 179  Loss: 167.19813948811634\n",
            "Epoch: 180  Loss: 167.1909748454651\n",
            "Epoch: 181  Loss: 167.21067091487342\n",
            "Epoch: 182  Loss: 167.19310190209774\n",
            "Epoch: 183  Loss: 167.18557347920202\n",
            "Epoch: 184  Loss: 167.17142205923037\n",
            "Epoch: 185  Loss: 167.18821193593988\n",
            "Epoch: 186  Loss: 167.17183513830543\n",
            "Epoch: 187  Loss: 167.16302233732932\n",
            "Epoch: 188  Loss: 167.15206438742092\n",
            "Epoch: 189  Loss: 167.16272819882417\n",
            "Epoch: 190  Loss: 167.14761772036036\n",
            "Epoch: 191  Loss: 167.13079648563223\n",
            "Epoch: 192  Loss: 167.1288526079829\n",
            "Epoch: 193  Loss: 167.14495214698124\n",
            "Epoch: 194  Loss: 167.12849851038777\n",
            "Epoch: 195  Loss: 167.1820574730163\n",
            "Epoch: 196  Loss: 167.16772034363228\n",
            "Epoch: 197  Loss: 167.15968978720775\n",
            "Epoch: 198  Loss: 167.16986146586962\n",
            "Epoch: 199  Loss: 167.17070280555052\n",
            "Epoch: 200  Loss: 167.1819609801689\n",
            "Epoch: 201  Loss: 167.17415587322643\n",
            "Epoch: 202  Loss: 167.2153416988526\n",
            "Epoch: 203  Loss: 167.21265766798615\n",
            "Epoch: 204  Loss: 167.1920283435793\n",
            "Epoch: 205  Loss: 167.19209849717555\n",
            "Epoch: 206  Loss: 167.17827438415645\n",
            "Epoch: 207  Loss: 167.1664498091105\n",
            "Epoch: 208  Loss: 167.14576278108257\n",
            "Epoch: 209  Loss: 167.16691621323872\n",
            "Epoch: 210  Loss: 167.1367941263117\n",
            "Epoch: 211  Loss: 167.12417537367327\n",
            "Epoch: 212  Loss: 167.12568620042393\n",
            "Epoch: 213  Loss: 167.12763399835936\n",
            "Epoch: 214  Loss: 167.097600455416\n",
            "Epoch: 215  Loss: 167.10502951385683\n",
            "Epoch: 216  Loss: 167.105007069928\n",
            "Epoch: 217  Loss: 167.14071363506133\n",
            "Epoch: 218  Loss: 167.1474866554637\n",
            "Epoch: 219  Loss: 167.12571202414043\n",
            "Epoch: 220  Loss: 167.12085513142853\n",
            "Epoch: 221  Loss: 167.13990627508986\n",
            "Epoch: 222  Loss: 167.11505722706997\n",
            "Epoch: 223  Loss: 167.13876359224057\n",
            "Epoch: 224  Loss: 167.12858940509187\n",
            "Epoch: 225  Loss: 167.0995447641591\n",
            "Epoch: 226  Loss: 167.1452141231226\n",
            "Epoch: 227  Loss: 167.14900168740093\n",
            "Epoch: 228  Loss: 167.10786834990733\n",
            "Epoch: 229  Loss: 167.10875742604648\n",
            "Epoch: 230  Loss: 167.1043119610714\n",
            "Epoch: 231  Loss: 167.11606021098723\n",
            "Epoch: 232  Loss: 167.10464488946397\n",
            "Epoch: 233  Loss: 167.0994863601391\n",
            "Epoch: 234  Loss: 167.1118967735097\n",
            "Epoch: 235  Loss: 167.13146499289894\n",
            "Epoch: 236  Loss: 167.1122503491425\n",
            "Epoch: 237  Loss: 167.09953436259292\n",
            "Epoch: 238  Loss: 167.10682015198964\n",
            "Epoch: 239  Loss: 167.0766901458006\n",
            "Epoch: 240  Loss: 167.10361110002492\n",
            "Epoch: 241  Loss: 167.09126552971907\n",
            "Epoch: 242  Loss: 167.10110377726178\n",
            "Epoch: 243  Loss: 167.0981137487904\n",
            "Epoch: 244  Loss: 167.09500499447267\n",
            "Epoch: 245  Loss: 167.08427071670292\n",
            "Epoch: 246  Loss: 167.09371822821396\n",
            "Epoch: 247  Loss: 167.0831000158169\n",
            "Epoch: 248  Loss: 167.10426720837148\n",
            "Epoch: 249  Loss: 167.06842218137413\n",
            "Epoch: 250  Loss: 167.075288001828\n",
            "Epoch: 251  Loss: 167.08367024967058\n",
            "Epoch: 252  Loss: 167.04852829065246\n",
            "Epoch: 253  Loss: 167.05638246390882\n",
            "Epoch: 254  Loss: 167.07586272795047\n",
            "Epoch: 255  Loss: 167.07411008695587\n",
            "Epoch: 256  Loss: 167.08086054945497\n",
            "Epoch: 257  Loss: 167.06155425487185\n",
            "Epoch: 258  Loss: 167.06542757622037\n",
            "Epoch: 259  Loss: 167.06918446477454\n",
            "Epoch: 260  Loss: 167.0359200559472\n",
            "Epoch: 261  Loss: 167.08337001681144\n",
            "Epoch: 262  Loss: 167.06365983275901\n",
            "Epoch: 263  Loss: 167.04428902306017\n",
            "Epoch: 264  Loss: 167.01907589440785\n",
            "Epoch: 265  Loss: 167.0529996713675\n",
            "Epoch: 266  Loss: 167.0294154752054\n",
            "Epoch: 267  Loss: 167.0514862137676\n",
            "Epoch: 268  Loss: 167.021478037036\n",
            "Epoch: 269  Loss: 166.99722760448154\n",
            "Epoch: 270  Loss: 167.0183577858997\n",
            "Epoch: 271  Loss: 167.00089452144042\n",
            "Epoch: 272  Loss: 166.99382870646758\n",
            "Epoch: 273  Loss: 166.9933483086144\n",
            "Epoch: 274  Loss: 166.9894815471074\n",
            "Epoch: 275  Loss: 166.97676364596077\n",
            "Epoch: 276  Loss: 166.97615766907205\n",
            "Epoch: 277  Loss: 166.95655633761817\n",
            "Epoch: 278  Loss: 166.98282074632985\n",
            "Epoch: 279  Loss: 166.98323443336417\n",
            "Epoch: 280  Loss: 166.96791715251456\n",
            "Epoch: 281  Loss: 166.96923111799143\n",
            "Epoch: 282  Loss: 166.98024318899664\n",
            "Epoch: 283  Loss: 166.9722989594008\n",
            "Epoch: 284  Loss: 166.92870291340856\n",
            "Epoch: 285  Loss: 166.93670437779585\n",
            "Epoch: 286  Loss: 166.96452461775482\n",
            "Epoch: 287  Loss: 166.91422575684422\n",
            "Epoch: 288  Loss: 166.9406637162229\n",
            "Epoch: 289  Loss: 166.93979188623086\n",
            "Epoch: 290  Loss: 166.9308309143017\n",
            "Epoch: 291  Loss: 166.9489392216417\n",
            "Epoch: 292  Loss: 166.9292038039911\n",
            "Epoch: 293  Loss: 166.96107740038693\n",
            "Epoch: 294  Loss: 166.9564704543139\n",
            "Epoch: 295  Loss: 166.92626815943913\n",
            "Epoch: 296  Loss: 166.94562726986834\n",
            "Epoch: 297  Loss: 166.92720008414446\n",
            "Epoch: 298  Loss: 166.9863336074287\n",
            "Epoch: 299  Loss: 166.96464479553285\n",
            "Epoch: 300  Loss: 166.96243720258965\n",
            "Done Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jlhjAbTCswZv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Adam"
      ]
    },
    {
      "metadata": {
        "id": "rmB14CZ3syyT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model6  = MLP(dims=[4,10,10,3], activations=[Activations.sigmoid]*3+[Activations.softmax])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kYJ3lk2_v4Wi",
        "colab_type": "code",
        "outputId": "eea2c40e-f78e-4be1-e4b9-1b6d30964b6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9126
        }
      },
      "cell_type": "code",
      "source": [
        "model6.train(x_train=x_train, y_train=y_train, epochs=500, batch_size=20, optimizer=Adam, opt_kwargs=dict(lr=0.0001, beta1=0.7, beta2=0.9, epsilon=1e-8))"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1  Loss: 207.1020570460366\n",
            "Epoch: 2  Loss: 207.0074043157225\n",
            "Epoch: 3  Loss: 206.9243005314945\n",
            "Epoch: 4  Loss: 206.82114386369096\n",
            "Epoch: 5  Loss: 206.72721879592413\n",
            "Epoch: 6  Loss: 206.63416828907998\n",
            "Epoch: 7  Loss: 206.54732060657034\n",
            "Epoch: 8  Loss: 206.45256281431406\n",
            "Epoch: 9  Loss: 206.34767352204537\n",
            "Epoch: 10  Loss: 206.26062306723995\n",
            "Epoch: 11  Loss: 206.16291916652747\n",
            "Epoch: 12  Loss: 206.0746615399479\n",
            "Epoch: 13  Loss: 205.96340755148495\n",
            "Epoch: 14  Loss: 205.85929167445082\n",
            "Epoch: 15  Loss: 205.74980562100384\n",
            "Epoch: 16  Loss: 205.67432143115653\n",
            "Epoch: 17  Loss: 205.57095076863715\n",
            "Epoch: 18  Loss: 205.47213103942445\n",
            "Epoch: 19  Loss: 205.38243111251896\n",
            "Epoch: 20  Loss: 205.28650745898597\n",
            "Epoch: 21  Loss: 205.19975247457955\n",
            "Epoch: 22  Loss: 205.09622472849443\n",
            "Epoch: 23  Loss: 204.99481768785375\n",
            "Epoch: 24  Loss: 204.90515218619856\n",
            "Epoch: 25  Loss: 204.81166033983772\n",
            "Epoch: 26  Loss: 204.69656577343855\n",
            "Epoch: 27  Loss: 204.5851596864015\n",
            "Epoch: 28  Loss: 204.48857212047116\n",
            "Epoch: 29  Loss: 204.39274667943616\n",
            "Epoch: 30  Loss: 204.28721245276645\n",
            "Epoch: 31  Loss: 204.18473978177067\n",
            "Epoch: 32  Loss: 204.08147770294818\n",
            "Epoch: 33  Loss: 203.99559493878658\n",
            "Epoch: 34  Loss: 203.88157925835938\n",
            "Epoch: 35  Loss: 203.77370568663522\n",
            "Epoch: 36  Loss: 203.67890687166823\n",
            "Epoch: 37  Loss: 203.58192509675857\n",
            "Epoch: 38  Loss: 203.4851479124258\n",
            "Epoch: 39  Loss: 203.39196157515602\n",
            "Epoch: 40  Loss: 203.2781811452401\n",
            "Epoch: 41  Loss: 203.16968430856573\n",
            "Epoch: 42  Loss: 203.06774675315356\n",
            "Epoch: 43  Loss: 202.96900445803766\n",
            "Epoch: 44  Loss: 202.87064408516528\n",
            "Epoch: 45  Loss: 202.74539286991796\n",
            "Epoch: 46  Loss: 202.66515018497378\n",
            "Epoch: 47  Loss: 202.57136631851006\n",
            "Epoch: 48  Loss: 202.46478406853447\n",
            "Epoch: 49  Loss: 202.37092223186815\n",
            "Epoch: 50  Loss: 202.27626832844348\n",
            "Epoch: 51  Loss: 202.18623968959486\n",
            "Epoch: 52  Loss: 202.09307523749004\n",
            "Epoch: 53  Loss: 201.98585355349093\n",
            "Epoch: 54  Loss: 201.88374713324845\n",
            "Epoch: 55  Loss: 201.7804607232012\n",
            "Epoch: 56  Loss: 201.69619741524826\n",
            "Epoch: 57  Loss: 201.61438566688688\n",
            "Epoch: 58  Loss: 201.50691255303747\n",
            "Epoch: 59  Loss: 201.40002057075068\n",
            "Epoch: 60  Loss: 201.29641636852605\n",
            "Epoch: 61  Loss: 201.2107452779624\n",
            "Epoch: 62  Loss: 201.09452271648124\n",
            "Epoch: 63  Loss: 201.00411721467128\n",
            "Epoch: 64  Loss: 200.911372483042\n",
            "Epoch: 65  Loss: 200.81758614287367\n",
            "Epoch: 66  Loss: 200.72108264014597\n",
            "Epoch: 67  Loss: 200.63375839989592\n",
            "Epoch: 68  Loss: 200.5568953677762\n",
            "Epoch: 69  Loss: 200.43604301124324\n",
            "Epoch: 70  Loss: 200.34686283761707\n",
            "Epoch: 71  Loss: 200.25485048962886\n",
            "Epoch: 72  Loss: 200.1720206241181\n",
            "Epoch: 73  Loss: 200.0858531178306\n",
            "Epoch: 74  Loss: 199.98658566959645\n",
            "Epoch: 75  Loss: 199.89863778246774\n",
            "Epoch: 76  Loss: 199.78969607127013\n",
            "Epoch: 77  Loss: 199.7148094143453\n",
            "Epoch: 78  Loss: 199.6176602578602\n",
            "Epoch: 79  Loss: 199.52251284490734\n",
            "Epoch: 80  Loss: 199.42794171940648\n",
            "Epoch: 81  Loss: 199.33842273310515\n",
            "Epoch: 82  Loss: 199.26538905638552\n",
            "Epoch: 83  Loss: 199.18380855982153\n",
            "Epoch: 84  Loss: 199.0820853639371\n",
            "Epoch: 85  Loss: 198.98286537693033\n",
            "Epoch: 86  Loss: 198.8920969488076\n",
            "Epoch: 87  Loss: 198.78884065326847\n",
            "Epoch: 88  Loss: 198.69126777494876\n",
            "Epoch: 89  Loss: 198.59448998021452\n",
            "Epoch: 90  Loss: 198.5035072006216\n",
            "Epoch: 91  Loss: 198.40639655112858\n",
            "Epoch: 92  Loss: 198.31172227764736\n",
            "Epoch: 93  Loss: 198.22026146132126\n",
            "Epoch: 94  Loss: 198.1302874100821\n",
            "Epoch: 95  Loss: 198.03314003520498\n",
            "Epoch: 96  Loss: 197.94629077363373\n",
            "Epoch: 97  Loss: 197.8541360472019\n",
            "Epoch: 98  Loss: 197.75900227254462\n",
            "Epoch: 99  Loss: 197.67900344843196\n",
            "Epoch: 100  Loss: 197.59802914984556\n",
            "Epoch: 101  Loss: 197.51128800060295\n",
            "Epoch: 102  Loss: 197.42948674101055\n",
            "Epoch: 103  Loss: 197.341036682467\n",
            "Epoch: 104  Loss: 197.2377788199773\n",
            "Epoch: 105  Loss: 197.14199230670292\n",
            "Epoch: 106  Loss: 197.07037920270704\n",
            "Epoch: 107  Loss: 196.97721459796622\n",
            "Epoch: 108  Loss: 196.89229866824076\n",
            "Epoch: 109  Loss: 196.82253606037955\n",
            "Epoch: 110  Loss: 196.73038668198382\n",
            "Epoch: 111  Loss: 196.621752019182\n",
            "Epoch: 112  Loss: 196.53709741896623\n",
            "Epoch: 113  Loss: 196.44580621680586\n",
            "Epoch: 114  Loss: 196.34965000408957\n",
            "Epoch: 115  Loss: 196.26584812983953\n",
            "Epoch: 116  Loss: 196.15533639351264\n",
            "Epoch: 117  Loss: 196.06363857162034\n",
            "Epoch: 118  Loss: 195.98832517464893\n",
            "Epoch: 119  Loss: 195.894668657949\n",
            "Epoch: 120  Loss: 195.78812990698464\n",
            "Epoch: 121  Loss: 195.70200930913282\n",
            "Epoch: 122  Loss: 195.60280424065857\n",
            "Epoch: 123  Loss: 195.52135537770096\n",
            "Epoch: 124  Loss: 195.43852230568646\n",
            "Epoch: 125  Loss: 195.33492096287804\n",
            "Epoch: 126  Loss: 195.25005944999097\n",
            "Epoch: 127  Loss: 195.1630417166314\n",
            "Epoch: 128  Loss: 195.06399130973242\n",
            "Epoch: 129  Loss: 194.96774665637946\n",
            "Epoch: 130  Loss: 194.88392734042418\n",
            "Epoch: 131  Loss: 194.8110794167613\n",
            "Epoch: 132  Loss: 194.72238909859172\n",
            "Epoch: 133  Loss: 194.63745317092798\n",
            "Epoch: 134  Loss: 194.55485511897263\n",
            "Epoch: 135  Loss: 194.48128958333064\n",
            "Epoch: 136  Loss: 194.40909952313146\n",
            "Epoch: 137  Loss: 194.33052549005458\n",
            "Epoch: 138  Loss: 194.25815098067721\n",
            "Epoch: 139  Loss: 194.1819784729497\n",
            "Epoch: 140  Loss: 194.09468265946595\n",
            "Epoch: 141  Loss: 194.00038007295979\n",
            "Epoch: 142  Loss: 193.90484429220828\n",
            "Epoch: 143  Loss: 193.80754813259375\n",
            "Epoch: 144  Loss: 193.7135570319757\n",
            "Epoch: 145  Loss: 193.6322006685802\n",
            "Epoch: 146  Loss: 193.55947784286695\n",
            "Epoch: 147  Loss: 193.46022893120116\n",
            "Epoch: 148  Loss: 193.3739941948829\n",
            "Epoch: 149  Loss: 193.296411594525\n",
            "Epoch: 150  Loss: 193.2115776350632\n",
            "Epoch: 151  Loss: 193.12748531724887\n",
            "Epoch: 152  Loss: 193.04567732015502\n",
            "Epoch: 153  Loss: 192.95671334123182\n",
            "Epoch: 154  Loss: 192.84243968295797\n",
            "Epoch: 155  Loss: 192.75585869957712\n",
            "Epoch: 156  Loss: 192.67419686703767\n",
            "Epoch: 157  Loss: 192.59813693979754\n",
            "Epoch: 158  Loss: 192.5008312112378\n",
            "Epoch: 159  Loss: 192.42590258702347\n",
            "Epoch: 160  Loss: 192.3392526884242\n",
            "Epoch: 161  Loss: 192.25476307352403\n",
            "Epoch: 162  Loss: 192.16069552141073\n",
            "Epoch: 163  Loss: 192.07104350749285\n",
            "Epoch: 164  Loss: 191.98082190714717\n",
            "Epoch: 165  Loss: 191.87340352538334\n",
            "Epoch: 166  Loss: 191.78604000141226\n",
            "Epoch: 167  Loss: 191.69709055024498\n",
            "Epoch: 168  Loss: 191.6082858659601\n",
            "Epoch: 169  Loss: 191.52718490134947\n",
            "Epoch: 170  Loss: 191.43446487904657\n",
            "Epoch: 171  Loss: 191.35356404909385\n",
            "Epoch: 172  Loss: 191.2715753419498\n",
            "Epoch: 173  Loss: 191.18167164690811\n",
            "Epoch: 174  Loss: 191.09755020504784\n",
            "Epoch: 175  Loss: 191.01281465507688\n",
            "Epoch: 176  Loss: 190.91241307551974\n",
            "Epoch: 177  Loss: 190.8097724015818\n",
            "Epoch: 178  Loss: 190.70527027612562\n",
            "Epoch: 179  Loss: 190.64201195826664\n",
            "Epoch: 180  Loss: 190.54898033137042\n",
            "Epoch: 181  Loss: 190.4596424186183\n",
            "Epoch: 182  Loss: 190.37485350316234\n",
            "Epoch: 183  Loss: 190.28756197266756\n",
            "Epoch: 184  Loss: 190.19362892061685\n",
            "Epoch: 185  Loss: 190.0842139798537\n",
            "Epoch: 186  Loss: 189.9986269470572\n",
            "Epoch: 187  Loss: 189.91948259526396\n",
            "Epoch: 188  Loss: 189.82540408450834\n",
            "Epoch: 189  Loss: 189.73463964657634\n",
            "Epoch: 190  Loss: 189.64133010708713\n",
            "Epoch: 191  Loss: 189.5599510112507\n",
            "Epoch: 192  Loss: 189.47174987274272\n",
            "Epoch: 193  Loss: 189.3893193221338\n",
            "Epoch: 194  Loss: 189.28827211543447\n",
            "Epoch: 195  Loss: 189.18764607129583\n",
            "Epoch: 196  Loss: 189.08709168347548\n",
            "Epoch: 197  Loss: 188.9945540127793\n",
            "Epoch: 198  Loss: 188.90308140663376\n",
            "Epoch: 199  Loss: 188.83437117766573\n",
            "Epoch: 200  Loss: 188.7420815009669\n",
            "Epoch: 201  Loss: 188.6575237111839\n",
            "Epoch: 202  Loss: 188.58733290887244\n",
            "Epoch: 203  Loss: 188.5029759154464\n",
            "Epoch: 204  Loss: 188.41362091836706\n",
            "Epoch: 205  Loss: 188.32439508875052\n",
            "Epoch: 206  Loss: 188.24684841344572\n",
            "Epoch: 207  Loss: 188.15072879900072\n",
            "Epoch: 208  Loss: 188.04047340440448\n",
            "Epoch: 209  Loss: 187.95325032061032\n",
            "Epoch: 210  Loss: 187.87629015534816\n",
            "Epoch: 211  Loss: 187.78190765354813\n",
            "Epoch: 212  Loss: 187.71082295026966\n",
            "Epoch: 213  Loss: 187.63482229051684\n",
            "Epoch: 214  Loss: 187.544030791\n",
            "Epoch: 215  Loss: 187.46107547817843\n",
            "Epoch: 216  Loss: 187.38869090993398\n",
            "Epoch: 217  Loss: 187.2979775967779\n",
            "Epoch: 218  Loss: 187.22028865636142\n",
            "Epoch: 219  Loss: 187.13055392805256\n",
            "Epoch: 220  Loss: 187.0462814541232\n",
            "Epoch: 221  Loss: 186.96543347873592\n",
            "Epoch: 222  Loss: 186.88842056187698\n",
            "Epoch: 223  Loss: 186.77787381353835\n",
            "Epoch: 224  Loss: 186.67742457543275\n",
            "Epoch: 225  Loss: 186.57116835438515\n",
            "Epoch: 226  Loss: 186.48885470293519\n",
            "Epoch: 227  Loss: 186.3972340885037\n",
            "Epoch: 228  Loss: 186.30295041335827\n",
            "Epoch: 229  Loss: 186.19679233998414\n",
            "Epoch: 230  Loss: 186.11016538128024\n",
            "Epoch: 231  Loss: 186.01398637955185\n",
            "Epoch: 232  Loss: 185.92489615151885\n",
            "Epoch: 233  Loss: 185.84951960355926\n",
            "Epoch: 234  Loss: 185.76894048163837\n",
            "Epoch: 235  Loss: 185.69373159611075\n",
            "Epoch: 236  Loss: 185.60602742943536\n",
            "Epoch: 237  Loss: 185.5195183239017\n",
            "Epoch: 238  Loss: 185.4229156666484\n",
            "Epoch: 239  Loss: 185.34148550425502\n",
            "Epoch: 240  Loss: 185.2518696091373\n",
            "Epoch: 241  Loss: 185.15318596176365\n",
            "Epoch: 242  Loss: 185.08089642079386\n",
            "Epoch: 243  Loss: 184.99831097901213\n",
            "Epoch: 244  Loss: 184.90516413919255\n",
            "Epoch: 245  Loss: 184.8250445573461\n",
            "Epoch: 246  Loss: 184.71895877136345\n",
            "Epoch: 247  Loss: 184.61769243526714\n",
            "Epoch: 248  Loss: 184.53518342062105\n",
            "Epoch: 249  Loss: 184.43889882847816\n",
            "Epoch: 250  Loss: 184.35796439160237\n",
            "Epoch: 251  Loss: 184.2796952547021\n",
            "Epoch: 252  Loss: 184.19192462576382\n",
            "Epoch: 253  Loss: 184.10312967505064\n",
            "Epoch: 254  Loss: 184.02942036553995\n",
            "Epoch: 255  Loss: 183.9436934185994\n",
            "Epoch: 256  Loss: 183.84542897659105\n",
            "Epoch: 257  Loss: 183.76137982934188\n",
            "Epoch: 258  Loss: 183.67405192253128\n",
            "Epoch: 259  Loss: 183.5910028673177\n",
            "Epoch: 260  Loss: 183.50862019614533\n",
            "Epoch: 261  Loss: 183.4224762873748\n",
            "Epoch: 262  Loss: 183.33849955196024\n",
            "Epoch: 263  Loss: 183.2620882314734\n",
            "Epoch: 264  Loss: 183.17572726429802\n",
            "Epoch: 265  Loss: 183.11019308671914\n",
            "Epoch: 266  Loss: 183.0344895166854\n",
            "Epoch: 267  Loss: 182.95675729359272\n",
            "Epoch: 268  Loss: 182.87100682621013\n",
            "Epoch: 269  Loss: 182.79683744209862\n",
            "Epoch: 270  Loss: 182.70638364037356\n",
            "Epoch: 271  Loss: 182.6256328336327\n",
            "Epoch: 272  Loss: 182.5463468915849\n",
            "Epoch: 273  Loss: 182.44301208923895\n",
            "Epoch: 274  Loss: 182.36799528969053\n",
            "Epoch: 275  Loss: 182.28535084465364\n",
            "Epoch: 276  Loss: 182.20554915030448\n",
            "Epoch: 277  Loss: 182.1271044565603\n",
            "Epoch: 278  Loss: 182.0441212371285\n",
            "Epoch: 279  Loss: 181.96228257708012\n",
            "Epoch: 280  Loss: 181.87687925447344\n",
            "Epoch: 281  Loss: 181.79240865391165\n",
            "Epoch: 282  Loss: 181.71990891263897\n",
            "Epoch: 283  Loss: 181.63354849557294\n",
            "Epoch: 284  Loss: 181.55139371147135\n",
            "Epoch: 285  Loss: 181.46946755371323\n",
            "Epoch: 286  Loss: 181.38762330683602\n",
            "Epoch: 287  Loss: 181.30827237418356\n",
            "Epoch: 288  Loss: 181.25588390333442\n",
            "Epoch: 289  Loss: 181.17380913402857\n",
            "Epoch: 290  Loss: 181.09211806224428\n",
            "Epoch: 291  Loss: 181.00942907698808\n",
            "Epoch: 292  Loss: 180.91893232373067\n",
            "Epoch: 293  Loss: 180.84850698773485\n",
            "Epoch: 294  Loss: 180.77515611794553\n",
            "Epoch: 295  Loss: 180.68971917644075\n",
            "Epoch: 296  Loss: 180.6090896320378\n",
            "Epoch: 297  Loss: 180.54769160334249\n",
            "Epoch: 298  Loss: 180.4598329887033\n",
            "Epoch: 299  Loss: 180.3759734521922\n",
            "Epoch: 300  Loss: 180.31726915140322\n",
            "Epoch: 301  Loss: 180.23109852938197\n",
            "Epoch: 302  Loss: 180.1362849715435\n",
            "Epoch: 303  Loss: 180.0716125678577\n",
            "Epoch: 304  Loss: 179.9998500899835\n",
            "Epoch: 305  Loss: 179.90533610840058\n",
            "Epoch: 306  Loss: 179.8334362601022\n",
            "Epoch: 307  Loss: 179.76567962454692\n",
            "Epoch: 308  Loss: 179.6843757118357\n",
            "Epoch: 309  Loss: 179.60581333722976\n",
            "Epoch: 310  Loss: 179.5140795011189\n",
            "Epoch: 311  Loss: 179.43924346272837\n",
            "Epoch: 312  Loss: 179.3570108089749\n",
            "Epoch: 313  Loss: 179.2895049862915\n",
            "Epoch: 314  Loss: 179.2214137546769\n",
            "Epoch: 315  Loss: 179.15654611357758\n",
            "Epoch: 316  Loss: 179.07773251524313\n",
            "Epoch: 317  Loss: 179.0016659381154\n",
            "Epoch: 318  Loss: 178.92040696005722\n",
            "Epoch: 319  Loss: 178.85149512444409\n",
            "Epoch: 320  Loss: 178.78588056093275\n",
            "Epoch: 321  Loss: 178.71100181526765\n",
            "Epoch: 322  Loss: 178.6382721493752\n",
            "Epoch: 323  Loss: 178.57790307937563\n",
            "Epoch: 324  Loss: 178.5148648872627\n",
            "Epoch: 325  Loss: 178.43304162630042\n",
            "Epoch: 326  Loss: 178.3667496563903\n",
            "Epoch: 327  Loss: 178.28574258551546\n",
            "Epoch: 328  Loss: 178.2197554851721\n",
            "Epoch: 329  Loss: 178.13610548249602\n",
            "Epoch: 330  Loss: 178.06758788966485\n",
            "Epoch: 331  Loss: 178.00091449736348\n",
            "Epoch: 332  Loss: 177.91892261771352\n",
            "Epoch: 333  Loss: 177.8502859186191\n",
            "Epoch: 334  Loss: 177.77902690523746\n",
            "Epoch: 335  Loss: 177.7065543140078\n",
            "Epoch: 336  Loss: 177.6393525465864\n",
            "Epoch: 337  Loss: 177.55673427878332\n",
            "Epoch: 338  Loss: 177.488429321168\n",
            "Epoch: 339  Loss: 177.42905594537567\n",
            "Epoch: 340  Loss: 177.37400971112652\n",
            "Epoch: 341  Loss: 177.30907553659276\n",
            "Epoch: 342  Loss: 177.23280130647083\n",
            "Epoch: 343  Loss: 177.15603337161343\n",
            "Epoch: 344  Loss: 177.09271581155159\n",
            "Epoch: 345  Loss: 177.0296818115495\n",
            "Epoch: 346  Loss: 176.95446585729292\n",
            "Epoch: 347  Loss: 176.87570037517636\n",
            "Epoch: 348  Loss: 176.81377377292029\n",
            "Epoch: 349  Loss: 176.7499984412234\n",
            "Epoch: 350  Loss: 176.68262832450534\n",
            "Epoch: 351  Loss: 176.61117898265988\n",
            "Epoch: 352  Loss: 176.53570471597203\n",
            "Epoch: 353  Loss: 176.46985080110505\n",
            "Epoch: 354  Loss: 176.40435436466643\n",
            "Epoch: 355  Loss: 176.3306742814251\n",
            "Epoch: 356  Loss: 176.26627453929837\n",
            "Epoch: 357  Loss: 176.21151598572436\n",
            "Epoch: 358  Loss: 176.12998099035892\n",
            "Epoch: 359  Loss: 176.05869050597812\n",
            "Epoch: 360  Loss: 175.9938087264999\n",
            "Epoch: 361  Loss: 175.94227189549704\n",
            "Epoch: 362  Loss: 175.87016486922562\n",
            "Epoch: 363  Loss: 175.80059845295756\n",
            "Epoch: 364  Loss: 175.7375576083071\n",
            "Epoch: 365  Loss: 175.6781820045252\n",
            "Epoch: 366  Loss: 175.62335712118508\n",
            "Epoch: 367  Loss: 175.54212274461216\n",
            "Epoch: 368  Loss: 175.46840368955566\n",
            "Epoch: 369  Loss: 175.40609443794733\n",
            "Epoch: 370  Loss: 175.34219471322197\n",
            "Epoch: 371  Loss: 175.27650391220638\n",
            "Epoch: 372  Loss: 175.21343893522743\n",
            "Epoch: 373  Loss: 175.14715919458328\n",
            "Epoch: 374  Loss: 175.08000239824074\n",
            "Epoch: 375  Loss: 175.024058439195\n",
            "Epoch: 376  Loss: 174.96579385480499\n",
            "Epoch: 377  Loss: 174.8932919494835\n",
            "Epoch: 378  Loss: 174.82898886296388\n",
            "Epoch: 379  Loss: 174.76254999245893\n",
            "Epoch: 380  Loss: 174.70103760604792\n",
            "Epoch: 381  Loss: 174.6482594398065\n",
            "Epoch: 382  Loss: 174.58848282978167\n",
            "Epoch: 383  Loss: 174.5163827586158\n",
            "Epoch: 384  Loss: 174.44480212675376\n",
            "Epoch: 385  Loss: 174.38328695704865\n",
            "Epoch: 386  Loss: 174.3239925273577\n",
            "Epoch: 387  Loss: 174.25172869759436\n",
            "Epoch: 388  Loss: 174.18442451430602\n",
            "Epoch: 389  Loss: 174.12321105506348\n",
            "Epoch: 390  Loss: 174.06281505036702\n",
            "Epoch: 391  Loss: 173.99036708041882\n",
            "Epoch: 392  Loss: 173.9316792416101\n",
            "Epoch: 393  Loss: 173.8766971980055\n",
            "Epoch: 394  Loss: 173.807648107368\n",
            "Epoch: 395  Loss: 173.74862966447228\n",
            "Epoch: 396  Loss: 173.68007552892095\n",
            "Epoch: 397  Loss: 173.6167649509472\n",
            "Epoch: 398  Loss: 173.54395554481871\n",
            "Epoch: 399  Loss: 173.48532416073667\n",
            "Epoch: 400  Loss: 173.4242908363472\n",
            "Epoch: 401  Loss: 173.3728272103046\n",
            "Epoch: 402  Loss: 173.30563568988484\n",
            "Epoch: 403  Loss: 173.24212902534586\n",
            "Epoch: 404  Loss: 173.17887040255818\n",
            "Epoch: 405  Loss: 173.12079486841753\n",
            "Epoch: 406  Loss: 173.07170724357593\n",
            "Epoch: 407  Loss: 172.99447215832276\n",
            "Epoch: 408  Loss: 172.929430764419\n",
            "Epoch: 409  Loss: 172.87299112239165\n",
            "Epoch: 410  Loss: 172.81532289052146\n",
            "Epoch: 411  Loss: 172.76561730202636\n",
            "Epoch: 412  Loss: 172.69439022165278\n",
            "Epoch: 413  Loss: 172.6319868106172\n",
            "Epoch: 414  Loss: 172.5840355217024\n",
            "Epoch: 415  Loss: 172.5058517920517\n",
            "Epoch: 416  Loss: 172.44480855321683\n",
            "Epoch: 417  Loss: 172.3896906302813\n",
            "Epoch: 418  Loss: 172.33867818157012\n",
            "Epoch: 419  Loss: 172.28959171407428\n",
            "Epoch: 420  Loss: 172.22679691383047\n",
            "Epoch: 421  Loss: 172.16006012177868\n",
            "Epoch: 422  Loss: 172.08916744920674\n",
            "Epoch: 423  Loss: 172.03203643958045\n",
            "Epoch: 424  Loss: 171.98396603061065\n",
            "Epoch: 425  Loss: 171.91928887246462\n",
            "Epoch: 426  Loss: 171.86374484634405\n",
            "Epoch: 427  Loss: 171.80612203178828\n",
            "Epoch: 428  Loss: 171.75282122678408\n",
            "Epoch: 429  Loss: 171.68581235233577\n",
            "Epoch: 430  Loss: 171.61765333953642\n",
            "Epoch: 431  Loss: 171.56124660962038\n",
            "Epoch: 432  Loss: 171.51177780401244\n",
            "Epoch: 433  Loss: 171.45992922005755\n",
            "Epoch: 434  Loss: 171.39442901486828\n",
            "Epoch: 435  Loss: 171.33358004258446\n",
            "Epoch: 436  Loss: 171.2895557604266\n",
            "Epoch: 437  Loss: 171.2253540401919\n",
            "Epoch: 438  Loss: 171.16748398533719\n",
            "Epoch: 439  Loss: 171.11423818659108\n",
            "Epoch: 440  Loss: 171.0573838321407\n",
            "Epoch: 441  Loss: 170.9963278899058\n",
            "Epoch: 442  Loss: 170.94706658556905\n",
            "Epoch: 443  Loss: 170.87942644904646\n",
            "Epoch: 444  Loss: 170.81799814478518\n",
            "Epoch: 445  Loss: 170.76339676719945\n",
            "Epoch: 446  Loss: 170.71042474958472\n",
            "Epoch: 447  Loss: 170.64777579539015\n",
            "Epoch: 448  Loss: 170.5995077797833\n",
            "Epoch: 449  Loss: 170.5471167441595\n",
            "Epoch: 450  Loss: 170.48838076595436\n",
            "Epoch: 451  Loss: 170.43142399316574\n",
            "Epoch: 452  Loss: 170.3930348687809\n",
            "Epoch: 453  Loss: 170.33515855797415\n",
            "Epoch: 454  Loss: 170.27711013299617\n",
            "Epoch: 455  Loss: 170.22303906312692\n",
            "Epoch: 456  Loss: 170.1621184408698\n",
            "Epoch: 457  Loss: 170.10543396314063\n",
            "Epoch: 458  Loss: 170.04661929870707\n",
            "Epoch: 459  Loss: 170.0040186212857\n",
            "Epoch: 460  Loss: 169.94268481434597\n",
            "Epoch: 461  Loss: 169.89093616858074\n",
            "Epoch: 462  Loss: 169.85286904371608\n",
            "Epoch: 463  Loss: 169.7909669693299\n",
            "Epoch: 464  Loss: 169.7362144819148\n",
            "Epoch: 465  Loss: 169.685071256328\n",
            "Epoch: 466  Loss: 169.63279090392535\n",
            "Epoch: 467  Loss: 169.58953016815067\n",
            "Epoch: 468  Loss: 169.52590322486614\n",
            "Epoch: 469  Loss: 169.47158694679624\n",
            "Epoch: 470  Loss: 169.41261981230838\n",
            "Epoch: 471  Loss: 169.36177060120568\n",
            "Epoch: 472  Loss: 169.32062405776904\n",
            "Epoch: 473  Loss: 169.26740979987858\n",
            "Epoch: 474  Loss: 169.21635162107845\n",
            "Epoch: 475  Loss: 169.1639900029161\n",
            "Epoch: 476  Loss: 169.1007285702686\n",
            "Epoch: 477  Loss: 169.04164062884135\n",
            "Epoch: 478  Loss: 168.99674743222573\n",
            "Epoch: 479  Loss: 168.9481929790169\n",
            "Epoch: 480  Loss: 168.89728338805335\n",
            "Epoch: 481  Loss: 168.84307048391645\n",
            "Epoch: 482  Loss: 168.8105818837496\n",
            "Epoch: 483  Loss: 168.75445918970146\n",
            "Epoch: 484  Loss: 168.70146630880507\n",
            "Epoch: 485  Loss: 168.65526822441234\n",
            "Epoch: 486  Loss: 168.60614741281495\n",
            "Epoch: 487  Loss: 168.54647695381348\n",
            "Epoch: 488  Loss: 168.5006591146025\n",
            "Epoch: 489  Loss: 168.44964792259057\n",
            "Epoch: 490  Loss: 168.3972627326006\n",
            "Epoch: 491  Loss: 168.33544673203303\n",
            "Epoch: 492  Loss: 168.29082744133797\n",
            "Epoch: 493  Loss: 168.25284172136918\n",
            "Epoch: 494  Loss: 168.19560586693632\n",
            "Epoch: 495  Loss: 168.15106228808068\n",
            "Epoch: 496  Loss: 168.11486751101904\n",
            "Epoch: 497  Loss: 168.0606583229473\n",
            "Epoch: 498  Loss: 168.01171848844479\n",
            "Epoch: 499  Loss: 167.96017794677402\n",
            "Epoch: 500  Loss: 167.918148187295\n",
            "Done Training\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}