{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an autoencoder with the sparsity constraint. You can build on the MLP implementation from EE5600. Choose your network size appropriately (meaning a size that you can train and test on your computer without running into memory issues)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the MNIST data from keras, downsample and save to csv file\n",
    "# No need to run this chunk of code again as csv files with 14x14 images are saved \n",
    "# Each image is saved as 196 dim vector with label as it's label\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = np.reshape(x_train, newshape=(*x_train.shape, 1))\n",
    "x_train = tf.image.resize_images(images=x_train, size=(14,14))\n",
    "x = tf.Session().run(x_train)\n",
    "x_train = np.asarray(x, dtype=np.uint8).reshape(x_train.shape[0], 196) / 255.\n",
    "# pd.DataFrame(x_train).to_csv('train.csv', sep=',', index=True, header=False)\n",
    "\n",
    "x_test = np.reshape(x_test, newshape=(*x_test.shape, 1))\n",
    "x_test = tf.image.resize_images(images=x_test, size=(14,14))\n",
    "x = tf.Session().run(x_test)\n",
    "x_test = np.asarray(x, dtype=np.uint8).reshape(x_test.shape[0], 196) / 255.\n",
    "# pd.DataFrame(x_test).to_csv('train.csv', sep=',', index=True, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def show_digit(x): # x - 196 dim vector\n",
    "    x = np.reshape(x, (14,14))\n",
    "    plt.imshow(x, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "    def __init__(self, input_dim, hidden_dim, learn_rate, sparsity, regularization):\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = input_dim\n",
    "        self.learn_rate = learn_rate\n",
    "        self.s = sparsity\n",
    "        self.Lambda = regularization\n",
    "        # intialize weights\n",
    "        self.A = np.random.normal(0,  1, (self.hidden_dim, self.input_dim))\n",
    "        self.B = np.random.normal(0,  1, (self.output_dim, self.hidden_dim))\n",
    "        self.a0 = np.random.normal(0, 1, self.hidden_dim)\n",
    "        self.b0 = np.random.normal(0, 1, self.output_dim)\n",
    "\n",
    "    def sigmoid(self, t):\n",
    "        return 1/(1  + np.exp(-t))\n",
    "\n",
    "    def dsigmoid(self, t):\n",
    "        sigt = self.sigmoid(t)\n",
    "        return sigt*(1-sigt)\n",
    "\n",
    "    def hidden_layer(self, x):\n",
    "        # A.shape:mxd; x.shape;(d,); so z.shape=(m,)\n",
    "        z = self.sigmoid(np.dot(x, self.A.T) + self.a0) \n",
    "        return z\n",
    "\n",
    "    def forward_pass(self, x):\n",
    "        # B.shape:kxm; z.shape;m; so y_hat.shape=k\n",
    "        y_hat = self.sigmoid(np.dot(self.hidden_layer(x), self.B.T) + self.b0)\n",
    "        return y_hat\n",
    "    \n",
    "    def back_propogate(self, X, Y, Y_hat):\n",
    "        self.N = len(X)\n",
    "        dSSE_A, dSSE_a0 = np.zeros_like(self.A), np.zeros_like(self.a0)\n",
    "        dSSE_B, dSSE_b0 = np.zeros_like(self.B), np.zeros_like(self.b0)\n",
    "        Z = self.hidden_layer(X) # Z.shape = (N,m)\n",
    "        dZ = Z * (1-Z) # dZ shape = (N,m)\n",
    "#         print('Z.shape = ', Z.shape, dZ.shape)\n",
    "        # y_delta.shape (N,k)\n",
    "        y_delta = 2*(Y_hat-Y) * self.dsigmoid(self.b0 + np.dot(Z, self.B.T))\n",
    "#         print('Y_delta shape', y_delta.shape)\n",
    "        # z_delta.shape (N,m)\n",
    "        z_delta = np.transpose(-dZ.T * np.sum(y_delta, axis=1))\n",
    "#         print('Z delta Shape', z_delta.shape)\n",
    "        zm = np.mean(Z, axis=0)\n",
    "#         print('Zm', zm, zm.shape, 'the fuck', Z)\n",
    "        # dKL shape = (N,m) \n",
    "        dKL = (-self.s/zm) + ((1-self.s)/(1-zm)) # shape: (m,)\n",
    "#         print('fuck you', dKL)\n",
    "        dKL = self.Lambda * dKL * dZ  # shape: (N,m)\n",
    "#         print('KL ', dKL.shape)\n",
    "        dSSE_A = np.matmul((z_delta + dKL).T, X)\n",
    "#         print('dSSE_A', dSSE_A.shape)\n",
    "        dSSE_a0 = np.sum((z_delta+dKL), axis=0)\n",
    "#         print('dSSE_a0', dSSE_a0.shape)\n",
    "        dSSE_B = np.matmul(y_delta.T, Z)\n",
    "        dSSE_b0 = np.sum(y_delta, axis=0)\n",
    "        # update weights \n",
    "        A_new = self.A - (self.learn_rate*dSSE_A)\n",
    "        a0_new = self.a0 - (self.learn_rate*dSSE_a0)\n",
    "        B_new = self.B - (self.learn_rate*dSSE_B)\n",
    "        b0_new = self.b0 - (self.learn_rate*dSSE_b0)\n",
    "        return [A_new, a0_new, B_new, b0_new]\n",
    "    \n",
    "    def loss(self, y_train, y_hat):\n",
    "        return np.sum((y_train - y_hat)**2) \n",
    "    \n",
    "    def train(self, x_train, y_train, epochs, shuffle=True): \n",
    "        if shuffle:\n",
    "            indices = np.arange(N)\n",
    "            np.random.shuffle(indices)\n",
    "            x_train, y_train = x_train[indices], y_train[indices]\n",
    "\n",
    "        epoch = 1\n",
    "        while(epoch <= epochs):\n",
    "            Y_hat = np.array([self.forward_pass(x) for x in x_train])\n",
    "            [self.A, self.a0, self.B, self.b0] = self.back_propogate(X=x_train, Y=y_train, Y_hat=Y_hat)\n",
    "            print('Epoch: ', epoch, 'Loss: ', self.loss(y_train, Y_hat))\n",
    "            # show_digit(x_train[0])\n",
    "#             show_digit(Y_hat[0])\n",
    "            epoch += 1\n",
    "        print('Done Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SparseAE = NN(input_dim=196, hidden_dim=225, learn_rate=1e-4, sparsity=0.1, regularization=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  5444631.479909311\n",
      "Epoch:  2 Loss:  2581558.107373863\n",
      "Epoch:  3 Loss:  2704771.430768816\n",
      "Epoch:  4 Loss:  1753073.6257336747\n",
      "Epoch:  5 Loss:  1573108.019140351\n",
      "Epoch:  6 Loss:  1447908.7647068172\n",
      "Epoch:  7 Loss:  1386337.1726443556\n",
      "Epoch:  8 Loss:  1298965.314197948\n",
      "Epoch:  9 Loss:  1259406.8262141324\n",
      "Epoch:  10 Loss:  1189933.6665408448\n",
      "Epoch:  11 Loss:  1166223.3075929396\n",
      "Epoch:  12 Loss:  1082987.2364296035\n",
      "Epoch:  13 Loss:  1092792.7305459108\n",
      "Epoch:  14 Loss:  1020579.550450141\n",
      "Epoch:  15 Loss:  1052088.5451923013\n",
      "Epoch:  16 Loss:  971576.6481220872\n",
      "Epoch:  17 Loss:  1015963.2606101857\n",
      "Epoch:  18 Loss:  928453.3983056364\n",
      "Epoch:  19 Loss:  976656.9580985332\n",
      "Epoch:  20 Loss:  893217.3319909839\n",
      "Epoch:  21 Loss:  937163.3744436707\n",
      "Epoch:  22 Loss:  863831.2035372965\n",
      "Epoch:  23 Loss:  897550.5772316945\n",
      "Epoch:  24 Loss:  837129.3557720016\n",
      "Epoch:  25 Loss:  861651.319656919\n",
      "Epoch:  26 Loss:  813756.0132732597\n",
      "Epoch:  27 Loss:  829447.5355609704\n",
      "Epoch:  28 Loss:  792095.4800384663\n",
      "Epoch:  29 Loss:  800147.7350871904\n",
      "Epoch:  30 Loss:  771164.0303933953\n",
      "Epoch:  31 Loss:  773328.238839732\n",
      "Epoch:  32 Loss:  750708.4669045645\n",
      "Epoch:  33 Loss:  748843.7853793345\n",
      "Epoch:  34 Loss:  731148.7558952137\n",
      "Epoch:  35 Loss:  727049.4321604006\n",
      "Epoch:  36 Loss:  713227.4185035641\n",
      "Epoch:  37 Loss:  707943.9046386158\n",
      "Epoch:  38 Loss:  696779.644749981\n",
      "Epoch:  39 Loss:  690769.2595956648\n",
      "Epoch:  40 Loss:  681318.7485640037\n",
      "Epoch:  41 Loss:  674919.699187775\n",
      "Epoch:  42 Loss:  666598.5092587587\n",
      "Epoch:  43 Loss:  660065.6205282202\n",
      "Epoch:  44 Loss:  652510.3942364217\n",
      "Epoch:  45 Loss:  646004.241985513\n",
      "Epoch:  46 Loss:  639000.7651071433\n",
      "Epoch:  47 Loss:  632648.2606229675\n",
      "Epoch:  48 Loss:  626121.833771467\n"
     ]
    }
   ],
   "source": [
    "SparseAE.train(x_train=x_train, y_train=x_train, epochs=50, shuffle=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
